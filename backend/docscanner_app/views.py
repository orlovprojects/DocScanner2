# --- Standard library ---
import hashlib
import io
import logging
import logging.config
import os
import tempfile
import zipfile
import json
from datetime import date, timedelta, time, datetime
from decimal import Decimal
import unicodedata
from django.http import HttpRequest
from django.contrib.auth import get_user_model

from django.core.files.base import ContentFile
from .tasks import process_uploaded_file_task 

# --- Django ---
from django.conf import settings
from django.db import transaction
from django.db.models import Q
from django.http import FileResponse, Http404, HttpResponse
from django.shortcuts import get_object_or_404
from django.utils import timezone
from django.utils.dateparse import parse_date

# --- Django REST Framework ---
from rest_framework import generics, permissions, status
from rest_framework.decorators import api_view, permission_classes, authentication_classes
from rest_framework.permissions import IsAuthenticated, AllowAny, IsAdminUser
from rest_framework.response import Response
from rest_framework.views import APIView

# --- DRF SimpleJWT ---
from rest_framework_simplejwt.views import TokenObtainPairView, TokenRefreshView

# --- Local (project) imports ---
from .data_import.data_import_from_buh import import_products_from_xlsx, import_clients_from_xlsx
from .exports.apskaita5 import export_documents_group_to_apskaita5_files
from .exports.centas import export_documents_group_to_centras_xml
from .exports.finvalda import (
    export_pirkimai_group_to_finvalda,
    export_pardavimai_group_to_finvalda,
)
from .exports.agnum import (
    export_pirkimai_group_to_agnum,
    export_pardavimai_group_to_agnum,
)
from .exports.rivile import (
    export_clients_group_to_rivile,
    export_pirkimai_group_to_rivile,
    export_pardavimai_group_to_rivile,
    export_prekes_paslaugos_kodai_group_to_rivile,
)
from .exports.rivile_erp import (
    export_clients_to_rivile_erp_xlsx,
    export_prekes_and_paslaugos_to_rivile_erp_xlsx,
    export_documents_to_rivile_erp_xlsx,
)
from .exports.dineta import send_dineta_bundle, DinetaError
# from .exports.optimum import optimum_hello, OptimumError
from .exports.pragma4 import export_to_pragma40_xml
from .exports.pragma3 import export_to_pragma_full, save_pragma_export_to_files
from .exports.butent import export_to_butent
from .exports.site_pro import export_to_site_pro
from .exports.debetas import export_to_debetas
from .validators.required_fields_checker import check_required_fields_for_export
from .validators.math_validator_for_export import validate_document_math_for_export

from .models import (
    CustomUser,
    ScannedDocument,
    ProductAutocomplete,
    ClientAutocomplete,
    LineItem,
    AdClick,
    MobileAccessKey,
    MobileInboxDocument,
    Payments
)

from .serializers import (
    CustomUserSerializer,
    ViewModeSerializer,
    ScannedDocumentSerializer,
    ScannedDocumentListSerializer,
    ScannedDocumentDetailSerializer,
    ScannedDocumentAdminDetailSerializer,
    AdClickSerializer,
    LineItemSerializer,
    CustomUserAdminListSerializer,
    DinetaSettingsSerializer,
    OptimumSettingsSerializer,
    MobileAccessKeySerializer,
    MobileInboxDocumentSerializer,
    PaymentSerializer
)
from django.db.models import Prefetch


from .tasks import process_uploaded_file_task
from .utils.data_resolver import build_preview
from .utils.pirkimas_pardavimas import determine_pirkimas_pardavimas
from .utils.prekes_kodas import assign_random_prekes_kodai
from .utils.save_document import _apply_sumiskai_defaults_from_user
from .utils.update_currency_rates import update_currency_rates
from .validators.vat_klas import auto_select_pvm_code

#dlia superuser dashboard
from django.db.models import Count
from .permissions import IsSuperUser, IsOwner

#wagtail imports
from rest_framework import viewsets, mixins
from .models import GuidePage, GuideCategoryPage
from rest_framework.decorators import action
from .serializers import (
    GuideCategoryListSerializer,
    GuideCategoryDetailSerializer,
    GuideArticleListSerializer,
    GuideArticleDetailSerializer,
)


#emails
from .emails import siusti_sveikinimo_laiska, siusti_kontakto_laiska
from .emails import siusti_masini_laiska_visiems
from .emails import siusti_mobilios_apps_kvietima
from .utils.play_store_link_gen import build_mobile_play_store_link

from time import perf_counter


# --- Logging setup ---
logging.config.dictConfig(settings.LOGGING)
logger = logging.getLogger('docscanner_app')
site_url = settings.SITE_URL_FRONTEND  # Ð±ÐµÑ€Ñ‘Ð¼ Ð¸Ð· settings.py


#admin dashboard
from datetime import datetime, time, timedelta
from django.utils import timezone
from rest_framework.decorators import api_view, permission_classes
from rest_framework.response import Response
import json

# from .models import ScannedDocument, CustomUser
# from .permissions import IsSuperUser
# from .views import summarize_doc_issues  # ÐµÑÐ»Ð¸ Ð² Ñ‚Ð¾Ð¼ Ð¶Ðµ Ñ„Ð°Ð¹Ð»Ðµ â€” Ð½Ðµ Ð½ÑƒÐ¶Ð½Ð¾

def _today_dates():
    today = timezone.localdate()
    yesterday = today - timedelta(days=1)
    return today, yesterday

def _count_by_date(model, date_field: str, target_date):
    return model.objects.filter(**{f"{date_field}__date": target_date}).count()

def _qs_by_date(model, date_field: str, target_date):
    start = timezone.make_aware(datetime.combine(target_date, time.min))
    end = timezone.make_aware(datetime.combine(target_date, time.max))
    return model.objects.filter(**{f"{date_field}__range": (start, end)})

def _qs_last_n_days(model, date_field: str, days: int):
    since = timezone.now() - timedelta(days=days)
    return model.objects.filter(**{f"{date_field}__gte": since})

def _ensure_dict(maybe_json):
    if not maybe_json:
        return {}
    if isinstance(maybe_json, dict):
        return maybe_json
    if isinstance(maybe_json, str):
        try:
            return json.loads(maybe_json)
        except Exception:
            return {}
    return {}

def _doc_has_issues(doc):
    raw = getattr(doc, 'structured_json', None) or getattr(doc, 'gpt_raw_json', None) or {}
    struct = _ensure_dict(raw)
    res = summarize_doc_issues(struct)
    return bool(res.get("has_issues"))

def _count_errors_in_qs(qs):
    n = 0
    for doc in qs.only('id', 'structured_json', 'gpt_raw_json'):
        if _doc_has_issues(doc):
            n += 1
    return n

def _pct(part, whole):
    return round((part / whole * 100.0), 2) if whole else 0.0

def _rate(ok_count, total_count):
    """% ÑƒÑÐ¿ÐµÑˆÐ½Ñ‹Ñ… (Ð±ÐµÐ· Ð¾ÑˆÐ¸Ð±Ð¾Ðº) Ð¾Ñ‚ Ð¾Ð±Ñ‰ÐµÐ³Ð¾ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð°."""
    return _pct(ok_count, total_count)

@api_view(["GET"])
@permission_classes([IsSuperUser])
def superuser_dashboard_stats(request):
    doc_date_field = "uploaded_at"
    user_date_field = "date_joined"

    today, yesterday = _today_dates()

    qs_all      = ScannedDocument.objects.all()
    qs_today    = _qs_by_date(ScannedDocument, doc_date_field, today)
    qs_yesterday= _qs_by_date(ScannedDocument, doc_date_field, yesterday)
    qs_7d       = _qs_last_n_days(ScannedDocument, doc_date_field, 7)
    qs_30d      = _qs_last_n_days(ScannedDocument, doc_date_field, 30)

    docs_today      = qs_today.count()
    docs_yesterday  = qs_yesterday.count()
    docs_7d         = qs_7d.count()
    docs_30d        = qs_30d.count()
    total_docs      = qs_all.count()

    err_today       = _count_errors_in_qs(qs_today)
    err_yesterday   = _count_errors_in_qs(qs_yesterday)
    err_7d          = _count_errors_in_qs(qs_7d)
    err_30d         = _count_errors_in_qs(qs_30d)
    err_total       = _count_errors_in_qs(qs_all)

    ok_today        = max(docs_today - err_today, 0)
    ok_yesterday    = max(docs_yesterday - err_yesterday, 0)
    ok_7d           = max(docs_7d - err_7d, 0)
    ok_30d          = max(docs_30d - err_30d, 0)
    ok_total        = max(total_docs - err_total, 0)

    # ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ (Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ð¸Ð· Ñ‚Ð²Ð¾ÐµÐ¹ Ð²ÐµÑ€ÑÐ¸Ð¸)
    start_today = timezone.make_aware(datetime.combine(today, time.min))
    end_today   = timezone.make_aware(datetime.combine(today, time.max))
    unique_users_excl_1_2_today = (
        ScannedDocument.objects
        .exclude(user_id__in=[1, 2])
        .filter(**{f"{doc_date_field}__range": (start_today, end_today)})
        .values("user_id").distinct().count()
    )

    # Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸/Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸
    new_users_today      = _count_by_date(CustomUser, user_date_field, today)
    new_users_yesterday  = _count_by_date(CustomUser, user_date_field, yesterday)
    new_users_7d         = _qs_last_n_days(CustomUser, user_date_field, 7).count()
    new_users_30d        = _qs_last_n_days(CustomUser, user_date_field, 30).count()
    total_users          = CustomUser.objects.count()

    # Ñ€Ð°Ð·Ð±Ð¸Ð²ÐºÐ° Ð¿Ð¾ Ñ‚Ð¸Ð¿Ð°Ð¼
    st_sumiskai = qs_all.filter(scan_type="sumiskai").count()
    st_detaliai = qs_all.filter(scan_type="detaliai").count()

    data = {
        "documents": {
            "today":            {"count": docs_today,     "errors": err_today},
            "yesterday":        {"count": docs_yesterday, "errors": err_yesterday},
            "last_7_days":      {"count": docs_7d,        "errors": err_7d},
            "last_30_days":     {"count": docs_30d,       "errors": err_30d},
            "total":            {"count": total_docs,     "errors": err_total},

            # âœ… ÐÐ¾Ð²Ñ‹Ð¹ Ð±Ð»Ð¾Ðº â€” Success rate (Ð±ÐµÐ· Ð¾ÑˆÐ¸Ð±Ð¾Ðº)
            "success_rate": {
                "today":        _rate(ok_today,     docs_today),
                "yesterday":    _rate(ok_yesterday, docs_yesterday),
                "last_7_days":  _rate(ok_7d,        docs_7d),
                "last_30_days": _rate(ok_30d,       docs_30d),
                "total":        _rate(ok_total,     total_docs),
            },

            "unique_users_excluding_1_2_today": unique_users_excl_1_2_today,
            "scan_types": {
                "sumiskai": {"count": st_sumiskai, "pct": _pct(st_sumiskai, total_docs)},
                "detaliai": {"count": st_detaliai, "pct": _pct(st_detaliai, total_docs)},
            },
        },
        "users": {
            "new_today":        new_users_today,
            "new_yesterday":    new_users_yesterday,
            "new_last_7_days":  new_users_7d,
            "new_last_30_days": new_users_30d,
            "total":            total_users,
        },
        "meta": {
            "timezone": str(timezone.get_current_timezone()),
            "generated_at": timezone.now().isoformat(),
        },
    }
    return Response(data)








def strip_diacritics(text):
    """
    PakeiÄia visas lietuviÅ¡kas ir kitas lotyniÅ¡kas raides su diakritika
    Ä¯ paprastas: Å¡->s, Ä…->a, Å½->Z ir t.t.
    """
    if not isinstance(text, str):
        return text
    normalized = unicodedata.normalize("NFD", text)
    return "".join(ch for ch in normalized if unicodedata.category(ch) != "Mn")



@api_view(['POST'])
@permission_classes([IsAuthenticated])
def export_documents(request):
    from datetime import date
    import io
    import zipfile
    import tempfile
    import logging

    logger = logging.getLogger(__name__)
    log_ctx = {"user": getattr(request.user, "id", None)}

    # ---- Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
    ids = request.data.get('ids', [])
    export_type = request.data.get('export_type') or getattr(request.user, 'default_accounting_program', 'centas')
    raw_overrides = request.data.get('overrides', {}) or {}
    mode_raw = (request.data.get('mode') or "").strip().lower()  # <<< NEW

    logger.info("[EXP] start user=%s export_type_raw=%r ids=%s raw_overrides=%r mode_raw=%r",
                log_ctx["user"], export_type, ids, raw_overrides, mode_raw)

    if not ids:
        logger.warning("[EXP] no ids provided")
        return Response({"error": "No document ids provided"}, status=400)

    user = request.user
    export_type = str(export_type).lower()

    # RivilÄ—: ar reikia nuimti lietuviÅ¡kas raides (Å¡->s ir t.t.)
    extra_settings = getattr(user, "extra_settings", {}) or {}
    rivile_strip_lt = bool(extra_settings.get("rivile_strip_lt_letters"))
    logger.info("[EXP] user extra_settings: rivile_strip_lt_letters=%s", rivile_strip_lt)

    # --- Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ overrides (id -> 'pirkimas'|'pardavimas')
    overrides = {}
    for k, v in raw_overrides.items():
        key = str(k)
        val = str(v).lower()
        if val in ('pirkimas', 'pardavimas'):
            overrides[key] = val
        else:
            logger.warning("[EXP] skip override key=%r val=%r (invalid)", key, v)

    # --- Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ mode: Ð±ÐµÑ€Ñ‘Ð¼ Ð¸Ð· ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°, Ð¸Ð½Ð°Ñ‡Ðµ ÐºÐ°Ðº Ñ€Ð°Ð½ÑŒÑˆÐµ (Ð¿Ð¾ overrides)
    if mode_raw in ("multi", "single"):                       # <<< NEW
        mode = mode_raw
        logger.info("[EXP] view mode taken from request: %s", mode)
    else:
        mode = 'multi' if overrides else 'single'
        logger.info("[EXP] view mode inferred for backward-compat: %s", mode)

    # Ð”Ð¾Ð¿. Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ°: ÐµÑÐ»Ð¸ Ð¿Ñ€Ð¸ÑˆÑ‘Ð» multi, Ð½Ð¾ overrides Ð¿ÑƒÑÑ‚Ð¾Ð¹
    if mode == "multi" and not overrides:
        logger.info("[EXP] mode is 'multi' but overrides are EMPTY (will rely on resolver/doc DB fields)")

    logger.info("[EXP] export_type=%s overrides_norm=%r", export_type, overrides)

    today_str = date.today().strftime('%Y-%m-%d')

    documents = ScannedDocument.objects.filter(pk__in=ids, user=user).prefetch_related('line_items')
    # documents = ScannedDocument.objects.filter(pk__in=ids, user=user)
    if not documents:
        logger.warning("[EXP] no documents found by ids=%s user=%s", ids, log_ctx["user"])
        return Response({"error": "No documents found"}, status=404)

    # === Ñ€ÐµÐ·Ð¾Ð»Ð²ÐµÑ€ ===
    from .utils.data_resolver import prepare_export_groups
    logger.info("[EXP] resolver_mode=%s", mode)

    try:
        prepared = prepare_export_groups(
            documents,
            user=user,
            overrides=overrides if mode == 'multi' else {},  # <<< ÑƒÐ²Ð°Ð¶Ð°Ñ‚ÑŒ Ð¿ÐµÑ€ÐµÐ´Ð°Ð½Ð½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼
            view_mode=mode,                                   # <<< ÑƒÐ²Ð°Ð¶Ð°Ñ‚ÑŒ Ð¿ÐµÑ€ÐµÐ´Ð°Ð½Ð½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼
        )
    except Exception as e:
        logger.exception("[EXP] prepare_export_groups failed: %s", e)
        return Response({"error": "Resolver failed", "detail": str(e)}, status=500)

    # Ð±Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹ Ð´Ð°Ð¼Ð¿ Ñ‚Ð¾Ð³Ð¾, Ñ‡Ñ‚Ð¾ Ð¿Ñ€Ð¸ÑˆÐ»Ð¾ Ð¸Ð· Ñ€ÐµÐ·Ð¾Ð»Ð²ÐµÑ€Ð°
    def _debug_dump(prepared_obj, where):
        for bucket in ("pirkimai", "pardavimai", "unknown"):
            packs = prepared_obj.get(bucket) or []
            logger.info("[EXPDBG:%s] bucket=%s count=%d", where, bucket, len(packs))
            for p in packs:
                d = p.get("doc")
                dpk = getattr(d, "pk", None)
                li = p.get("line_items") or []
                logger.info(
                    "[EXPDBG:%s] bucket=%s doc=%s dir=%r pack_keys=%s pvm=%r lines=%d",
                    where, bucket, dpk, p.get("direction"), list(p.keys()),
                    p.get("pvm_kodas", None), len(li)
                )
                if li:
                    preview = [(x.get("id"), x.get("pvm_kodas")) for x in li[:3]]
                    logger.info("[EXPDBG:%s] doc=%s sample_line_items=%s", where, dpk, preview)

    _debug_dump(prepared, "after_resolver")

    # Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Â«Ð² Ð¿Ð°Ð¼ÑÑ‚ÑŒÂ» (Ð±ÐµÐ· ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð² Ð‘Ð”)
    def _apply_resolved(pack_list, tag):
        out_docs = []
        for pack in pack_list:
            d = pack["doc"]
            setattr(d, "pirkimas_pardavimas", pack.get("direction"))
            setattr(d, "pvm_kodas", pack.get("pvm_kodas", None))  # ÑÐ²Ð½Ð¾Ðµ Ð¿ÐµÑ€ÐµÑ‚Ð¸Ñ€Ð°Ð½Ð¸Ðµ

            line_map = {}
            for li in (pack.get("line_items") or []):
                li_id = li.get("id")
                if li_id is not None:
                    line_map[li_id] = li.get("pvm_kodas")
            setattr(d, "_pvm_line_map", line_map)

            logger.info("[EXPDBG:apply] tag=%s doc=%s dir=%r pvm_kodas=%r line_map_size=%d",
                        tag, getattr(d, "pk", None), getattr(d, "pirkimas_pardavimas", None),
                        getattr(d, "pvm_kodas", None), len(line_map))
            out_docs.append(d)
        return out_docs

    pirkimai_docs   = _apply_resolved(prepared.get("pirkimai", []), "pirkimai")
    pardavimai_docs = _apply_resolved(prepared.get("pardavimai", []), "pardavimai")
    unknown_docs    = _apply_resolved(prepared.get("unknown", []), "unknown")

    logger.info("[EXP] ready_for_export counts: pirkimai=%d pardavimai=%d unknown=%d",
                len(pirkimai_docs), len(pardavimai_docs), len(unknown_docs))

    # --- Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð´Ð»Ñ ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ„Ð¸Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°
    response = None
    export_success = False
    exported_ids = [d.pk for d in (pirkimai_docs + pardavimai_docs)]

    # Ð¾Ð±Ñ‰Ð¸Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€ (Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ð²ÐµÑ‚Ð¾Ðº Ð¼Ð¾Ð¶Ð½Ð¾ Ð¿ÐµÑ€ÐµÐ¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÑ‚ÑŒ/Ð¾Ñ‡Ð¸Ñ‰Ð°Ñ‚ÑŒ)
    files_to_export = []

    # ========================= CENTAS =========================
    if export_type == 'centas':
        logger.info("[EXP] CENTAS export started")
        assign_random_prekes_kodai(documents)

        if pirkimai_docs:
            logger.info("[EXP] CENTAS exporting pirkimai: %d docs", len(pirkimai_docs))
            # Ð˜Ð—ÐœÐ•ÐÐ•ÐÐž: Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€ user=request.user
            xml_bytes = export_documents_group_to_centras_xml(
                pirkimai_docs, 
                direction="pirkimas",
                user=request.user  # <-- Ð”ÐžÐ‘ÐÐ’Ð¬ Ð­Ð¢Ðž
            )
            files_to_export.append((f"{today_str}_pirkimai.xml", xml_bytes))
            
        if pardavimai_docs:
            logger.info("[EXP] CENTAS exporting pardavimai: %d docs", len(pardavimai_docs))
            # Ð˜Ð—ÐœÐ•ÐÐ•ÐÐž: Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€ user=request.user
            xml_bytes = export_documents_group_to_centras_xml(
                pardavimai_docs, 
                direction="pardavimas",
                user=request.user  # <-- Ð”ÐžÐ‘ÐÐ’Ð¬ Ð­Ð¢Ðž
            )
            files_to_export.append((f"{today_str}_pardavimai.xml", xml_bytes))

        logger.info("[EXP] CENTAS files_to_export=%s", [n for n, _ in files_to_export])

        if len(files_to_export) > 1:
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w") as zf:
                for filename, xml_content in files_to_export:
                    zf.writestr(filename, xml_content)
            zip_buffer.seek(0)
            response = HttpResponse(zip_buffer.read(), content_type='application/zip')
            response['Content-Disposition'] = f'attachment; filename={today_str}_importui.zip'
            export_success = True
        elif len(files_to_export) == 1:
            filename, xml_content = files_to_export[0]
            response = HttpResponse(
                xml_content,
                content_type='application/xml; charset=utf-8'
            )
            response['Content-Disposition'] = f'attachment; filename={filename}'
            export_success = True
        else:
            logger.warning("[EXP] CENTAS nothing to export")
            response = Response({"error": "No documents to export"}, status=400)

    # ========================= RIVILÄ– (EIP) =========================

    elif export_type == 'rivile':
        logger.info("[EXP] RIVILE export started")
        assign_random_prekes_kodai(documents)

        files_to_export = []

        # 1) ÐšÐ»Ð¸ÐµÐ½Ñ‚Ñ‹ (N08+N33): ÑÐ¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð˜Ð— Ð”ÐžÐšÐ£ÐœÐ•ÐÐ¢ÐžÐ’; ÐºÑÑˆ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð½Ðµ Ð½ÑƒÐ¶ÐµÐ½
        docs_for_clients = (pirkimai_docs or []) + (pardavimai_docs or [])
        if docs_for_clients:
            klientai_xml = export_clients_group_to_rivile(
                clients=None,
                documents=docs_for_clients,
            )
            if klientai_xml and klientai_xml.strip():
                files_to_export.append(('klientai.eip', klientai_xml))
                logger.info("[EXP] RIVILE clients exported")

        # 2) ÐŸÐ˜Ð ÐšÐ˜ÐœÐÐ˜ (I06/I07)
        if pirkimai_docs:
            logger.info("[EXP] RIVILE exporting pirkimai: %d docs", len(pirkimai_docs))
            pirkimai_xml = export_pirkimai_group_to_rivile(pirkimai_docs, request.user)
            files_to_export.append(('pirkimai.eip', pirkimai_xml))

        # 3) ÐŸÐÐ Ð”ÐÐ’Ð˜ÐœÐÐ˜ (I06/I07)
        if pardavimai_docs:
            logger.info("[EXP] RIVILE exporting pardavimai: %d docs", len(pardavimai_docs))
            pardavimai_xml = export_pardavimai_group_to_rivile(pardavimai_docs, request.user)
            files_to_export.append(('pardavimai.eip', pardavimai_xml))

        # 4) N17/N25 - Ð˜Ð—ÐœÐ•ÐÐ•ÐÐž: Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‘Ð¼ request.user
        prekes_xml, paslaugos_xml, kodai_xml = export_prekes_paslaugos_kodai_group_to_rivile(
            documents, 
            request.user  # â† Ð”ÐžÐ‘ÐÐ’Ð›Ð•ÐÐž
        )
        if prekes_xml and prekes_xml.strip():
            files_to_export.append(('prekes.eip', prekes_xml))
        if paslaugos_xml and paslaugos_xml.strip():
            files_to_export.append(('paslaugos.eip', paslaugos_xml))
        if kodai_xml and kodai_xml.strip():
            files_to_export.append(('kodai.eip', kodai_xml))

        logger.info("[EXP] RIVILE files_to_export=%s", [n for n, _ in files_to_export])

        # Jei profilyje nustatyta â€žrivile_strip_lt_letters" â€“ nuimame diakritikÄ…
        # if rivile_strip_lt and files_to_export:
        #     new_files = []
        #     for filename, xml_content in files_to_export:
        #         # Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€ÑƒÐµÐ¼, Ñ‡Ñ‚Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÐ¼ Ñ bytes
        #         if isinstance(xml_content, bytes):
        #             try:
        #                 # Ð´ÐµÐºÐ¾Ð´Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚Ð°Ðº Ð¶Ðµ, ÐºÐ°Ðº Ð¸ Ð¿Ð¸ÑÐ°Ð»Ð¸ â€” windows-1257
        #                 xml_text = xml_content.decode("windows-1257", errors="ignore")
        #             except Exception:
        #                 # ÐºÑ€Ð°Ð¹Ð½Ð¸Ð¹ ÑÐ»ÑƒÑ‡Ð°Ð¹ â€“ Ñ…Ð¾Ñ‚Ñ ÑÑŽÐ´Ð° Ð²Ñ€ÑÐ´ Ð»Ð¸ Ð¿Ð¾Ð¿Ð°Ð´Ñ‘Ð¼
        #                 xml_text = xml_content.decode("latin-1", errors="ignore")
        #         else:
        #             xml_text = xml_content

        #         stripped = strip_diacritics(xml_text)

        #         # Ð’ÐÐ–ÐÐž: ÑÐ½Ð¾Ð²Ð° ÐºÐ¾Ð´Ð¸Ñ€ÑƒÐµÐ¼ Ð² windows-1257, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ñ„Ð°Ð¹Ð» Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ Ð±Ñ‹Ð» ANSI
        #         stripped_bytes = stripped.encode("windows-1257", errors="replace")

        #         logger.info(
        #             "[EXP] RIVILE strip_lt applied to %s (len %d -> %d)",
        #             filename, len(xml_text), len(stripped)
        #         )
        #         new_files.append((filename, stripped_bytes))

        #     files_to_export = new_files

        if files_to_export:
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w") as zf:
                for filename, xml_content in files_to_export:
                    zf.writestr(filename, xml_content)
            zip_buffer.seek(0)
            response = HttpResponse(zip_buffer.read(), content_type='application/zip')
            response['Content-Disposition'] = f'attachment; filename={today_str}_rivile_eip.zip'
            export_success = True
        else:
            logger.warning("[EXP] RIVILE nothing to export")
            response = Response({"error": "No documents to export"}, status=400)


    # ========================= FINVALDA =========================
    elif export_type == 'finvalda':
        logger.info("[EXP] FINVALDA export started")
        assign_random_prekes_kodai(documents)

        files_to_export = []

        if pirkimai_docs:
            logger.info("[EXP] FINVALDA exporting pirkimai: %d docs", len(pirkimai_docs))
            xml_bytes = export_pirkimai_group_to_finvalda(pirkimai_docs)
            files_to_export.append((f"{today_str}_pirkimai_finvalda.xml", xml_bytes))
        if pardavimai_docs:
            logger.info("[EXP] FINVALDA exporting pardavimai: %d docs", len(pardavimai_docs))
            xml_bytes = export_pardavimai_group_to_finvalda(pardavimai_docs)
            files_to_export.append((f"{today_str}_pardavimai_finvalda.xml", xml_bytes))

        logger.info("[EXP] FINVALDA files_to_export=%s", [n for n, _ in files_to_export])

        if len(files_to_export) > 1:
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w") as zf:
                for filename, xml_content in files_to_export:
                    zf.writestr(filename, xml_content)
            zip_buffer.seek(0)
            response = HttpResponse(zip_buffer.read(), content_type='application/zip')
            response['Content-Disposition'] = f'attachment; filename={today_str}_finvalda.zip'
            export_success = True
        elif len(files_to_export) == 1:
            filename, xml_content = files_to_export[0]
            response = HttpResponse(xml_content, content_type='application/xml')
            response['Content-Disposition'] = f'attachment; filename={filename}'
            export_success = True
        else:
            logger.warning("[EXP] FINVALDA nothing to export")
            response = Response({"error": "No documents to export"}, status=400)



    # ========================= PRAGMA 3.2 =========================
    elif export_type == 'pragma3':
        logger.info("[EXP] PRAGMA32 export started")
        assign_random_prekes_kodai(documents)

        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑƒÐ¶Ðµ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð»ÐµÐ½Ð½Ñ‹Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ñ Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ‚Ð°Ð¼Ð¸ Ð¾Ñ‚ _apply_resolved
        # (pirkimas_pardavimas, pvm_kodas, _pvm_line_map)
        # Ð’ÐÐ–ÐÐž: Ð´Ð¾Ð±Ð°Ð²ÑŒ .prefetch_related('line_items') Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð³Ð´Ðµ documents = ...
        all_docs = (pirkimai_docs or []) + (pardavimai_docs or [])

        files_to_export = []

        try:
            # ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚ (4 Ð¸Ð»Ð¸ 6 Ñ„Ð°Ð¹Ð»Ð¾Ð²)
            export_data = export_to_pragma_full(
                documents=all_docs,
                user=request.user,
                include_reference_data=True
            )
            
            logger.info("[EXP] PRAGMA32 export_data keys: %s", list(export_data.keys()))

            # Pirkimai (ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ)
            if export_data.get('pirkimai'):
                files_to_export.append((
                    f'{today_str}_pirkimai.txt',
                    export_data['pirkimai']
                ))
            
            if export_data.get('pirkimai_det'):
                files_to_export.append((
                    f'{today_str}_pirkimai_det.txt',
                    export_data['pirkimai_det']
                ))

            # Pardavimai (ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ)
            if export_data.get('pardavimai'):
                files_to_export.append((
                    f'{today_str}_pardavimai.txt',
                    export_data['pardavimai']
                ))
            
            if export_data.get('pardavimai_det'):
                files_to_export.append((
                    f'{today_str}_pardavimai_det.txt',
                    export_data['pardavimai_det']
                ))
            
            # Ð¡Ð¿Ñ€Ð°Ð²Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ (Ð¾Ð±Ñ‰Ð¸Ðµ)
            if export_data.get('companies'):
                files_to_export.append((
                    f'{today_str}_Imones.txt',
                    export_data['companies']
                ))
            
            if export_data.get('products'):
                files_to_export.append((
                    f'{today_str}_Prekes.txt',
                    export_data['products']
                ))

            logger.info("[EXP] PRAGMA32 files_to_export=%s", [n for n, _ in files_to_export])

        except Exception as e:
            logger.exception("[EXP] PRAGMA32 export failed: %s", e)
            return Response({"error": "Pragma 3.2 export failed", "detail": str(e)}, status=500)

        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð°
        if len(files_to_export) > 1:
            # ÐÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ñ„Ð°Ð¹Ð»Ð¾Ð² -> ZIP
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w") as zf:
                for filename, txt_content in files_to_export:
                    zf.writestr(filename, txt_content)
            zip_buffer.seek(0)
            response = HttpResponse(zip_buffer.read(), content_type='application/zip')
            response['Content-Disposition'] = f'attachment; filename={today_str}_pragma32.zip'
            export_success = True
            
        elif len(files_to_export) == 1:
            # ÐžÐ´Ð¸Ð½ Ñ„Ð°Ð¹Ð» -> Ð¿Ñ€ÑÐ¼Ð°Ñ Ð¾Ñ‚Ð´Ð°Ñ‡Ð°
            filename, txt_content = files_to_export[0]
            response = HttpResponse(
                txt_content,
                content_type='text/plain; charset=windows-1257'
            )
            response['Content-Disposition'] = f'attachment; filename={filename}'
            export_success = True
            
        else:
            logger.warning("[EXP] PRAGMA32 nothing to export")
            response = Response({"error": "No documents to export"}, status=400)


    # ========================= PRAGMA 4.0 =========================
    elif export_type == 'pragma4':
        logger.info("[EXP] PRAGMA40 export started")
        assign_random_prekes_kodai(documents)

        # Ð¢Ð²Ð¾Ð¹ ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ñ‘Ñ€: ÑÐ°Ð¼ Ñ€ÐµÑˆÐ°ÐµÑ‚, XML Ð¸Ð»Ð¸ ZIP
        try:
            content = export_to_pragma40_xml(
                pirkimai_documents=pirkimai_docs,
                pardavimai_documents=pardavimai_docs
            )
        except Exception as e:
            logger.exception("[EXP] PRAGMA40 export failed: %s", e)
            return Response({"error": "Pragma 4.0 export failed", "detail": str(e)}, status=500)

        if not content:
            logger.warning("[EXP] PRAGMA40 nothing to export")
            response = Response({"error": "No documents to export"}, status=400)
        else:
            # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼, ZIP ÑÑ‚Ð¾ Ð¸Ð»Ð¸ XML Ð¿Ð¾ ÑÐ¸Ð³Ð½Ð°Ñ‚ÑƒÑ€Ðµ ZIP ("PK")
            if content[:2] == b'PK':
                filename = f"{today_str}_pragma40.zip"
                content_type = "application/zip"
            else:
                # Ð•ÑÐ»Ð¸ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð»Ð¸ÑÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ÐºÑƒÐ¿ÐºÐ¸/Ð¿Ñ€Ð¾Ð´Ð°Ð¶Ð¸ â€“ Ð±Ð¾Ð»ÐµÐµ Ð³Ð¾Ð²Ð¾Ñ€ÑÑ‰Ð¸Ðµ Ð¸Ð¼ÐµÐ½Ð°
                if pirkimai_docs and not pardavimai_docs:
                    filename = f"{today_str}_pragma40_pirkimai.xml"
                elif pardavimai_docs and not pirkimai_docs:
                    filename = f"{today_str}_pragma40_pardavimai.xml"
                else:
                    filename = f"{today_str}_pragma40.xml"

                content_type = "application/xml; charset=utf-8"

            response = HttpResponse(content, content_type=content_type)
            response['Content-Disposition'] = f'attachment; filename="{filename}"'
            export_success = True


    # ========================= DINETA =========================
    elif export_type == 'dineta':
        logger.info("[EXP] DINETA export started")

        # Ð”Ð»Ñ Dineta Ð½Ð°Ð¼ Ð»Ð¾Ð³Ð¸Ñ‡Ð½ÐµÐµ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÑ‚ÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑƒÐ¶Ðµ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ð¾ÐºÑƒÐ¿ÐºÐ¸/Ð¿Ñ€Ð¾Ð´Ð°Ð¶Ð¸
        all_docs = (pirkimai_docs or []) + (pardavimai_docs or [])

        if not all_docs:
            logger.warning("[EXP] DINETA no documents to export (no pirkimai/pardavimai)")
            return Response({"error": "No documents to send to Dineta"}, status=400)

        try:
            dineta_result = send_dineta_bundle(request.user, all_docs)
        except DinetaError as e:
            logger.exception("[EXP] DINETA export failed (DinetaError): %s", e)
            return Response(
                {
                    "error": "Dineta export failed",
                    "detail": str(e),
                },
                status=502,  # bad gateway / Ð²Ð½ÐµÑˆÐ½ÑÑ ÑÐ¸ÑÑ‚ÐµÐ¼Ð°
            )
        except Exception as e:
            logger.exception("[EXP] DINETA export failed (unexpected): %s", e)
            return Response(
                {
                    "error": "Dineta export failed (unexpected)",
                    "detail": str(e),
                },
                status=500,
            )

        # Ð•ÑÐ»Ð¸ ÑÑŽÐ´Ð° Ð´Ð¾ÑˆÐ»Ð¸ â€“ ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚ ÑƒÑÐ¿ÐµÑˆÐ½Ñ‹Ð¼
        export_success = True
        # response â€” Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ JSON Ñ Ñ‚ÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð²ÐµÑ€Ð½ÑƒÐ»Ð° Dineta
        response = Response(
            {
                "status": "ok",
                "dineta": dineta_result,
            },
            status=200,
        )



    # ========================= Butent =========================
    elif export_type == 'butent':
        logger.info("[EXP] BUTENT export started")
        assign_random_prekes_kodai(documents)

        # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð²ÑÐµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð´Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð° (BÅ«tent Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ ÑÐ¼ÐµÑˆÐ¸Ð²Ð°Ð½Ð¸Ðµ)
        all_docs = (pirkimai_docs or []) + (pardavimai_docs or [])
        
        if not all_docs:
            logger.warning("[EXP] BUTENT no documents to export")
            return Response({"error": "No documents to export"}, status=400)

        try:
            # Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð² Excel (mode='auto' Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Dict[str, bytes])
            result = export_to_butent(
                documents=all_docs,
                mode='auto',
                user=request.user
            )
            
            logger.info("[EXP] BUTENT export completed, files=%s", list(result.keys()))
            
            # Ð•ÑÐ»Ð¸ Ð¾Ð´Ð¸Ð½ Ñ„Ð°Ð¹Ð» - Ð¾Ñ‚Ð´Ð°ÐµÐ¼ ÐµÐ³Ð¾ Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ
            if len(result) == 1:
                mode, excel_bytes = list(result.items())[0]
                filename = f'{today_str}_butent_{mode}_import.xlsx'
                
                logger.info("[EXP] BUTENT single file: %s, size=%d bytes", filename, len(excel_bytes))
                
                response = HttpResponse(
                    excel_bytes,
                    content_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
                )
                response['Content-Disposition'] = f'attachment; filename="{filename}"'
                export_success = True
            
            # Ð•ÑÐ»Ð¸ Ð´Ð²Ð° Ñ„Ð°Ð¹Ð»Ð° - ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ ZIP Ð°Ñ€Ñ…Ð¸Ð²
            else:
                import zipfile
                from io import BytesIO
                
                zip_buffer = BytesIO()
                
                with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                    for mode, excel_bytes in result.items():
                        filename = f'{today_str}_butent_{mode}_import.xlsx'
                        zip_file.writestr(filename, excel_bytes)
                        logger.info("[EXP] BUTENT added to ZIP: %s, size=%d bytes", filename, len(excel_bytes))
                
                zip_buffer.seek(0)
                zip_bytes = zip_buffer.read()
                
                logger.info("[EXP] BUTENT ZIP created, size=%d bytes", len(zip_bytes))
                
                response = HttpResponse(zip_bytes, content_type='application/zip')
                response['Content-Disposition'] = f'attachment; filename="{today_str}_butent_import.zip"'
                export_success = True

        except FileNotFoundError as e:
            logger.error("[EXP] BUTENT template not found: %s", e)
            return Response({
                "error": "BÅ«tent template not found",
                "detail": "Please create template using create_butent_template()"
            }, status=500)
        
        except Exception as e:
            logger.exception("[EXP] BUTENT export failed: %s", e)
            return Response({
                "error": "BÅ«tent export failed",
                "detail": str(e)
            }, status=500)
        

    # ========================= SITE.PRO (B1) =========================
    elif export_type == 'site_pro':
        logger.info("[EXP] SITE.PRO(B1) export started")
        assign_random_prekes_kodai(documents)

        # Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑƒÐ¶Ðµ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ (pirkimai + pardavimai)
        all_docs = (pirkimai_docs or []) + (pardavimai_docs or [])

        if not all_docs:
            logger.warning("[EXP] SITE.PRO(B1) no documents to export (no pirkimai/pardavimai)")
            return Response({"error": "No documents to export"}, status=400)

        try:
            # result: {"clients": bytes, "items": bytes, "purchases": bytes, "sales": bytes}
            result = export_to_site_pro(all_docs, user=request.user)
            logger.info("[EXP] SITE.PRO(B1) export completed, keys=%s", list(result.keys()))

            files_to_export = []

            if result.get("clients"):
                files_to_export.append((f"{today_str}_site_pro_klientai.xlsx", result["clients"]))
            if result.get("items"):
                files_to_export.append((f"{today_str}_site_pro_prekes_paslaugos.xlsx", result["items"]))
            if result.get("purchases"):
                files_to_export.append((f"{today_str}_site_pro_pirkimai.xlsx", result["purchases"]))
            if result.get("sales"):
                files_to_export.append((f"{today_str}_site_pro_pardavimai.xlsx", result["sales"]))

            if not files_to_export:
                logger.warning("[EXP] SITE.PRO(B1) nothing to export (empty bytes)")
                return Response({"error": "No documents to export"}, status=400)

            # B1 Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ = 4 Ñ„Ð°Ð¹Ð»Ð° -> ZIP
            if len(files_to_export) > 1:
                zip_buffer = io.BytesIO()
                with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                    for filename, content in files_to_export:
                        zf.writestr(filename, content)
                        logger.info("[EXP] SITE.PRO(B1) added to ZIP: %s size=%d", filename, len(content))
                zip_buffer.seek(0)

                response = HttpResponse(zip_buffer.read(), content_type="application/zip")
                response["Content-Disposition"] = f'attachment; filename="{today_str}_site_pro_importas.zip"'
                export_success = True

            else:
                filename, content = files_to_export[0]
                response = HttpResponse(
                    content,
                    content_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
                response["Content-Disposition"] = f'attachment; filename="{filename}"'
                export_success = True

        except FileNotFoundError as e:
            logger.exception("[EXP] SITE.PRO(B1) template not found: %s", e)
            return Response(
                {"error": "B1 template not found", "detail": str(e)},
                status=500
            )
        except Exception as e:
            logger.exception("[EXP] SITE.PRO(B1) export failed: %s", e)
            return Response(
                {"error": "B1 export failed", "detail": str(e)},
                status=500
            )



    # ========================= APSKAITA5 =========================
    elif export_type == 'apskaita5':
        logger.info("[EXP] APSKAITA5 export started")
        assign_random_prekes_kodai(documents)

        content, filename, content_type = export_documents_group_to_apskaita5_files(
            documents=documents,
            site_url=site_url,   # Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ, Ñ‡Ñ‚Ð¾ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð° Ð²Ñ‹ÑˆÐµ Ð¿Ð¾ Ð¼Ð¾Ð´ÑƒÐ»ÑŽ/ÐºÐ¾Ð½Ñ„Ð¸Ð³Ñƒ
            company_code=None,
            direction=None,
        )
        logger.info("[EXP] APSKAITA5 produced file=%s content_type=%s size=%d",
                    filename, content_type, len(content))
        response = HttpResponse(content, content_type=content_type)
        response['Content-Disposition'] = f'attachment; filename="{filename}"'
        response['X-Content-Type-Options'] = 'nosniff'
        export_success = True

    # ========================= AGNUM =========================
    elif export_type == 'agnum':
        logger.info("[EXP] AGNUM export started")
        assign_random_prekes_kodai(documents)

        files_to_export = []

        # 1) Pirkimai (Type="2")
        if pirkimai_docs:
            logger.info("[EXP] AGNUM exporting pirkimai: %d docs", len(pirkimai_docs))
            pirkimai_xml = export_pirkimai_group_to_agnum(pirkimai_docs, request.user)
            files_to_export.append((f'{today_str}_pirkimai_agnum.xml', pirkimai_xml))

        # 2) Pardavimai (Type="4")
        if pardavimai_docs:
            logger.info("[EXP] AGNUM exporting pardavimai: %d docs", len(pardavimai_docs))
            pardavimai_xml = export_pardavimai_group_to_agnum(pardavimai_docs, request.user)
            files_to_export.append((f'{today_str}_pardavimai_agnum.xml', pardavimai_xml))

        logger.info("[EXP] AGNUM files_to_export=%s", [n for n, _ in files_to_export])

        if len(files_to_export) > 1:
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w") as zf:
                for filename, xml_content in files_to_export:
                    zf.writestr(filename, xml_content)
            zip_buffer.seek(0)
            response = HttpResponse(zip_buffer.read(), content_type='application/zip')
            response['Content-Disposition'] = f'attachment; filename={today_str}_agnum.zip'
            export_success = True
        elif len(files_to_export) == 1:
            filename, xml_content = files_to_export[0]
            response = HttpResponse(
                xml_content,
                content_type='application/xml; charset=utf-8'
            )
            response['Content-Disposition'] = f'attachment; filename={filename}'
            export_success = True
        else:
            logger.warning("[EXP] AGNUM nothing to export")
            response = Response({"error": "No documents to export"}, status=400)


    # # ========================= DEBETAS =========================
    elif export_type == 'debetas':
        logger.info("[EXP] DEBETAS export started")
        assign_random_prekes_kodai(documents)

        # Ð‘ÐµÑ€Ñ‘Ð¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑƒÐ¶Ðµ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ (pirkimai + pardavimai)
        all_docs = (pirkimai_docs or []) + (pardavimai_docs or [])

        if not all_docs:
            logger.warning("[EXP] DEBETAS no documents to export (no pirkimai/pardavimai)")
            return Response({"error": "No documents to export"}, status=400)

        try:
            debetas_result = export_to_debetas(
                documents=all_docs,
                user=request.user,
            )
        except FileNotFoundError as e:
            logger.exception("[EXP] DEBETAS template not found: %s", e)
            return Response(
                {
                    "error": "Debetas template not found",
                    "detail": str(e),
                },
                status=500,
            )
        except Exception as e:
            logger.exception("[EXP] DEBETAS export failed: %s", e)
            return Response(
                {
                    "error": "Debetas export failed",
                    "detail": str(e),
                },
                status=500,
            )

        logger.info("[EXP] DEBETAS export result keys: %s", list(debetas_result.keys()))

        # Ð•ÑÐ»Ð¸ ÐµÑÑ‚ÑŒ zip (Ð¸ pirkimai, Ð¸ pardavimai) â€” Ð¾Ñ‚Ð´Ð°Ñ‘Ð¼ ÐµÐ³Ð¾
        if debetas_result.get("zip"):
            content = debetas_result["zip"]
            filename = debetas_result.get("zip_filename", f"Debetas_Import_{today_str}.zip")
            response = HttpResponse(content, content_type="application/zip")
            response['Content-Disposition'] = f'attachment; filename="{filename}"'
            export_success = True

        # Ð•ÑÐ»Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ pirkimai
        elif debetas_result.get("pirkimai"):
            content = debetas_result["pirkimai"]
            filename = debetas_result.get("pirkimai_filename", f"Debetas_Pirkimai_{today_str}.csv")
            response = HttpResponse(
                content,
                content_type='text/csv; charset=windows-1257'
            )
            response['Content-Disposition'] = f'attachment; filename="{filename}"'
            export_success = True

        # Ð•ÑÐ»Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ pardavimai
        elif debetas_result.get("pardavimai"):
            content = debetas_result["pardavimai"]
            filename = debetas_result.get("pardavimai_filename", f"Debetas_Pardavimai_{today_str}.csv")
            response = HttpResponse(
                content,
                content_type='text/csv; charset=windows-1257'
            )
            response['Content-Disposition'] = f'attachment; filename="{filename}"'
            export_success = True

        else:
            logger.warning("[EXP] DEBETAS nothing to export (empty result dict)")
            response = Response({"error": "No documents to export"}, status=400)


    # ========================= RIVILÄ– ERP (XLSX) =========================
    elif export_type == 'rivile_erp':
        logger.info("[EXP] RIVILE_ERP export started")
        assign_random_prekes_kodai(documents)
        rivile_defaults = getattr(request.user, "rivile_erp_extra_fields", None) or {}

        klientai = []
        seen = set()

        for pack in (prepared.get("pirkimai", []) + prepared.get("pardavimai", [])):
            doc = pack["doc"]
            dir_ = pack.get("direction")

            if dir_ == 'pirkimas':
                is_person = doc.seller_is_person
                klient_type = 'pirkimas'
                # ÐšÐ¾Ð´ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°: id â†’ vat_code â†’ id_programoje (ÐºÐ°Ðº Ð² get_party_code)
                client_code = doc.seller_id or doc.seller_vat_code or doc.seller_id_programoje or ""
                client = {
                    'id': client_code,
                    'vat': doc.seller_vat_code or "",
                    'name': doc.seller_name or "",
                    'address': doc.seller_address or "",
                    'country_iso': doc.seller_country_iso or "",
                    'currency': doc.currency or "EUR",
                    'kodas_ds': 'PT001',
                    'type': klient_type,
                    'is_person': is_person,
                    'iban': doc.seller_iban or "",
                }
            elif dir_ == 'pardavimas':
                is_person = doc.buyer_is_person
                klient_type = 'pardavimas'
                # ÐšÐ¾Ð´ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°: id â†’ vat_code â†’ id_programoje (ÐºÐ°Ðº Ð² get_party_code)
                client_code = doc.buyer_id or doc.buyer_vat_code or doc.buyer_id_programoje or ""
                client = {
                    'id': client_code,
                    'vat': doc.buyer_vat_code or "",
                    'name': doc.buyer_name or "",
                    'address': doc.buyer_address or "",
                    'country_iso': doc.buyer_country_iso or "",
                    'currency': doc.currency or "EUR",
                    'kodas_ds': 'PT001',
                    'type': klient_type,
                    'is_person': is_person,
                    'iban': doc.buyer_iban or "",
                }
            else:
                continue

            client_key = (client['id'], client['vat'], client['name'], client['type'])
            if client['id'] and client_key not in seen:
                klientai.append(client)
                seen.add(client_key)

        logger.info("[EXP] RIVILE_ERP klientai=%d docs_pirk=%d docs_pard=%d",
                    len(klientai), len(pirkimai_docs), len(pardavimai_docs))

        files_to_export = []

        if klientai:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
                export_clients_to_rivile_erp_xlsx(klientai, tmp.name)
                tmp.seek(0)
                klientai_xlsx_bytes = tmp.read()
            files_to_export.append((f'klientai_{today_str}.xlsx', klientai_xlsx_bytes))

        with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
            export_prekes_and_paslaugos_to_rivile_erp_xlsx(documents, tmp.name)
            tmp.seek(0)
            prekes_xlsx_bytes = tmp.read()
        files_to_export.append((f'prekes_paslaugos_{today_str}.xlsx', prekes_xlsx_bytes))

        if pirkimai_docs:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
                export_documents_to_rivile_erp_xlsx(
                    pirkimai_docs,
                    tmp.name,
                    doc_type="pirkimai",
                    rivile_erp_extra_fields=rivile_defaults,  # ðŸ”¹ Ð²Ð¾Ñ‚ Ð·Ð´ÐµÑÑŒ
                )
                tmp.seek(0)
                pirkimai_xlsx_bytes = tmp.read()
            files_to_export.append((f'pirkimai_{today_str}.xlsx', pirkimai_xlsx_bytes))

        if pardavimai_docs:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
                export_documents_to_rivile_erp_xlsx(
                    pardavimai_docs,
                    tmp.name,
                    doc_type="pardavimai",
                    rivile_erp_extra_fields=rivile_defaults,  # ðŸ”¹ Ð¸ Ð·Ð´ÐµÑÑŒ
                )
                tmp.seek(0)
                pardavimai_xlsx_bytes = tmp.read()
            files_to_export.append((f'pardavimai_{today_str}.xlsx', pardavimai_xlsx_bytes))

        logger.info("[EXP] RIVILE_ERP files_to_export=%s", [n for n, _ in files_to_export])

        if len(files_to_export) > 1:
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w") as zf:
                for filename, file_bytes in files_to_export:
                    zf.writestr(filename, file_bytes)
            zip_buffer.seek(0)
            response = HttpResponse(zip_buffer.read(), content_type='application/zip')
            response['Content-Disposition'] = f'attachment; filename={today_str}_rivile_erp.zip'
            export_success = True
        elif len(files_to_export) == 1:
            filename, file_bytes = files_to_export[0]
            response = HttpResponse(
                file_bytes,
                content_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
            )
            response['Content-Disposition'] = f'attachment; filename={filename}'
            export_success = True
        else:
            logger.warning("[EXP] RIVILE_ERP nothing to export")
            response = Response({"error": "No clients or products to export"}, status=400)




    else:
        logger.error("[EXP] unknown export type: %s", export_type)
        return Response({"error": "Unknown export type"}, status=400)

    # --- universal finalize ---
    if response is not None:
        try:
            if export_success and exported_ids:
                ScannedDocument.objects.filter(pk__in=exported_ids).update(status="exported")
                logger.info("[EXP] Marked %d documents as exported (universal)", len(exported_ids))
        except Exception as e:
            logger.warning("[EXP] Failed to mark documents as exported: %s", e)
        return response

    logger.warning("[EXP] fell through unexpectedly")
    return Response({"error": "No documents to export"}, status=400)



# Soxranenije user infy s Dineta
class DinetaSettingsView(APIView):
    permission_classes = [IsAuthenticated]

    def get(self, request):
        """
        Ð’ÐµÑ€Ð½ÑƒÑ‚ÑŒ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Dineta ÑÑ‚Ð¾Ð³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.
        ÐŸÐ°Ñ€Ð¾Ð»ÑŒ ÐÐ• Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ÑÑ.
        """
        user = request.user
        settings_dict = user.dineta_settings or {}

        serializer = DinetaSettingsSerializer(instance=settings_dict)
        return Response(serializer.data, status=status.HTTP_200_OK)

    def put(self, request):
        """
        ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Dineta.
        Ð¤Ñ€Ð¾Ð½Ñ‚ ÑˆÐ»Ñ‘Ñ‚ server/client/username/password (+ Ð¾Ð¿Ñ†Ð¸Ð¸).
        """
        user = request.user

        serializer = DinetaSettingsSerializer(data=request.data)
        serializer.is_valid(raise_exception=True)

        settings_to_store = serializer.build_settings_dict()

        user.dineta_settings = settings_to_store
        user.save(update_fields=["dineta_settings"])

        # Ð² Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚Ð´Ð°Ñ‘Ð¼ Ð±ÐµÐ· Ð¿Ð°Ñ€Ð¾Ð»Ñ (serializer.instance â†’ dict)
        response_serializer = DinetaSettingsSerializer(instance=settings_to_store)
        return Response(response_serializer.data, status=status.HTTP_200_OK)



# Soxranenije user infy s Optimum i do soxranenija delajet probnyj Hello test
class OptimumSettingsView(APIView):
    permission_classes = [IsAuthenticated]

    def put(self, request):
        """
        Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ A:
        - user Ð²Ð²Ð¾Ð´Ð¸Ñ‚ key -> backend Ð´ÐµÐ»Ð°ÐµÑ‚ Hello
        - ÐµÑÐ»Ð¸ Success: ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ key + verified_at + last_ok=true, Ñ‡Ð¸ÑÑ‚Ð¸Ð¼ last_error*
        - ÐµÑÐ»Ð¸ Error: key ÐÐ• ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼, Ð½Ð¾ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ last_ok=false + last_error_at + last_error
        """
        user = request.user

        serializer = OptimumSettingsSerializer(data=request.data)
        serializer.is_valid(raise_exception=True)

        raw_key = (serializer.validated_data.get("key") or "").strip()
        now_iso = timezone.now().isoformat()

        try:
            optimum_hello(raw_key)
        except OptimumError as exc:
            # --- ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ (key Ð½Ðµ Ñ‚Ñ€Ð¾Ð³Ð°ÐµÐ¼) ---
            patch = OptimumSettingsSerializer.build_error_patch(
                error_at=now_iso,
                error_msg=str(exc) or "Optimum: klaida",
            )

            current = user.optimum_settings or {}
            current.update(patch)
            user.optimum_settings = current
            user.save(update_fields=["optimum_settings"])

            # Ñ„Ñ€Ð¾Ð½Ñ‚Ñƒ Ð¾Ñ‚Ð´Ð°Ñ‘Ð¼ Ð¿Ð¾Ð½ÑÑ‚Ð½ÑƒÑŽ Ð¾ÑˆÐ¸Ð±ÐºÑƒ
            return Response(
                {"detail": patch["last_error"] or "Nepavyko patikrinti Optimum API Key."},
                status=status.HTTP_400_BAD_REQUEST,
            )

        # --- SUCCESS: ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ð¹ key + Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ ---
        settings_to_store = serializer.build_success_settings_dict(verified_at=now_iso)

        user.optimum_settings = settings_to_store
        user.save(update_fields=["optimum_settings"])

        response_serializer = OptimumSettingsSerializer(instance=settings_to_store)
        return Response(response_serializer.data, status=status.HTTP_200_OK)







@api_view(['POST'])
@permission_classes([IsAuthenticated])
def process_image(request):
    raw_files = request.FILES.getlist("files")
    scan_type = request.data.get("scan_type", "sumiskai")

    if not raw_files:
        return Response({'error': 'Ð¤Ð°Ð¹Ð»Ñ‹ Ð½Ðµ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ñ‹'}, status=400)

    user = request.user

    # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ñ†ÐµÐ½Ñƒ Ð·Ð° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚
    if scan_type == "detaliai":
        credits_per_doc = Decimal("1.3")
    else:
        credits_per_doc = Decimal("1")

    files_count = len(raw_files)

    # --- ÐŸÐ ÐžÐ’Ð•Ð ÐšÐ ÐºÑ€ÐµÐ´Ð¸Ñ‚Ð¾Ð² Ð”Ðž Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ---
    if user.credits < credits_per_doc * files_count:
        return Response({
            'error': f'Nepakanka kreditÅ³. Liko â€“ {user.credits}, reikia â€“ {credits_per_doc * files_count}.'
        }, status=402)

    results = []
    for raw_file in raw_files:
        original_filename = raw_file.name

        # 1. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð·Ð°Ð¿Ð¸ÑÑŒ Ð² Ð‘Ð” ÑÑ€Ð°Ð·Ñƒ!
        doc = ScannedDocument.objects.create(
            user=user,
            original_filename=original_filename,
            status='processing',
            scan_type=scan_type
        )
        doc.file.save(original_filename, raw_file)
        doc.save()

        # 2. Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ celery-task c ID
        process_uploaded_file_task.delay(
            user.id,
            doc.id,
            scan_type
        )

        results.append({
            "id": doc.id,
            "original_filename": original_filename,
            "status": "processing",
            "uploaded_at": doc.uploaded_at
        })

    return Response({
        'status': 'processing',
        'results': results,
        'msg': 'Dokumentai uÅ¾registruoti ir apdorojami. Po keliÅ³ sekundÅ¾iÅ³ statusas atsinaujins.'
    })






# Obnovit company details v Nustatymai

@api_view(['PATCH'])
@permission_classes([IsAuthenticated])
def update_own_company_details(request):
    user = request.user
    serializer = CustomUserSerializer(user, data=request.data, partial=True)
    if serializer.is_valid():
        serializer.save()
        return Response(serializer.data)
    return Response(serializer.errors, status=400)



# Udaliajet zapisi s dashboard i BD

@api_view(['DELETE'])
@permission_classes([IsAuthenticated])
def bulk_delete_documents(request):
    ids = request.data.get('ids', [])
    if not ids:
        return Response({'error': 'No IDs provided'}, status=status.HTTP_400_BAD_REQUEST)
    # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
    queryset = ScannedDocument.objects.filter(id__in=ids, user=request.user)
    deleted, _ = queryset.delete()
    return Response({'deleted': deleted}, status=status.HTTP_200_OK)




# #poluciajem vsio infu iz BD dlia otobrazhenija v dashboard pri zagruzke

# /documents/
@api_view(['GET'])
@permission_classes([IsAuthenticated])
def get_user_documents(request):
    user = request.user
    status = request.GET.get('status')
    date_from = request.GET.get('from')
    date_to = request.GET.get('to')

    docs = ScannedDocument.objects.filter(user=user)
    if status:
        docs = docs.filter(status=status)
    if date_from:
        docs = docs.filter(created_at__date__gte=parse_date(date_from))
    if date_to:
        docs = docs.filter(created_at__date__lte=parse_date(date_to))

    serializer = ScannedDocumentListSerializer(docs.order_by('-uploaded_at'), many=True)
    return Response(serializer.data)





# @api_view(['GET'])
# @permission_classes([IsAuthenticated])
# def get_document_detail(request, pk):
#     """
#     Ð”ÐµÑ‚Ð°Ð»Ð¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°.
#     - Superuser: Ð¼Ð¾Ð¶ÐµÑ‚ ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ð›Ð®Ð‘ÐžÐ™ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚; ÐµÑÐ»Ð¸ view_mode == "multi", Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ preview.
#     - ÐžÐ±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ:
#         * single-Ñ€ÐµÐ¶Ð¸Ð¼ â€” Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Ð‘Ð”;
#         * multi-Ñ€ÐµÐ¶Ð¸Ð¼ â€” Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ preview (Ð½Ð¸Ñ‡ÐµÐ³Ð¾ Ð² Ð‘Ð” Ð½Ðµ Ð¿Ð¸ÑˆÐµÐ¼).
#     """
#     user = request.user

#     # --- Ð¡ÑƒÐ¿ÐµÑ€ÑŽÐ·ÐµÑ€ ---
#     if user.is_superuser:
#         doc = get_object_or_404(ScannedDocument, pk=pk)
#         ser = ScannedDocumentAdminDetailSerializer(doc, context={'request': request})
#         data = ser.data

#         if getattr(user, "view_mode", None) == "multi":
#             cp_key = request.query_params.get("cp_key")
#             preview = build_preview(
#                 doc,
#                 user,
#                 cp_key=cp_key,
#                 view_mode="multi",
#                 base_vat_percent=data.get("vat_percent"),
#                 base_preke_paslauga=data.get("preke_paslauga"),
#             )
#             data["preview"] = preview

#         return Response(data)

#     # --- ÐžÐ±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÐ²Ð¾Ð¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹) ---
#     doc = get_object_or_404(ScannedDocument, pk=pk, user=user)
#     ser = ScannedDocumentDetailSerializer(doc, context={'request': request})
#     data = ser.data

#     # preview Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð² multi-Ñ€ÐµÐ¶Ð¸Ð¼Ðµ
#     if getattr(user, "view_mode", None) != "multi":
#         return Response(data)

#     cp_key = request.query_params.get("cp_key")
#     preview = build_preview(
#         doc,
#         user,
#         cp_key=cp_key,
#         view_mode="multi",
#         base_vat_percent=data.get("vat_percent"),
#         base_preke_paslauga=data.get("preke_paslauga"),
#     )
#     data["preview"] = preview
#     return Response(data)

@api_view(['GET'])
@permission_classes([IsAuthenticated])
def get_document_detail(request, pk):
    """
    Ð”ÐµÑ‚Ð°Ð»Ð¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°.
    - Superuser: Ð¼Ð¾Ð¶ÐµÑ‚ ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ð›Ð®Ð‘ÐžÐ™ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚; ÐµÑÐ»Ð¸ view_mode == "multi", Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ preview.
    - ÐžÐ±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ:
        * single-Ñ€ÐµÐ¶Ð¸Ð¼ â€” Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Ð‘Ð”;
        * multi-Ñ€ÐµÐ¶Ð¸Ð¼ â€” Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ preview (Ð½Ð¸Ñ‡ÐµÐ³Ð¾ Ð² Ð‘Ð” Ð½Ðµ Ð¿Ð¸ÑˆÐµÐ¼).
    """
    user = request.user

    # Prefetch Ñ ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ¾Ð¹ Ð´Ð»Ñ line_items
    line_items_prefetch = Prefetch(
        'line_items',
        queryset=LineItem.objects.order_by('id')
    )

    # --- Ð¡ÑƒÐ¿ÐµÑ€ÑŽÐ·ÐµÑ€ ---
    if user.is_superuser:
        doc = get_object_or_404(
            ScannedDocument.objects.prefetch_related(line_items_prefetch),
            pk=pk
        )
        ser = ScannedDocumentAdminDetailSerializer(doc, context={'request': request})
        data = ser.data

        if getattr(user, "view_mode", None) == "multi":
            cp_key = request.query_params.get("cp_key")
            preview = build_preview(
                doc,
                user,
                cp_key=cp_key,
                view_mode="multi",
                base_vat_percent=data.get("vat_percent"),
                base_preke_paslauga=data.get("preke_paslauga"),
            )
            data["preview"] = preview

        return Response(data)

    # --- ÐžÐ±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÐ²Ð¾Ð¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹) ---
    doc = get_object_or_404(
        ScannedDocument.objects.prefetch_related(line_items_prefetch),
        pk=pk,
        user=user
    )
    ser = ScannedDocumentDetailSerializer(doc, context={'request': request})
    data = ser.data

    # preview Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð² multi-Ñ€ÐµÐ¶Ð¸Ð¼Ðµ
    if getattr(user, "view_mode", None) != "multi":
        return Response(data)

    cp_key = request.query_params.get("cp_key")
    preview = build_preview(
        doc,
        user,
        cp_key=cp_key,
        view_mode="multi",
        base_vat_percent=data.get("vat_percent"),
        base_preke_paslauga=data.get("preke_paslauga"),
    )
    data["preview"] = preview
    return Response(data)


# @api_view(['GET'])
# @permission_classes([IsAuthenticated])
# def get_document_detail(request, pk):
#     """
#     Ð”ÐµÑ‚Ð°Ð»Ð¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°.
#     - Ð’ single-Ñ€ÐµÐ¶Ð¸Ð¼Ðµ: Ð¾Ñ‚Ð´Ð°ÐµÐ¼ ÑÐµÑ€Ð¸Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Ð‘Ð” (Ð±ÐµÐ· preview).
#     - Ð’ multi-Ñ€ÐµÐ¶Ð¸Ð¼Ðµ: Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ preview Ñ‡ÐµÑ€ÐµÐ· data_resolver.build_preview (Ð½Ð¸Ñ‡ÐµÐ³Ð¾ Ð² Ð‘Ð” Ð½Ðµ Ð¿Ð¸ÑˆÐµÐ¼).
#     """
#     try:
#         doc = ScannedDocument.objects.get(pk=pk, user=request.user)
#     except ScannedDocument.DoesNotExist:
#         return Response({'error': 'Not found'}, status=404)

#     serializer = ScannedDocumentDetailSerializer(doc)
#     data = serializer.data

#     # ÐŸÑ€ÐµÐ²ÑŒÑŽ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð² multi
#     if getattr(request.user, "view_mode", None) != "multi":
#         return Response(data)

#     cp_key = request.query_params.get("cp_key")
#     preview = build_preview(
#         doc,
#         request.user,
#         cp_key=cp_key,
#         view_mode="multi",
#         base_vat_percent=data.get("vat_percent"),
#         base_preke_paslauga=data.get("preke_paslauga"),
#     )

#     data["preview"] = preview
#     return Response(data)






#Obnovit extra field vzavisimosti ot vybranoj buh programy
@api_view(['PATCH'])
@permission_classes([IsAuthenticated])
def update_scanned_document_extra_fields(request, pk):
    import logging
    from django.db import transaction
    from .models import ScannedDocument, LineItem
    from .serializers import ScannedDocumentSerializer
    from .utils.pirkimas_pardavimas import determine_pirkimas_pardavimas
    from .validators.vat_klas import auto_select_pvm_code
    from .utils.save_document import _apply_sumiskai_defaults_from_user
    from .validators.required_fields_checker import check_required_fields_for_export  # Ð”ÐžÐ‘ÐÐ’Ð˜Ð¢Ð¬

    log = logging.getLogger("docscanner_app.api.update_extra_fields")

    doc = ScannedDocument.objects.filter(pk=pk, user=request.user).first()
    if not doc:
        log.warning("PATCH extra_fields pk=%s: document not found for user=%s", pk, request.user.id)
        return Response({'error': 'Dokumentas nerastas'}, status=404)

    ALLOWED_FIELDS = [
        'buyer_id', 'buyer_name', 'buyer_vat_code', 'buyer_iban', 'buyer_address', 'buyer_country_iso',
        'seller_id', 'seller_name', 'seller_vat_code', 'seller_iban', 'seller_address', 'seller_country_iso',
        'prekes_kodas', 'prekes_barkodas', 'prekes_pavadinimas', 'preke_paslauga',
        'vat_percent', 'scan_type', 'doc_96_str',
    ]

    # helpers
    # def _is_cleared(prefix: str) -> bool:
    #     keys = [
    #         f"{prefix}_name", f"{prefix}_id", f"{prefix}_vat_code",
    #         f"{prefix}_iban", f"{prefix}_address", f"{prefix}_country_iso",
    #     ]
    #     touched = any(k in request.data for k in keys)
    #     if not touched:
    #         return False
    #     return all(not str(request.data.get(k) or "").strip() for k in keys)

    def _is_cleared(prefix: str) -> bool:
        keys = [
            f"{prefix}_name", f"{prefix}_id", f"{prefix}_vat_code",
            f"{prefix}_iban", f"{prefix}_address", f"{prefix}_country_iso",
        ]
        provided = [k for k in keys if k in request.data]  # Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ Ð¿Ñ€Ð¸ÑÐ»Ð°Ð½Ð½Ñ‹Ðµ
        if not provided:
            return False
        return all(not str(request.data.get(k) or "").strip() for k in provided)

    def _to_bool_allow(x):
        if x is None: return None
        if isinstance(x, bool): return x
        s = str(x).strip().lower()
        if s in {"0","false","no","ne","off"}: return False
        if s in {"1","true","taip","yes","on"}: return True
        return None

    def _normalize_ps(v):
        if v is None: return None
        if isinstance(v, int): return v if v in (1,2,3,4) else None
        s = str(v).strip()
        return int(s) if s.isdigit() and int(s) in (1,2,3,4) else None

    def _normalize_vat_percent(v):
        if v is None: return None
        try:
            from decimal import Decimal
            if isinstance(v, Decimal): return float(v)
            s = str(v).strip().replace(",", ".")
            if not s: return None
            if s.endswith("%"): s = s[:-1]
            return float(Decimal(s))
        except Exception:
            return None

    def _nz(s):
        if s is None: return None
        s2 = str(s).strip()
        return s2 if s2 else None

    # Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ (ÑÑ‹Ñ€Ð¾), Ð»Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼
    fields_to_update = []
    for field in ALLOWED_FIELDS:
        if field in request.data:
            old_val = getattr(doc, field, None)
            new_val = request.data[field]
            setattr(doc, field, new_val)
            fields_to_update.append(field)
            if str(old_val) != str(new_val):
                log.info("pk=%s: field %s changed: %r -> %r", pk, field, old_val, new_val)

    buyer_cleared = _is_cleared("buyer")
    seller_cleared = _is_cleared("seller")
    if buyer_cleared or seller_cleared:
        log.info("pk=%s: clear detected: buyer_cleared=%s seller_cleared=%s", pk, buyer_cleared, seller_cleared)

    apply_defaults_req = _to_bool_allow(request.data.get("apply_defaults", None))
    log.info("pk=%s: apply_defaults_req=%r", pk, apply_defaults_req)

    with transaction.atomic():
        # 0) Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð¸ÑÐ»Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ð¾Ð»Ñ
        if fields_to_update:
            doc.save(update_fields=fields_to_update)

        # 1) ÐŸÐµÑ€ÐµÑÑ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ pirkimas/pardavimas
        doc_struct = {
            "seller_id": doc.seller_id,
            "seller_vat_code": doc.seller_vat_code,
            "seller_name": doc.seller_name,
            "buyer_id": doc.buyer_id,
            "buyer_vat_code": doc.buyer_vat_code,
            "buyer_name": doc.buyer_name,
        }
        doc.pirkimas_pardavimas = determine_pirkimas_pardavimas(doc_struct, request.user)
        log.info("pk=%s: pirkimas_pardavimas=%r", pk, doc.pirkimas_pardavimas)

        # 1.1) Ð¤Ð»Ð°Ð³Ð¸ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ñ VAT ÐºÐ¾Ð´Ð°
        buyer_has_vat_code = bool((doc.buyer_vat_code or "").strip())
        seller_has_vat_code = bool((doc.seller_vat_code or "").strip())
        if hasattr(doc, "buyer_has_vat_code"):
            doc.buyer_has_vat_code = buyer_has_vat_code
        if hasattr(doc, "seller_has_vat_code"):
            doc.seller_has_vat_code = seller_has_vat_code
        log.info("pk=%s: buyer_has_vat_code=%s seller_has_vat_code=%s", pk, buyer_has_vat_code, seller_has_vat_code)

        # 1.2) Ð•Ð¡Ð›Ð˜ Ð¾Ñ‡Ð¸Ñ‰Ð°ÐµÐ¼ buyer/seller â€” Ñ‡Ð¸ÑÑ‚Ð¸Ð¼ Ñ‚Ð¾Ð²Ð°Ñ€Ð½Ñ‹Ðµ Ð¿Ð¾Ð»Ñ Ð¸ PVM Ð˜ Ð’Ð«Ð¥ÐžÐ”Ð˜Ðœ Ð ÐÐÐž
        if buyer_cleared or seller_cleared:
            doc.prekes_pavadinimas = ""
            doc.prekes_kodas = ""
            doc.prekes_barkodas = ""
            doc.preke_paslauga = ""
            doc.pvm_kodas = None
            update_fields_now = ["prekes_pavadinimas","prekes_kodas","prekes_barkodas","preke_paslauga","pvm_kodas","pirkimas_pardavimas"]

            if hasattr(doc, "buyer_has_vat_code"): update_fields_now.append("buyer_has_vat_code")
            if hasattr(doc, "seller_has_vat_code"): update_fields_now.append("seller_has_vat_code")

            if (doc.scan_type or "").strip().lower() == "detaliai":
                cleared = LineItem.objects.filter(document=doc).update(pvm_kodas=None)
                log.info("pk=%s: cleared LineItem.pvm_kodas for %d items", pk, cleared)

            doc.save(update_fields=update_fields_now)
            
            # ============ Ð”ÐžÐ‘ÐÐ’Ð˜Ð¢Ð¬ Ð’ÐÐ›Ð˜Ð”ÐÐ¦Ð˜Ð® ÐŸÐ•Ð Ð•Ð” Ð ÐÐÐÐ˜Ðœ Ð’ÐžÐ—Ð’Ð ÐÐ¢ÐžÐœ ============
            try:
                is_valid = check_required_fields_for_export(doc)
                doc.ready_for_export = is_valid
                doc.save(update_fields=['ready_for_export'])
                log.info("pk=%s: validated after clear, ready_for_export=%s", pk, is_valid)
            except Exception as e:
                log.error("pk=%s: validation error after clear: %s", pk, str(e))
            # ====================================================================
            
            log.info("pk=%s: PVM cleared due to party clear, early return", pk)
            return Response(ScannedDocumentSerializer(doc).data)

        # 2) ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð´ÐµÑ„Ð¾Ð»Ñ‚Ñ‹ (sumiskai, ÐµÑÐ»Ð¸ Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¾)
        scan_type = (doc.scan_type or "").strip().lower()
        allow_defaults = (scan_type == "sumiskai" and (apply_defaults_req is None or apply_defaults_req is True))
        if allow_defaults:
            changed = _apply_sumiskai_defaults_from_user(doc, request.user)
            log.info("pk=%s: defaults applied=%s", pk, changed)
            if changed:
                doc.save(update_fields=["prekes_pavadinimas","prekes_kodas","prekes_barkodas","preke_paslauga"])

        # 3) ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð»Ñ Ñ€Ð°ÑÑ‡Ñ‘Ñ‚Ð°
        buyer_iso = _nz(doc.buyer_country_iso)
        seller_iso = _nz(doc.seller_country_iso)
        doc_vat_norm = _normalize_vat_percent(doc.vat_percent)
        doc_ps = _normalize_ps(doc.preke_paslauga)

        log.info("pk=%s: buyer_iso=%r seller_iso=%r vat_percent_norm=%r preke_paslauga_norm=%r",
                 pk, buyer_iso, seller_iso, doc_vat_norm, doc_ps)

        # Ñ‚Ñ€ÐµÐ±ÑƒÐµÐ¼ ÑÑ‚Ñ€Ð°Ð½Ñ‹/Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ 0%
        need_countries_doc = (doc_vat_norm == 0.0)
        missing_crit = need_countries_doc and (
            doc.pirkimas_pardavimas not in ("pirkimas", "pardavimas") or not (buyer_iso and seller_iso)
        )
        log.info("pk=%s: need_countries_doc=%s missing_crit=%s", pk, need_countries_doc, missing_crit)

        # 4) ÐŸÐµÑ€ÐµÑÑ‡Ñ‘Ñ‚ PVM
        if scan_type == "detaliai":
            items = list(LineItem.objects.filter(document=doc))
            pvm_codes = set()
            vat_percents = set()
            log.info("pk=%s: recalc items count=%d", pk, len(items))

            for item in items:
                item_vat = item.vat_percent if item.vat_percent is not None else doc.vat_percent
                item_vat_norm = _normalize_vat_percent(item_vat)
                item_ps = _normalize_ps(item.preke_paslauga)
                if item_ps is None:
                    item_ps = doc_ps

                item_pvm = auto_select_pvm_code(
                    pirkimas_pardavimas=doc.pirkimas_pardavimas,
                    buyer_country_iso=buyer_iso,
                    seller_country_iso=seller_iso,
                    preke_paslauga=item_ps,
                    vat_percent=item_vat_norm,
                    separate_vat=bool(doc.separate_vat),
                    buyer_has_vat_code=buyer_has_vat_code,
                    seller_has_vat_code=seller_has_vat_code,
                    doc_96_str=bool(getattr(doc, "doc_96_str", False)),
                )

                old = item.pvm_kodas
                item.pvm_kodas = item_pvm
                item.save(update_fields=["pvm_kodas"])
                log.info("pk=%s: item[%s] vat=%r ps=%r -> pvm %r (was %r)",
                         pk, item.id, item_vat_norm, item_ps, item_pvm, old)

                if item_pvm is not None: pvm_codes.add(item_pvm)
                if item_vat_norm is not None: vat_percents.add(item_vat_norm)

            if bool(doc.separate_vat):
                doc.pvm_kodas = "Keli skirtingi PVM"
                doc.vat_percent = None
                log.info("pk=%s: separate_vat=True -> doc.pvm_kodas='Keli skirtingi PVM'", pk)
            else:
                if len(pvm_codes) == 1 and len(vat_percents) == 1:
                    doc.pvm_kodas = next(iter(pvm_codes))
                    doc.vat_percent = next(iter(vat_percents))
                    log.info("pk=%s: unified items -> doc.pvm_kodas=%r vat_percent=%r",
                             pk, doc.pvm_kodas, doc.vat_percent)
                else:
                    doc.pvm_kodas = ""
                    doc.vat_percent = None
                    log.info("pk=%s: heterogeneous items -> doc.pvm_kodas cleared", pk)

        else:
            # sumiskai / detaliai Ð±ÐµÐ· ÑÑ‚Ñ€Ð¾Ðº â€” Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð½Ñ‹Ð¹ Ñ€Ð°ÑÑ‡Ñ‘Ñ‚
            doc_pvm = auto_select_pvm_code(
                pirkimas_pardavimas=doc.pirkimas_pardavimas,
                buyer_country_iso=buyer_iso,
                seller_country_iso=seller_iso,
                preke_paslauga=doc_ps,
                vat_percent=doc_vat_norm,
                separate_vat=bool(doc.separate_vat),
                buyer_has_vat_code=buyer_has_vat_code,
                seller_has_vat_code=seller_has_vat_code,
                doc_96_str=bool(getattr(doc, "doc_96_str", False)),
            )
            old_doc_pvm = doc.pvm_kodas
            doc.pvm_kodas = doc_pvm
            log.info("pk=%s: doc-level recalc -> pvm %r (was %r)", pk, doc_pvm, old_doc_pvm)

        # 5) Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ
        update_set = {"pirkimas_pardavimas","pvm_kodas","vat_percent"}
        if hasattr(doc, "buyer_has_vat_code"): update_set.add("buyer_has_vat_code")
        if hasattr(doc, "seller_has_vat_code"): update_set.add("seller_has_vat_code")

        doc.save(update_fields=list(update_set))
        log.info("pk=%s: saved fields=%s", pk, sorted(update_set))

    # ============ Ð”ÐžÐ‘ÐÐ’Ð˜Ð¢Ð¬ Ð’ÐÐ›Ð˜Ð”ÐÐ¦Ð˜Ð® Ð’ ÐšÐžÐÐ¦Ð• ============
    try:
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¸Ð·Ð¼ÐµÐ½Ð¸Ð»Ð¸ÑÑŒ Ð»Ð¸ Ð¿Ð¾Ð»Ñ buyer/seller
        buyer_seller_changed = any(
            k.startswith(('buyer_', 'seller_')) 
            for k in request.data.keys()
        )
        
        if buyer_seller_changed:
            is_valid = check_required_fields_for_export(doc)
            doc.ready_for_export = is_valid
            doc.save(update_fields=['ready_for_export'])
            log.info("pk=%s: validated after update, ready_for_export=%s", pk, is_valid)
    except Exception as e:
        log.error("pk=%s: validation error: %s", pk, str(e))
    # =====================================================

    return Response(ScannedDocumentSerializer(doc).data)





# Udaliajet produkt s doka
@api_view(['POST'])
@permission_classes([IsAuthenticated])
def clear_document_product(request, pk):
    from .models import ScannedDocument
    from .serializers import ScannedDocumentSerializer

    doc = ScannedDocument.objects.filter(pk=pk, user=request.user).first()
    if not doc:
        return Response({'error': 'Not found'}, status=404)

    # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾Ð»Ñ Ñ‚Ð¾Ð²Ð°Ñ€Ð°
    doc.prekes_pavadinimas = ""
    doc.prekes_kodas = ""
    doc.prekes_barkodas = ""
    doc.preke_paslauga = ""
    doc.save(update_fields=["prekes_pavadinimas", "prekes_kodas", "prekes_barkodas", "preke_paslauga"])

    return Response(ScannedDocumentSerializer(doc).data)



# Udaliajet produkt s lineitem
@api_view(['POST'])
@permission_classes([IsAuthenticated])
def clear_lineitem_product(request, pk, lineitem_id):
    from .models import ScannedDocument, LineItem
    from .serializers import LineItemSerializer

    doc = ScannedDocument.objects.filter(pk=pk, user=request.user).first()
    if not doc:
        return Response({'error': 'Document not found'}, status=404)

    item = LineItem.objects.filter(document=doc, pk=lineitem_id).first()
    if not item:
        return Response({'error': 'Line item not found'}, status=404)

    item.prekes_pavadinimas = ""
    item.prekes_kodas = ""
    item.prekes_barkodas = ""
    item.preke_paslauga = ""
    item.save(update_fields=["prekes_pavadinimas", "prekes_kodas", "prekes_barkodas", "preke_paslauga"])

    return Response(LineItemSerializer(item).data)



@api_view(['PATCH'])
@permission_classes([IsAuthenticated])
def update_view_mode(request):
    """
    PATCH /users/me/view-mode/
    Body: { "view_mode": "single" | "multi" }
    """
    serializer = ViewModeSerializer(data=request.data)
    serializer.is_valid(raise_exception=True)

    request.user.view_mode = serializer.validated_data['view_mode']
    request.user.save(update_fields=['view_mode'])

    return Response({'view_mode': request.user.view_mode}, status=status.HTTP_200_OK)




@api_view(['PATCH'])
@permission_classes([IsAuthenticated])
def update_lineitem_fields(request, doc_id, lineitem_id):
    from .serializers import LineItemSerializer  # ÑƒÐ±ÐµÐ´Ð¸ÑÑŒ, Ñ‡Ñ‚Ð¾ ÐµÑÑ‚ÑŒ
    doc = get_object_or_404(ScannedDocument, pk=doc_id, user=request.user)
    lineitem = get_object_or_404(LineItem, pk=lineitem_id, document=doc)

    allowed = ['prekes_kodas', 'prekes_pavadinimas', 'prekes_barkodas']
    for field in allowed:
        if field in request.data:
            setattr(lineitem, field, request.data[field])
    lineitem.save()

    return Response(LineItemSerializer(lineitem).data, status=200)





#Autocomplete funkcii

@api_view(['GET'])
@permission_classes([IsAuthenticated])
def autocomplete_products(request):
    query = request.GET.get('q', '').strip()
    qs = ProductAutocomplete.objects.filter(user=request.user)
    if query:
        qs = qs.filter(
            Q(prekes_pavadinimas__icontains=query) |
            Q(prekes_kodas__icontains=query) |
            Q(prekes_barkodas__icontains=query)
        )
    qs = qs.order_by('prekes_pavadinimas')[:30]  # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÑŒ 30, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð²ÑÑ‘
    data = [
        {
            "id": prod.id,
            "prekes_pavadinimas": prod.prekes_pavadinimas,
            "prekes_kodas": prod.prekes_kodas,
            "prekes_barkodas": prod.prekes_barkodas,
            # Ð´Ð¾Ð±Ð°Ð²ÑŒ Ð½ÑƒÐ¶Ð½Ñ‹Ðµ Ð¿Ð¾Ð»Ñ!
        }
        for prod in qs
    ]
    return Response(data)




@api_view(['GET'])
@permission_classes([IsAuthenticated])
def autocomplete_clients(request):
    query = request.GET.get('q', '').strip()
    qs = ClientAutocomplete.objects.filter(user=request.user)
    if query:
        qs = qs.filter(
            Q(pavadinimas__icontains=query) |
            Q(imones_kodas__icontains=query) |
            Q(pvm_kodas__icontains=query)
        )
    qs = qs.order_by('pavadinimas')[:30]
    data = [
        {
            "id": c.id,
            "pavadinimas": c.pavadinimas,
            "imones_kodas": c.imones_kodas,
            "pvm_kodas": c.pvm_kodas,
            "company_address": c.address,
            "company_country_iso": c.country_iso,
            "company_iban": c.ibans,
            # Ð½ÑƒÐ¶Ð½Ñ‹Ðµ Ð¿Ð¾Ð»Ñ
        }
        for c in qs
    ]
    return Response(data)





























# --- Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚ Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð² (products) ---
@api_view(["POST"])
@permission_classes([IsAuthenticated])
def import_products_view(request):
    file = request.FILES.get('file')
    if not file:
        return Response({"error": "No file provided."}, status=status.HTTP_400_BAD_REQUEST)
    try:
        report = import_products_from_xlsx(request.user, file)
        return Response(report, status=status.HTTP_200_OK)
    except Exception as e:
        return Response({"error": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# --- Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð² (clients) ---
@api_view(["POST"])
@permission_classes([IsAuthenticated])
def import_clients_view(request):
    file = request.FILES.get('file')
    if not file:
        return Response({"error": "No file provided."}, status=status.HTTP_400_BAD_REQUEST)
    try:
        report = import_clients_from_xlsx(request.user, file)
        return Response(report, status=status.HTTP_200_OK)
    except Exception as e:
        return Response({"error": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)






#Proverka ili obnovlenija default accounting program

@api_view(['GET', 'PATCH'])
@permission_classes([IsAuthenticated])
def user_profile(request):
    user = request.user
    if request.method == 'PATCH':
        old_company_code = user.company_code
        serializer = CustomUserSerializer(user, data=request.data, partial=True, context={'request': request})
        if serializer.is_valid():
            serializer.save()
            new_company_code = serializer.validated_data.get('company_code', old_company_code)

            # ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð½Ð°Ð¹Ñ‚Ð¸ ÑÑ‚Ð°Ñ€ÑƒÑŽ ÑÑ‚Ñ€Ð¾ÐºÑƒ Ð¿Ð¾ ÑÑ‚Ð°Ñ€Ð¾Ð¼Ñƒ company_code
            ca = ClientAutocomplete.objects.filter(
                user=user,
                imones_kodas=old_company_code
            ).first()

            if not ca:
                # Ð•ÑÐ»Ð¸ Ð½Ðµ Ð½Ð°ÑˆÐ»Ð¸ â€” Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð¿Ð¾ Ð½Ð¾Ð²Ð¾Ð¼Ñƒ
                ca = ClientAutocomplete.objects.filter(
                    user=user,
                    imones_kodas=new_company_code
                ).first()

            if not ca:
                # Ð•ÑÐ»Ð¸ Ð²ÑÑ‘ Ñ€Ð°Ð²Ð½Ð¾ Ð½Ðµ Ð½Ð°ÑˆÐ»Ð¸ â€” ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð½Ð¾Ð²ÑƒÑŽ
                ca = ClientAutocomplete(user=user, imones_kodas=new_company_code)

            # Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð²ÑÐµ Ð¿Ð¾Ð»Ñ:
            ca.pavadinimas = user.company_name
            ca.imones_kodas = new_company_code
            ca.pvm_kodas = user.vat_code
            ca.ibans = user.company_iban
            ca.address = user.company_address
            ca.country_iso = user.company_country_iso
            ca.save()
            
            return Response(serializer.data)
        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)
    serializer = CustomUserSerializer(user)
    return Response(serializer.data)





@api_view(['POST'])
@permission_classes([IsAdminUser])
def update_currency_rates_view(request):
    d = request.data.get('date')
    if d:
        try:
            from datetime import datetime
            d = datetime.strptime(d, '%Y-%m-%d').date()
        except Exception:
            return Response({'error': 'Invalid date'}, status=400)
    else:
        d = date.today()
    count = update_currency_rates(d)
    return Response({'message': f'Updated {count} currency rates for {d}.'})





@api_view(['GET'])
@permission_classes([IsAuthenticated])
def user_me_view(request):
    serializer = CustomUserSerializer(request.user)
    return Response(serializer.data)


class TrackAdClickView(generics.CreateAPIView):
    queryset = AdClick.objects.all()
    serializer_class = AdClickSerializer
    permission_classes = [permissions.AllowAny]  # Ð´Ð°Ð¶Ðµ Ð³Ð¾ÑÑ‚Ð¸ Ð¼Ð¾Ð³ÑƒÑ‚

    def create(self, request, *args, **kwargs):
        user = request.user if request.user.is_authenticated else None
        ip = request.META.get("REMOTE_ADDR")
        ua = request.META.get("HTTP_USER_AGENT", "")

        ad_click = AdClick.objects.create(
            ad_name=request.data.get("ad_name", "Unknown"),
            user=user,
            ip_address=ip,
            user_agent=ua
        )
        return Response({"status": "ok"})


#Skacivanje Apskaita5 plugina

FILE_PATH = "/var/files/DokSkenas_apskaita5_adapteris.zip"

def _sha256(path: str) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()

@api_view(['GET'])
@permission_classes([IsAuthenticated])  # Ð´Ð¾ÑÑ‚ÑƒÐ¿ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ñ€Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ…
def download_apskaita5_adapter(request):
    if not os.path.exists(FILE_PATH):
        raise Http404("Adapter not found")

    resp = FileResponse(open(FILE_PATH, "rb"))
    resp["Content-Type"] = "application/zip"
    resp["Content-Disposition"] = f'attachment; filename="{os.path.basename(FILE_PATH)}"'
    resp["X-Checksum-SHA256"] = _sha256(FILE_PATH)
    return resp




#JWT functions

class CustomTokenObtainPairView(TokenObtainPairView):
    def post(self, request, *args, **kwargs):
        
        try:
            response = super().post(request, *args, **kwargs)
            tokens = response.data

            access_token = tokens['access']
            refresh_token = tokens['refresh']

            res = Response()

            res.data = {'success':True}

            res.set_cookie(
                key="access_token",
                value=access_token,
                httponly=True,
                secure=True,
                samesite='Lax',
                path='/'
            )

            res.set_cookie(
                key="refresh_token",
                value=refresh_token,
                httponly=True,
                secure=True,
                samesite='Lax',
                path='/'
            )

            return res


        except:
            return Response({'success':False})

class CustomRefreshTokenView(TokenRefreshView):
    def post(self, request, *args, **kwargs):
        try:
            refresh_token = request.COOKIES.get('refresh_token')

            request.data['refresh'] = refresh_token

            response = super().post(request, *args, **kwargs)

            tokens = response.data
            access_token = tokens['access']

            res = Response()

            res.data = {'refreshed':True}

            res.set_cookie(
                key='access_token',
                value=access_token,
                httponly=True,
                secure=True,
                samesite='Lax',
                path='/'
            )

            return res

        except:
            return Response({'refreshed':False})


@api_view(['POST'])
def logout(request):
    try:
        res = Response()
        res.data = {'success':True}
        res.delete_cookie('access_token', path='/', samesite='Lax')
        res.delete_cookie('refresh_token', path='/', samesite='Lax')
        return res
    except:
        return Response({'success':False})
    

@api_view(['POST'])
@permission_classes([IsAuthenticated])
def is_authenticated(request):
    return Response({'authenticated':True})


# Funkcija dlia sozdanija trial

@permission_classes([AllowAny])
def create_trial_subscription(user):
    """
    Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ñ‚ Ð¿Ñ€Ð¾Ð±Ð½ÑƒÑŽ Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÑƒ Ð´Ð»Ñ Ð½Ð¾Ð²Ð¾Ð³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.
    """
    logger.info(f"ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ‚Ñ€Ð¸Ð°Ð»-Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÐ¸ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ: {user.email}")
    try:
        # Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ñ‚Ñ€Ð¸Ð°Ð»-Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÑƒ
        trial_start_date = timezone.now()
        trial_end_date = trial_start_date + timedelta(days=3000)

        user.subscription_status = 'trial'
        user.subscription_plan = 'trial'
        user.subscription_start_date = trial_start_date
        user.subscription_end_date = trial_end_date
        user.save()

        logger.info(f"Ð¢Ñ€Ð¸Ð°Ð»-Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÐ° ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½Ð° Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ: {user.email}")
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ð¸ Ñ‚Ñ€Ð¸Ð°Ð»-Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÐ¸ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {user.email}: {str(e)}")
        raise e



@api_view(['POST'])
@authentication_classes([])  # ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð°ÑƒÑ‚ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸
@permission_classes([AllowAny])  # Ð Ð°Ð·Ñ€ÐµÑˆÐ¸Ñ‚ÑŒ Ð²ÑÐµÐ¼ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑÐ¼ Ð´Ð¾ÑÑ‚ÑƒÐ¿ Ðº ÑÑ‚Ð¾Ð¼Ñƒ ÑÐ½Ð´Ð¿Ð¾Ð¸Ð½Ñ‚Ñƒ
def register(request):
    """
    Ð ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð½Ð¾Ð²Ð¾Ð³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð¸ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ‚Ñ€Ð¸Ð°Ð»-Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÐ¸.
    """
    logger.info("ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð½Ð° Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸ÑŽ Ð½Ð¾Ð²Ð¾Ð³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.")

    # Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ cookies Ñ Ñ‚Ð¾ÐºÐµÐ½Ð°Ð¼Ð¸
    if 'access_token' in request.COOKIES:
        logger.info("Ð£Ð´Ð°Ð»ÑÐµÐ¼ access_token Ð¸Ð· cookies.")
        del request.COOKIES['access_token']

    if 'refresh_token' in request.COOKIES:
        logger.info("Ð£Ð´Ð°Ð»ÑÐµÐ¼ refresh_token Ð¸Ð· cookies.")
        del request.COOKIES['refresh_token']

    try:
        serializer = CustomUserSerializer(data=request.data)
        if serializer.is_valid():
            logger.info("Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹.")

            # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
            user = serializer.save()
            logger.info(f"ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ {user.email} ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð½.")

            # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ñ‚Ñ€Ð¸Ð°Ð»-Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÑƒ Ð´Ð»Ñ Ð½Ð¾Ð²Ð¾Ð³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
            create_trial_subscription(user)

            # 3ï¸âƒ£ Ð¡Ñ‚Ð°Ð²Ð¸Ð¼ welcome email Ð² Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑŒ Celery (Ð¿Ð¾ÑÐ»Ðµ ÐºÐ¾Ð¼Ð¼Ð¸Ñ‚Ð°)

            try:
                t0 = perf_counter()
                siusti_sveikinimo_laiska(user)
                t1 = perf_counter()
                logger.info(f"Welcome email iÅ¡siÅ³stas vartotojui {user.email} per {t1 - t0:.4f}s (be Celery).")
            except Exception as mail_err:
                logger.exception(f"Nepavyko iÅ¡siÅ³sti welcome email be Celery: {mail_err}")

            # try:
            #     t_reg = perf_counter()

            #     def _enqueue():
            #         t0 = perf_counter()
            #         logger.info(f"[ENQUEUE] on_commit fired; start apply_async for {user.email}")
            #         try:
            #             task_siusti_sveikinimo_laiska.apply_async(args=[user.id], ignore_result=True)
            #         finally:
            #             t1 = perf_counter()
            #             logger.info(f"[ENQUEUE] apply_async_time={t1 - t0:.4f}s for {user.email}")

            #     transaction.on_commit(_enqueue)

            #     t_reg_done = perf_counter()
            #     logger.info(f"Queued welcome email for {user.email}. on_commit_registration_time={t_reg_done - t_reg:.4f}s")

            # except Exception as mail_err:
            #     logger.exception(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ welcome email Ð² Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑŒ: {mail_err}")

            return Response({
                "message": "User successfully registered!",
                "user": {
                    "id": user.id,
                    "email": user.email,
                    "subscription_status": user.subscription_status,
                    "subscription_plan": user.subscription_plan,
                    "subscription_start_date": user.subscription_start_date,
                    "subscription_end_date": user.subscription_end_date
                }
            }, status=201)

        logger.warning(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸: {serializer.errors}")
        return Response(serializer.errors, status=400)

    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ: {str(e)}")
        return Response({"error": "An error occurred during registration."}, status=500)




# Proveriajem status subscriptiona usera
@api_view(['GET'])
@permission_classes([IsAuthenticated])
def subscription_status(request):
    try:
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
        user = request.user

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ (CustomUser)
        user_profile = get_object_or_404(CustomUser, pk=user.pk)

        # Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¼ÐµÑ‚Ð¾Ð´ get_subscription_status Ð¸Ð· Ð¼Ð¾Ð´ÐµÐ»Ð¸ CustomUser
        status = user_profile.get_subscription_status()

        # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÐ¸
        return Response({'status': status}, status=200)

    except Exception as e:
        # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¾ÑˆÐ¸Ð±Ð¾Ðº Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‚ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ
        return Response({'error': str(e)}, status=500)
    




#DLIA SUPERUSEROV!!!:
#1) dlia admin-suvestine


def _ensure_dict(x):
    if x is None:
        return {}
    if isinstance(x, dict):
        return x
    if isinstance(x, str):
        try:
            return json.loads(x)
        except Exception:
            return {}
    return {}

# def summarize_doc_issues(doc_struct):
#     """
#     Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ 'error' Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð¸Ð¸ Ð¶ÐµÑÑ‚ÐºÐ¸Ñ… ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸ÐµÐ²:
#       - _check_minimum_anchors_ok == False
#       - _doc_amounts_consistent == False
#       - ar_sutapo == False Ð˜ (Ð»ÑŽÐ±Ð¾Ð¹ Ð¸Ð· _lines_sum_matches_wo/with/vat == False)
#       - 'ÐºÑ€Ð°ÑÐ½Ñ‹Ðµ' hints (DOC-LINES-NOT-MATCHING-*, LI-PRICE-MISMATCH, LI-ZERO-VAT-DISCOUNTS-MISMATCH)
#       - Ð² Ð»Ð¾Ð³Ð°Ñ… ÐµÑÑ‚ÑŒ ÑÑ‚Ñ€Ð¾ÐºÐ¸, Ð½Ð°Ñ‡Ð¸Ð½Ð°ÑŽÑ‰Ð¸ÐµÑÑ Ð½Ð° 'â—'
#     """
#     doc = _ensure_dict(doc_struct)

#     # --- Ð»Ð¾Ð³Ð¸ / Ñ…Ð¸Ð½Ñ‚Ñ‹ ---
#     logs = doc.get("_global_validation_log") or []
#     if isinstance(logs, str):
#         logs = [logs]
#     hints = doc.get("_lines_structured_hints") or []
#     if isinstance(hints, str):
#         hints = [hints]

#     bang = [s for s in logs if isinstance(s, str) and s.strip().startswith("â—")]
#     red_hints = [h for h in hints if (
#         isinstance(h, str) and (
#             h.startswith("DOC-LINES-NOT-MATCHING-")
#             or "LI-PRICE-MISMATCH" in h
#             or "LI-ZERO-VAT-DISCOUNTS-MISMATCH" in h
#         )
#     )]

#     # --- Ñ„Ð»Ð°Ð³Ð¸ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚Ð¸ ---
#     min_ok        = bool(doc.get("_check_minimum_anchors_ok", True))
#     doc_consistent= bool(doc.get("_doc_amounts_consistent", True))
#     ar_sutapo     = bool(doc.get("ar_sutapo", True))

#     match_wo   = doc.get("_lines_sum_matches_wo")
#     match_with = doc.get("_lines_sum_matches_with")
#     match_vat  = doc.get("_lines_sum_matches_vat")

#     separate_vat = bool(doc.get("separate_vat"))
#     vat_failed = (match_vat is False) if not separate_vat else False

#     lines_failed = (match_wo is False) or (match_with is False) or vat_failed
#     lines_block  = (not ar_sutapo) and lines_failed

#     # --- error-ÑƒÑÐ»Ð¾Ð²Ð¸Ñ ---
#     errors = []
#     if not min_ok:
#         errors.append("min-anchors")
#     if not doc_consistent:
#         errors.append("doc-core")
#     if lines_block:
#         errors.append("lines-vs-doc")
#     if red_hints:
#         errors.append("hints")
#     if bang:
#         errors.append("bang")

#     has_error = bool(errors)

#     # --- Ð¾Ñ„Ð¾Ñ€Ð¼Ð»ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð° ---
#     badges = []
#     if not min_ok:        badges.append("minâŠ„")
#     if not doc_consistent:badges.append("coreâœ—")
#     if lines_block:       badges.append("Î£(lines)â‰ doc")
#     if red_hints:         badges.append("hint!")
#     if bang:              badges.append(f"â—Ã—{len(bang)}")

#     # ÐºÑ€Ð°Ñ‚ÐºÐ°Ñ ÑÐ²Ð¾Ð´ÐºÐ°: Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚ â€” ÐºÑ€Ð°ÑÐ½Ñ‹Ð¹ Ñ…Ð¸Ð½Ñ‚ â†’ 'â—' â†’ Ð±ÐµÐ¹Ð´Ð¶Ð¸
#     summary = (red_hints[0] if red_hints else (bang[0] if bang else " ".join(badges)))[:300]

#     issue_count = int(len(red_hints) + len(bang)
#                       + (not min_ok) + (not doc_consistent) + (1 if lines_block else 0))

#     return {
#         "has_issues": has_error,
#         "severity": "error" if has_error else "ok",
#         "issue_badges": " ".join(badges),
#         "issue_summary": summary,
#         "issue_count": issue_count,
#     }

def _ensure_dict(x):
    if x is None:
        return {}
    if isinstance(x, dict):
        return x
    if isinstance(x, str):
        try:
            return json.loads(x)
        except Exception:
            return {}
    return {}

def summarize_doc_issues(doc_struct):
    """
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ 'error' Ð¢ÐžÐ›Ð¬ÐšÐž ÐµÑÐ»Ð¸ overall_status == "FAIL" Ð¸Ð· Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸.
    Ð­Ñ‚Ð¾ ÐµÐ´Ð¸Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸Ð¹ Ð´Ð»Ñ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð².
    """
    doc = _ensure_dict(doc_struct)

    # âœ… Ð•Ð”Ð˜ÐÐ¡Ð¢Ð’Ð•ÐÐÐÐ¯ ÐŸÐ ÐžÐ’Ð•Ð ÐšÐ: overall_status Ð¸Ð· Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸
    math_failed = False
    math_badge = None
    validation_type = None
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð´Ð»Ñ detaliai (Ñ line_items)
    final_validation = doc.get("_final_math_validation")
    if final_validation:
        overall = final_validation.get("summary", {}).get("overall_status")
        if overall == "FAIL":
            math_failed = True
            math_badge = "MATHâœ—"
            validation_type = "detaliai"
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð´Ð»Ñ sumiskai (Ð±ÐµÐ· line_items)
    sumiskai_validation = doc.get("_final_math_validation_sumiskai")
    if sumiskai_validation:
        overall = sumiskai_validation.get("overall_status")
        if overall == "FAIL":
            math_failed = True
            math_badge = "MATHâœ—"
            validation_type = "sumiskai"

    has_error = math_failed

    # --- Ð¾Ñ„Ð¾Ñ€Ð¼Ð»ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð° ---
    badges = []
    if math_badge:
        badges.append(math_badge)

    summary = " ".join(badges) if badges else ""
    if validation_type:
        summary = f"{summary} ({validation_type})".strip()

    issue_count = 1 if has_error else 0

    return {
        "has_issues": has_error,
        "severity": "error" if has_error else "ok",
        "issue_badges": " ".join(badges),
        "issue_summary": summary,
        "issue_count": issue_count,
    }


@api_view(['GET'])
@permission_classes([IsAuthenticated])
def admin_documents_with_errors(request):
    """Ð”Ð»Ñ superuser â€” Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð²ÑÐµÑ… Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹ Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°Ð¼Ð¸ (Ð²ÑÐµ, Ð±ÐµÐ· Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ Ð¿Ð¾ Ð´Ð°Ñ‚Ðµ)."""
    user = request.user
    if not user.is_superuser:
        return Response({"detail": "Not authorized"}, status=status.HTTP_403_FORBIDDEN)

    # Ñ„Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ ÑÑ‚Ð°Ñ‚ÑƒÑÑƒ â€” Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾
    status_filter = request.GET.get('status')

    qs = ScannedDocument.objects.all()

    if status_filter:
        qs = qs.filter(status=status_filter)

    # ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð¿Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ (Ð½Ð¾Ð²Ñ‹Ðµ Ð¿ÐµÑ€Ð²Ñ‹Ð¼Ð¸)
    qs = qs.order_by('-uploaded_at')

    ser = ScannedDocumentListSerializer(qs, many=True)

    data = []
    for obj, row in zip(qs, ser.data):
        doc_struct_raw = getattr(obj, 'structured_json', None) or getattr(obj, 'gpt_raw_json', None)
        issues = summarize_doc_issues(doc_struct_raw)
        if not issues["has_issues"]:
            continue

        r = dict(row)
        r["owner_email"]   = getattr(obj.user, "email", None)
        r["issue_has"]     = issues["has_issues"]
        r["issue_badges"]  = issues["issue_badges"]
        r["issue_summary"] = issues["issue_summary"]
        r["issue_count"]   = issues["issue_count"]
        data.append(r)

    return Response(data)


#2) dlia visi-failai
@api_view(['GET'])
@permission_classes([IsAuthenticated])
def admin_all_documents(request):
    """
    Ð”Ð»Ñ superuser â€” ÑÐ²Ð¾Ð´Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº Ð’Ð¡Ð•Ð¥ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð²ÑÐµÑ… Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹.
    Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ‹ user_id Ð¸ email.
    """
    user = request.user
    if not user.is_superuser:
        return Response({"detail": "Not authorized"}, status=status.HTTP_403_FORBIDDEN)

    qs = ScannedDocument.objects.select_related('user').all()

    # --- Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹ ---
    status_filter = request.GET.get('status')
    if status_filter:
        qs = qs.filter(status=status_filter)

    owner = request.GET.get('owner')
    if owner:
        qs = qs.filter(user__email__icontains=owner)

    from django.utils.dateparse import parse_datetime, parse_date
    from datetime import timedelta

    def _parse_any_dt(value):
        if not value:
            return None
        dt = parse_datetime(value)
        if dt:
            return dt
        d = parse_date(value)
        return d

    date_from = _parse_any_dt(request.GET.get('date_from'))
    date_to = _parse_any_dt(request.GET.get('date_to'))

    if date_from:
        qs = qs.filter(uploaded_at__gte=date_from)
    if date_to:
        # ÐµÑÐ»Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð°Ñ‚Ð° â€” Ð´Ð¾Ð±Ð°Ð²Ð¸Ð¼ ÐºÐ¾Ð½ÐµÑ† Ð´Ð½Ñ
        if hasattr(date_to, 'hour'):
            qs = qs.filter(uploaded_at__lt=date_to)
        else:
            qs = qs.filter(uploaded_at__lt=(date_to + timedelta(days=1)))

    # --- ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ° ---
    order = request.GET.get('order') or '-uploaded_at'
    if order not in {'uploaded_at', '-uploaded_at'}:
        order = '-uploaded_at'
    qs = qs.order_by(order)

    # --- Ð¿Ð°Ð³Ð¸Ð½Ð°Ñ†Ð¸Ñ ---
    from rest_framework.pagination import PageNumberPagination
    paginator = PageNumberPagination()
    paginator.page_size = int(request.GET.get('page_size', 50))
    page = paginator.paginate_queryset(qs, request)

    from .serializers import ScannedDocumentListSerializer
    ser = ScannedDocumentListSerializer(page, many=True)

    # --- Ð¾Ð±Ð¾Ð³Ð°Ñ‰ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… ---
    data = []
    for obj, row in zip(page, ser.data):
        doc_struct_raw = getattr(obj, 'structured_json', None) or getattr(obj, 'gpt_raw_json', None)
        issues = summarize_doc_issues(doc_struct_raw)

        # Ð²ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ user_id Ð¸ email Ð² Ð½Ð°Ñ‡Ð°Ð»Ð¾
        enriched_row = {
            "user_id": getattr(obj.user, "id", None),
            "owner_email": getattr(obj.user, "email", None),
        }
        enriched_row.update(row)

        # Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°Ñ…
        enriched_row["issue_has"] = issues["has_issues"]
        enriched_row["issue_badges"] = issues["issue_badges"]
        enriched_row["issue_summary"] = issues["issue_summary"]
        enriched_row["issue_count"] = issues["issue_count"]

        data.append(enriched_row)

    return paginator.get_paginated_response(data)


#3) Dlia users
@api_view(["GET"])
@permission_classes([IsAuthenticated])
def admin_users_simple(request):
    """
    Ð”Ð»Ñ superuser â€” ÑÐ¿Ð¸ÑÐ¾Ðº Ð²ÑÐµÑ… Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹ (CustomUser).
    Ð‘ÐµÐ· Ð¿Ð¾Ð´Ð¿Ð¸ÑÐ¾Ñ‡Ð½Ñ‹Ñ… Ð¿Ð¾Ð»ÐµÐ¹, Ð±ÐµÐ· Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð², Ð±ÐµÐ· Ð¿Ð°Ð³Ð¸Ð½Ð°Ñ†Ð¸Ð¸.
    """
    if not request.user.is_superuser:
        return Response({"detail": "Not authorized"}, status=status.HTTP_403_FORBIDDEN)

    qs = CustomUser.objects.all().order_by("-date_joined")
    ser = CustomUserAdminListSerializer(qs, many=True, context={'request': request})
    return Response(ser.data)



#Wagtail blog
class GuideCategoryViewSet(viewsets.ReadOnlyModelViewSet):
    """
    /guides-api/v2/guide-categories/                 -> ÑÐ¿Ð¸ÑÐ¾Ðº ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¹
    /guides-api/v2/guide-categories/<slug>/          -> ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ + articles[] (Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¾)
    /guides-api/v2/guide-categories/<slug>/articles/ -> Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÐ¿Ð¸ÑÐ¾Ðº ÑÑ‚Ð°Ñ‚ÐµÐ¹ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸
    """
    permission_classes = [AllowAny]
    lookup_field = "slug"
    queryset = GuideCategoryPage.objects.live().public().order_by("order", "title")

    def get_serializer_class(self):
        # list -> ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ ÑÐµÑ€Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€
        # retrieve -> Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ (Ñ Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ð¼ Ð¼Ð°ÑÑÐ¸Ð²Ð¾Ð¼ ÑÑ‚Ð°Ñ‚ÐµÐ¹)
        return (
            GuideCategoryDetailSerializer
            if self.action == "retrieve"
            else GuideCategoryListSerializer
        )

    @action(detail=True, methods=["get"], url_path="articles")
    def articles(self, request, slug=None):
        """
        Ð’ÐµÑ€Ð½Ñ‘Ñ‚ ÑÐ¿Ð¸ÑÐ¾Ðº ÑÑ‚Ð°Ñ‚ÐµÐ¹ Ð¾Ð´Ð½Ð¾Ð¹ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸ (ÑƒÐ´Ð¾Ð±Ð½Ð¾ Ð´Ð»Ñ Ð¿Ð°Ð³Ð¸Ð½Ð°Ñ†Ð¸Ð¸ Ñ„Ñ€Ð¾Ð½Ñ‚Ð°).
        GET-Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹: ?limit=12&offset=0
        """
        category = self.get_object()

        try:
            limit = int(request.query_params.get("limit", 100))
            offset = int(request.query_params.get("offset", 0))
        except ValueError:
            limit, offset = 100, 0

        qs = (
            GuidePage.objects.child_of(category)
            .live()
            .public()
            .specific()
            .order_by("-first_published_at")
        )
        total = qs.count()
        items = qs[offset : offset + limit]

        data = GuideArticleListSerializer(items, many=True, context={"request": request}).data
        return Response(
            {
                "count": total,
                "limit": limit,
                "offset": offset,
                "results": data,
            }
        )


class GuideArticleViewSet(viewsets.ReadOnlyModelViewSet):
    """
    /guides-api/v2/guides/           -> (Ð¾Ð¿Ñ†.) ÑÐ¿Ð¸ÑÐ¾Ðº Ð²ÑÐµÑ… ÑÑ‚Ð°Ñ‚ÐµÐ¹ (ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ðµ ÐºÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ¸)
    /guides-api/v2/guides/<slug>/    -> Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð°Ñ ÑÑ‚Ð°Ñ‚ÑŒÑ
    """
    permission_classes = [AllowAny]
    lookup_field = "slug"

    def get_queryset(self):
        return GuidePage.objects.live().public().specific()

    def get_serializer_class(self):
        return (
            GuideArticleDetailSerializer
            if self.action == "retrieve"
            else GuideArticleListSerializer
        )


# Update doc and item field in Preview

# --- Ñ€Ð°Ð·Ñ€ÐµÑˆÑ‘Ð½Ð½Ñ‹Ðµ Ð¿Ð¾Ð»Ñ ---
ALLOWED_DOC_FIELDS = {
    "invoice_date","due_date","operation_date","document_series","document_number","order_number",
    "amount_wo_vat","vat_amount","vat_percent","amount_with_vat","currency","paid_by_cash",
    "buyer_name","buyer_id","buyer_vat_code","seller_name","seller_id","seller_vat_code",
    "prekes_kodas","prekes_pavadinimas","prekes_barkodas", "invoice_discount_wo_vat", "invoice_discount_with_vat"
}

ALLOWED_LINE_FIELDS = {
    "prekes_kodas","prekes_pavadinimas","prekes_barkodas",
    "unit","quantity","price","subtotal","vat","vat_percent","total",
}

REQUIRED_FIELDS = {
    'invoice_date', 'document_number', 'amount_wo_vat', 'vat_amount', 
    'amount_with_vat', 'currency', 'seller_name', 'seller_vat_code', 
    'buyer_name', 'buyer_vat_code', 'seller_id', 'buyer_id'
}

MATH_FIELDS = {
    'amount_wo_vat', 'vat_amount', 'amount_with_vat', 'vat_percent',
    'invoice_discount_wo_vat', 'invoice_discount_with_vat'
}

LINE_MATH_FIELDS = {
    'quantity', 'price', 'subtotal', 'vat', 'vat_percent', 'total',
    'discount_wo_vat', 'discount_with_vat'
}


class InlineDocUpdateView(APIView):
    permission_classes = [IsOwner]

    def patch(self, request, doc_id):
        doc = get_object_or_404(ScannedDocument, pk=doc_id, user=request.user)
        field = request.data.get("field")
        value = request.data.get("value")

        if field not in ALLOWED_DOC_FIELDS:
            return Response({"detail": "Field not allowed"}, status=400)

        if value in ("", None):
            value = None

        setattr(doc, field, value)
        doc.save(update_fields=[field])

        # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ
        response_data = {
            "ok": True,
            "id": doc.id,
            field: getattr(doc, field),
        }
        
        try:
            if field in REQUIRED_FIELDS:
                is_valid = check_required_fields_for_export(doc)
                doc.ready_for_export = is_valid
                response_data['ready_for_export'] = is_valid
            
            if field in MATH_FIELDS:
                is_valid, _ = validate_document_math_for_export(doc)
                doc.math_validation_passed = is_valid
                response_data['math_validation_passed'] = is_valid
            
            if field in REQUIRED_FIELDS or field in MATH_FIELDS:
                update_fields = []
                if field in REQUIRED_FIELDS:
                    update_fields.append('ready_for_export')
                if field in MATH_FIELDS:
                    update_fields.append('math_validation_passed')
                if update_fields:
                    doc.save(update_fields=update_fields)
        except Exception as e:
            logger.error(f"Validation error: {str(e)}")
        
        return Response(response_data)


class InlineLineUpdateView(APIView):
    permission_classes = [IsOwner]

    def patch(self, request, doc_id, line_id):
        doc = get_object_or_404(ScannedDocument, pk=doc_id, user=request.user)
        line = get_object_or_404(LineItem, pk=line_id, document=doc)

        field = request.data.get("field")
        value = request.data.get("value")

        if field not in ALLOWED_LINE_FIELDS:
            return Response({"detail": "Field not allowed"}, status=400)

        if value in ("", None):
            value = None

        setattr(line, field, value)
        line.save(update_fields=[field])

        # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ
        response_data = {
            "ok": True,
            "id": line.id,
            field: getattr(line, field),
        }
        
        try:
            if field in LINE_MATH_FIELDS:
                is_valid, _ = validate_document_math_for_export(doc)
                doc.math_validation_passed = is_valid
                doc.save(update_fields=['math_validation_passed'])
                response_data['math_validation_passed'] = is_valid
        except Exception as e:
            logger.error(f"Validation error: {str(e)}")
        
        return Response(response_data)


# Add / delete lineitem in Preview

class ScannedDocumentViewSet(viewsets.ModelViewSet):
    queryset = ScannedDocument.objects.all()
    serializer_class = ScannedDocumentDetailSerializer
    permission_classes = [IsAuthenticated]

    # --- Ð”ÐžÐ‘ÐÐ’Ð˜Ð¢Ð¬ ÐŸÐ£Ð¡Ð¢ÐžÐ™ LINE ITEM ---
    @action(detail=True, methods=["post"], url_path="add-lineitem")
    def add_lineitem(self, request, pk=None):
        doc = self.get_object()
        line = LineItem.objects.create(document=doc)
        return Response(LineItemSerializer(line).data, status=status.HTTP_201_CREATED)

    # --- Ð£Ð”ÐÐ›Ð˜Ð¢Ð¬ LINE ITEM ---
    @action(detail=True, methods=["delete"], url_path="delete-lineitem/(?P<line_id>[^/.]+)")
    def delete_lineitem(self, request, pk=None, line_id=None):
        doc = self.get_object()
        try:
            line = doc.line_items.get(id=line_id)
            line.delete()
            return Response(status=status.HTTP_204_NO_CONTENT)
        except LineItem.DoesNotExist:
            return Response({"detail": "Line item not found"}, status=status.HTTP_404_NOT_FOUND)
        


# contact email sender
@api_view(["POST"])
@permission_classes([AllowAny])
def contact_form(request):
    vardas  = (request.data.get("name") or "").strip()
    email   = (request.data.get("email") or "").strip()
    zinute  = (request.data.get("message") or "").strip()
    # subject nÄ—ra formoje â€“ paliekame None (bus generinÄ—)

    if not vardas or not email or len(zinute) < 10:
        return Response({"detail": "Klaida formoje"}, status=status.HTTP_400_BAD_REQUEST)

    ok = siusti_kontakto_laiska(vardas=vardas, email=email, zinute=zinute, tema=None)
    if ok:
        return Response({"detail": "Å½inutÄ— sÄ—kmingai iÅ¡siÅ³sta. AÄiÅ«!"})
    return Response({"detail": "Nepavyko iÅ¡siÅ³sti Å¾inutÄ—s. Pabandykite vÄ—liau."},
                    status=status.HTTP_500_INTERNAL_SERVER_ERROR)





def send_newsletter():
    text_tpl = (
        "Sveiki,\n\n"
        "ðŸŽ„ Sveikiname su artÄ—janÄiomis KalÄ—domis ir Naujaisiais metais!\n\n"
        "NuoÅ¡irdÅ¾iai dÄ—kojame, kad Å¡iais metais prisidÄ—jote prie DokSkeno starto ir augimo.\n"
        "JÅ«sÅ³ pasitikÄ—jimas mums labai svarbus.\n\n"
        "Kitais metais paÅ¾adame DokSkenÄ… padaryti dar patogesnÄ¯ ir inovatyvesnÄ¯.\n\n"
        "Linkime sÄ—kmÄ—s darbuose, sklandÅ¾iÅ³ procesÅ³ ir puikiÅ³ rezultatÅ³ Naujaisiais metais!\n\n"
        "Su pagarba,\n"
        "DokSkeno komanda\n"
        "Denis"
    )

    siusti_masini_laiska_visiems(
        subject="Sveikinimas iÅ¡ DokSkeno komandos",
        text_template=text_tpl,
        html_template_name=None,      # â† ÐÐ• Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ HTML Ð²Ð¾Ð¾Ð±Ñ‰Ðµ
        extra_context=None,           # Ð¼Ð¾Ð¶Ð½Ð¾ Ð¾Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ
        exclude_user_ids=[46, 2, 16, 24, 31, 69, 105, 133, ],   # ÐºÐ¾Ð³Ð¾ Ð¸ÑÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
        tik_aktyviems=True,
    )


@api_view(['POST'])
@permission_classes([IsAuthenticated])
def generate_mobile_key(request):
    """
    POST /api/mobile/generate-key/

    ÐÐ¾Ð²Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ: ÑÐ¾Ð·Ð´Ð°Ñ‘Ñ‚ ÐžÐ¢Ð”Ð•Ð›Ð¬ÐÐ«Ð™ MobileAccessKey Ð´Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.

    ÐžÐ¶Ð¸Ð´Ð°ÐµÑ‚ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾):
    - email: Ð´Ð»Ñ ÐºÐ°ÐºÐ¾Ð³Ð¾ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÐµÐ»Ñ (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ user.email)
    - label: Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ¾Ð¿Ð¾Ð½ÑÑ‚Ð½Ð¾Ðµ Ð¸Ð¼Ñ (pvz. "Jonas (ofisas)", "Kasa #2")

    ÐÐ˜Ð§Ð•Ð“Ðž Ð½Ðµ ÑˆÐ»Ñ‘Ñ‚ Ð¿Ð¾ email â€“ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚.
    """
    user = request.user

    raw_email = (request.data.get("email") or "").strip().lower()
    label = (request.data.get("label") or "").strip()

    if not raw_email:
        # Ð•ÑÐ»Ð¸ email Ð½Ðµ Ð¿Ñ€Ð¸ÑˆÑ‘Ð» â€“ Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð²Ð·ÑÑ‚ÑŒ email ÑÐ°Ð¼Ð¾Ð³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
        raw_email = (user.email or "").strip().lower()

    if not raw_email:
        return Response({"error": "EMAIL_REQUIRED"}, status=400)

    # ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ MobileAccessKey Ð¸ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ raw_key (ÑÑ‚Ñ€Ð¾ÐºÐ°, ÐºÐ¾Ñ‚Ð¾Ñ€ÑƒÑŽ Ð¿Ð¾ÐºÐ°Ð¶ÐµÐ¼/Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð¸Ð¼)
    access_key, raw_key = MobileAccessKey.create_for_user(
        user=user,
        sender_email=raw_email,
        label=label or None,
    )

    play_store_link = build_mobile_play_store_link(raw_key)

    return Response({
        "id": access_key.id,
        "mobile_key": raw_key,          # ÐŸÐžÐ›ÐÐ«Ð™ ÐºÐ»ÑŽÑ‡ â€“ Ð¿Ð¾ÐºÐ°Ð¶ÐµÐ¼ Ð¾Ð´Ð¸Ð½ Ñ€Ð°Ð·
        "key_last4": access_key.key_last4,
        "sender_email": access_key.sender_email,
        "label": access_key.label,
        "play_store_link": play_store_link,
    })



@api_view(['POST'])
@permission_classes([IsAuthenticated])
def send_mobile_invitation(request):
    """
    POST /api/mobile/send-invitation/

    ÐžÐ¶Ð¸Ð´Ð°ÐµÑ‚:
    - email: Ð¿Ð¾Ð»ÑƒÑ‡Ð°Ñ‚ÐµÐ»ÑŒ Ð¿Ñ€Ð¸Ð³Ð»Ð°ÑˆÐµÐ½Ð¸Ñ
    - (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾) label: pvz. "Jonas (ofisas)", "Kasa #2"

    Ð”ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ:
    - ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð½Ð¾Ð²Ñ‹Ð¹ MobileAccessKey Ð´Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ email
    - ÑÑ‚Ñ€Ð¾Ð¸Ð¼ Play Store ÑÑÑ‹Ð»ÐºÑƒ Ñ ÑÑ‚Ð¸Ð¼ ÐºÐ»ÑŽÑ‡Ð¾Ð¼
    - ÑˆÐ»Ñ‘Ð¼ Ð¿Ð¸ÑÑŒÐ¼Ð¾ (siusti_mobilios_apps_kvietima)
    """
    user = request.user

    raw_email = (request.data.get("email") or "").strip().lower()
    label = (request.data.get("label") or "").strip()

    if not raw_email:
        return Response({"error": "EMAIL_REQUIRED"}, status=400)

    # ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ð¹ ÐºÐ»ÑŽÑ‡ Ð¿Ð¾Ð´ ÑÑ‚Ð¾Ñ‚ email
    access_key, raw_key = MobileAccessKey.create_for_user(
        user=user,
        sender_email=raw_email,
        label=label or None,
    )

    play_store_link = build_mobile_play_store_link(raw_key)

    ok = siusti_mobilios_apps_kvietima(
        kvietejas=user,
        gavejo_email=raw_email,
        play_store_link=play_store_link,
        mobile_key=raw_key,  # Ð²Ð°Ð¶Ð½Ñ‹Ð¹ Ð¼Ð¾Ð¼ÐµÐ½Ñ‚: ÑÑŽÐ´Ð° ÐºÐ»Ð°Ð´Ñ‘Ð¼ Ð¡Ð«Ð ÐžÐ™ ÐºÐ»ÑŽÑ‡
    )

    if not ok:
        return Response(
            {"error": "EMAIL_SEND_FAILED"},
            status=500,
        )

    return Response({
        "status": "OK",
        "email": raw_email,
        "label": access_key.label,
        "key_last4": access_key.key_last4,
        "play_store_link": play_store_link,
        "id": access_key.id,
    })


User = get_user_model()


@api_view(['POST'])
@authentication_classes([])   # Ð°Ð²Ñ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ Ð¼Ð¾Ð±Ð¸Ð»ÑŒÐ½Ð¾Ð¼Ñƒ ÐºÐ»ÑŽÑ‡Ñƒ
@permission_classes([AllowAny])
def mobile_upload_documents(request: HttpRequest):
    """
    POST /api/mobile/upload/

    Headers:
      X-Mobile-Key: <pilnas mobilus raktas>

    Body (multipart/form-data):
      files: <pdf1>, files: <pdf2>, ...
      (neprivaloma) sender_email: jei nori perraÅ¡yti (daÅ¾niausiai nereikÄ—s)
    """

    raw_key = (
        request.META.get("HTTP_X_MOBILE_KEY")
        or request.data.get("mobile_key")
    )

    if not raw_key:
        return Response(
            {"error": "MOBILE_KEY_REQUIRED"},
            status=status.HTTP_400_BAD_REQUEST,
        )

    key_hash = MobileAccessKey.make_hash(raw_key)

    try:
        access_key = MobileAccessKey.objects.select_related("user").get(
            key_hash=key_hash,
            is_active=True,
        )
    except MobileAccessKey.DoesNotExist:
        return Response(
            {"error": "INVALID_OR_REVOKED_MOBILE_KEY"},
            status=status.HTTP_401_UNAUTHORIZED,
        )

    user = access_key.user

    files = request.FILES.getlist("files")
    if not files:
        return Response(
            {"error": "NO_FILES"},
            status=status.HTTP_400_BAD_REQUEST,
        )

    sender_email = request.data.get("sender_email") or access_key.sender_email or None

    access_key.last_used_at = timezone.now()
    access_key.save(update_fields=["last_used_at"])

    created_docs = []
    for f in files:
        doc = MobileInboxDocument.objects.create(
            user=user,
            access_key=access_key,
            uploaded_file=f,
            original_filename=f.name,
            size_bytes=getattr(f, "size", 0) or 0,
            page_count=None,          # page_count v budushchem mozhno budet peredavat iz mobile
            sender_email=sender_email,
            is_processed=False,       # v inbox po umolchaniyu neperenesennyj
        )

        # posle soxranenija u polja uploaded_file uzhe est .url
        doc.preview_url = f"{settings.SITE_URL_BACKEND}{doc.uploaded_file.url}"
        doc.save(update_fields=["preview_url"])

        created_docs.append({
            "id": doc.id,
            "original_filename": doc.original_filename,
            "size_bytes": doc.size_bytes,
            "page_count": doc.page_count,
            "sender_email": doc.sender_email,
            "created_at": doc.created_at.isoformat(),
            # bez URL-ov, kak dogovorilis'
        })

    return Response(
        {
            "status": "OK",
            "count": len(created_docs),
            "documents": created_docs,
        },
        status=status.HTTP_201_CREATED,
    )



@api_view(["GET", "POST"])
@permission_classes([IsAuthenticated])
def mobile_access_keys_list_create(request):
    """
    GET  /api/mobile/keys/   -> sÄ…raÅ¡as visÅ³ MobileAccessKey Å¡itam user'iui
    POST /api/mobile/keys/   -> sukuria naujÄ… MobileAccessKey ir iÅ¡siunÄia kvietimÄ… el. paÅ¡tu

    Body (POST):
      - email (required)
      - label (optional)
    """
    user = request.user

    if request.method == "GET":
        qs = MobileAccessKey.objects.filter(user=user).order_by("-created_at")
        serializer = MobileAccessKeySerializer(qs, many=True)
        return Response(serializer.data)

    # POST
    raw_email = (request.data.get("email") or "").strip().lower()
    label = (request.data.get("label") or "").strip()

    if not raw_email:
        return Response({"error": "EMAIL_REQUIRED"}, status=status.HTTP_400_BAD_REQUEST)

    # ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ð¹ ÐºÐ»ÑŽÑ‡ Ð¿Ð¾Ð´ ÑÑ‚Ð¾Ñ‚ email
    access_key, raw_key = MobileAccessKey.create_for_user(
        user=user,
        sender_email=raw_email,
        label=label or None,
    )

    play_store_link = build_mobile_play_store_link(raw_key)

    ok = siusti_mobilios_apps_kvietima(
        kvietejas=user,
        gavejo_email=raw_email,
        play_store_link=play_store_link,
        mobile_key=raw_key,
    )

    if not ok:
        return Response(
            {"error": "EMAIL_SEND_FAILED"},
            status=status.HTTP_500_INTERNAL_SERVER_ERROR,
        )

    serializer = MobileAccessKeySerializer(access_key)
    data = serializer.data
    # play_store_link Ð¼Ñ‹ Ð¼Ð¾Ð¶ÐµÐ¼ Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð·Ð´ÐµÑÑŒ (ÐºÐ¾Ð³Ð´Ð° ÐµÑ‰Ñ‘ ÐµÑÑ‚ÑŒ raw_key)
    data["play_store_link"] = play_store_link

    return Response(data, status=status.HTTP_201_CREATED)



@api_view(["PATCH", "DELETE"])
@permission_classes([IsAuthenticated])
def mobile_access_key_detail(request, pk: int):
    """
    PATCH  /api/mobile/keys/<id>/   -> keiÄiam is_active (toggle)
    DELETE /api/mobile/keys/<id>/   -> iÅ¡trinam raktÄ…

    PATCH body:
      { "is_active": true/false }
    """
    try:
        access_key = MobileAccessKey.objects.get(pk=pk, user=request.user)
    except MobileAccessKey.DoesNotExist:
        return Response({"detail": "Not found."}, status=status.HTTP_404_NOT_FOUND)

    if request.method == "DELETE":
        access_key.delete()
        return Response(status=status.HTTP_204_NO_CONTENT)

    # PATCH
    new_is_active = request.data.get("is_active", None)

    # Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð¼ ÑÑ‚Ñ€Ð¾ÐºÐ¸ "true"/"false" Ð½Ð° Ð²ÑÑÐºÐ¸Ð¹ ÑÐ»ÑƒÑ‡Ð°Ð¹
    if isinstance(new_is_active, str):
        new_is_active = new_is_active.lower() in ("1", "true", "yes", "on")

    if new_is_active is None:
        return Response(
            {"detail": "is_active is required"},
            status=status.HTTP_400_BAD_REQUEST,
        )

    if not new_is_active:
        # Ð²Ñ‹ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð°ÐºÐºÑƒÑ€Ð°Ñ‚Ð½Ð¾ Ñ‡ÐµÑ€ÐµÐ· Ð¼ÐµÑ‚Ð¾Ð´ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (ÑÑ‚Ð°Ð²Ð¸Ñ‚ revoked_at)
        access_key.revoke()
    else:
        # Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾, Ñ‡Ð¸ÑÑ‚Ð¸Ð¼ revoked_at
        if not access_key.is_active:
            access_key.is_active = True
            access_key.revoked_at = None
            access_key.save(update_fields=["is_active", "revoked_at"])

    serializer = MobileAccessKeySerializer(access_key)
    return Response(serializer.data)




@api_view(["GET"])
@permission_classes([IsAuthenticated])
def web_mobile_inbox(request):
    """
    GET /api/web/mobile-inbox/

    Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð¼Ð¾Ð±Ð¸Ð»ÑŒÐ½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ (WEB).

    ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ðµ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÐµÑ‰Ñ‘ ÐÐ• Ð¿ÐµÑ€ÐµÐ½ÐµÑÐµÐ½Ñ‹ Ð² suvestinÄ™:
      is_processed = False
    """

    user = request.user

    qs = (
        MobileInboxDocument.objects
        .filter(user=user, is_processed=False)  # Ñ‚Ð¾Ð»ÑŒÐºÐ¾ "inbox"
        .select_related("processed_document", "access_key")
        .order_by("-created_at")
    )

    serializer = MobileInboxDocumentSerializer(qs, many=True)
    return Response(serializer.data)


#Udaliajem faily s IsKlientu spiska
@api_view(["DELETE"])
@permission_classes([IsAuthenticated])
def web_mobile_inbox_bulk_delete(request):
    """
    DELETE /api/web/mobile-inbox/bulk-delete/

    Body (JSON):
      { "ids": [1, 2, 3, ...] }

    Ð£Ð´Ð°Ð»ÑÐµÑ‚ MobileInboxDocument Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.
    Ð›Ð¾Ð³Ð¸Ñ‡Ð½Ð¾ ÑƒÐ´Ð°Ð»ÑÑ‚ÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ðµ, Ñ‡Ñ‚Ð¾ ÐµÑ‰Ñ‘ Ð½Ðµ Ð¿ÐµÑ€ÐµÐ½ÐµÑÐµÐ½Ñ‹ (is_processed=False).
    """

    user = request.user
    ids = request.data.get("ids") or []

    if not isinstance(ids, list) or not ids:
        return Response(
            {"error": "NO_IDS", "detail": "Pateikite bent vienÄ… ID."},
            status=status.HTTP_400_BAD_REQUEST,
        )

    docs = MobileInboxDocument.objects.filter(
        user=user,
        is_processed=False,
        id__in=ids,
    )

    # Ð•ÑÐ»Ð¸ Ñ…Ð¾Ñ‡ÐµÑˆÑŒ Ñ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ Ð¸ Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ„Ð°Ð¹Ð»Ñ‹ â€“ Ð¼Ð¾Ð¶Ð½Ð¾ Ñ‚Ð°Ðº:
    file_paths = []
    for d in docs:
        if d.uploaded_file and d.uploaded_file.name:
            try:
                file_paths.append(d.uploaded_file.path)
            except Exception:
                pass

    deleted_count = docs.count()
    docs.delete()

    # ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ ÑƒÐ´Ð°Ð»Ð¸Ñ‚ÑŒ Ñ„Ð°Ð¹Ð»Ñ‹ Ñ Ð´Ð¸ÑÐºÐ° (Ð½Ðµ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ð¾, ÐµÑÐ»Ð¸ Ð½Ðµ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑÑ)
    for path in file_paths:
        try:
            if path and os.path.exists(path):
                os.remove(path)
        except Exception:
            continue

    return Response(
        {"status": "OK", "count": deleted_count, "deleted_ids": ids},
        status=status.HTTP_200_OK,
    )


#Perevodim vybranyje faily s IsKlientu v Suvestine (mobile file(original) i preview_url ostajuca)
@api_view(["POST"])
@permission_classes([IsAuthenticated])
def web_mobile_inbox_promote(request):
    """
    POST /api/web/mobile-inbox/promote/
    Body: { "ids": [1, 2, 3] }
    """

    user = request.user
    ids = request.data.get("ids") or []

    if not isinstance(ids, list) or not ids:
        return Response(
            {"error": "NO_IDS", "detail": "Pateikite bent vienÄ… ID."},
            status=status.HTTP_400_BAD_REQUEST,
        )

    mobile_docs = (
        MobileInboxDocument.objects
        .filter(user=user, is_processed=False, id__in=ids)
        .select_related("access_key")
    )

    if not mobile_docs.exists():
        return Response(
            {"status": "OK", "count": 0, "processed_ids": []},
            status=status.HTTP_200_OK,
        )

    processed_ids = []
    scan_type = "sumiskai"  # Ð¿Ð¾ÐºÐ° Ñ„Ð¸ÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾, ÐºÐ°Ðº Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ web-upload

    for mobile_doc in mobile_docs:
        if not mobile_doc.uploaded_file:
            continue

        try:
            with transaction.atomic():
                # 1) Ð·Ð°Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð±Ð°Ð¹Ñ‚Ñ‹ Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð³Ð¾ PDF Ð¸Ð· MobileInboxDocument
                original_file = mobile_doc.uploaded_file
                original_file.open("rb")
                content = original_file.read()
                original_file.close()

                # 2) ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ ScannedDocument Ñ Ñ‚ÐµÐ¼ Ð¶Ðµ original_filename
                scanned = ScannedDocument(
                    user=user,
                    original_filename=mobile_doc.original_filename,
                    status="processing",
                    scan_type=scan_type,
                )

                # 3) ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÐšÐžÐŸÐ˜Ð® Ñ„Ð°Ð¹Ð»Ð° (user_upload_path ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ‚ ÑÐ²Ð¾Ñ‘ Ð¸Ð¼Ñ)
                scanned.file.save(
                    original_file.name.split("/")[-1],
                    ContentFile(content),
                    save=True,
                )
                scanned.save()

                # 4) Ð¿Ð¾Ð¼ÐµÑ‡Ð°ÐµÐ¼ mobile-Ð´Ð¾Ðº ÐºÐ°Ðº Ð¿ÐµÑ€ÐµÐ½ÐµÑÑ‘Ð½Ð½Ñ‹Ð¹
                mobile_doc.processed_document = scanned
                mobile_doc.processed_at = timezone.now()
                mobile_doc.is_processed = True
                mobile_doc.save(
                    update_fields=["processed_document", "processed_at", "is_processed"]
                )

                # 5) Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ñ‚Ð²Ð¾Ð¹ Celery-Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½
                process_uploaded_file_task.delay(user.id, scanned.id, scan_type)

                processed_ids.append(mobile_doc.id)

        except Exception as e:
            # Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¼Ð¾Ð¶Ð½Ð¾ Ñ‚ÑƒÑ‚
            continue

    return Response(
        {
            "status": "OK",
            "count": len(processed_ids),
            "processed_ids": processed_ids,
        },
        status=status.HTTP_200_OK,
    )



@api_view(["GET", "POST"])
@permission_classes([IsAuthenticated])
def payments_list(request):
    """
    /api/payments/
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ð¿Ð»Ð°Ñ‚ÐµÐ¶ÐµÐ¹ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.
    GET Ð¸ POST Ð´ÐµÐ»Ð°ÑŽÑ‚ Ð¾Ð´Ð½Ð¾ Ð¸ Ñ‚Ð¾ Ð¶Ðµ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð»Ð¾Ð¼Ð°Ñ‚ÑŒ Ñ‚Ð²Ð¾Ð¹ Ð¿Ñ€Ð¸Ð²Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½.
    """
    qs = (
        Payments.objects
        .filter(user=request.user)
        .order_by('-paid_at')
    )

    serializer = PaymentSerializer(
        qs,
        many=True,
        context={'request': request},  # Ð²Ð°Ð¶Ð½Ð¾ Ð´Ð»Ñ invoice_url Ð² ÑÐµÑ€Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ðµ
    )
    return Response(serializer.data)



@api_view(["GET", "POST"])
@permission_classes([IsAuthenticated])
def payment_invoice(request, pk):
    """
    /api/payments/<pk>/invoice/
    Dati skirti PDF sÄ…skaitai.
    """
    payment = get_object_or_404(Payments, pk=pk, user=request.user)
    user = request.user  # CustomUser

    # PardavÄ—jas 
    seller = {
        "pavadinimas": "Denis Orlov - DokSkenas",
        "iv_numeris": "1292165",
        "imonesKodas": "",  
        "pvmKodas": "",
        "adresas": "Kreivasis skg. 18-19, Vilnius",
        "telefonas": "",
        "bankoPavadinimas": "",
        "iban": "",
        "swift": "",
    }

    # PirkÄ—jas â€“ klientas Ð¸Ð· CustomUser
    buyer = {
        "pavadinimas": user.company_name or user.email,
        "imonesKodas": user.company_code or "",
        "pvmKodas": user.vat_code or "",
        "adresas": user.company_address or "",
        "telefonas": "",
        "bankoPavadinimas": "",
        "iban": user.company_iban or "",
        "swift": "",
        "salis": user.company_country_iso or "",
    }

    data = {
        "id": payment.id,
        "dok_number": payment.dok_number,
        "paid_at": payment.paid_at,
        "credits_purchased": payment.credits_purchased,
        "net_amount": payment.net_amount,
        "currency": (payment.currency or "EUR").upper(),
        "buyer_email": payment.buyer_email,
        "buyer_address": payment.buyer_address_json,
        "seller": seller,
        "buyer": buyer,
    }

    return Response(data)