# --- Standard library ---
import hashlib
import io
import logging
import logging.config
import os
import tempfile
import zipfile
import json
from datetime import date, timedelta, time, datetime
from decimal import Decimal
import unicodedata


# --- Django ---
from django.conf import settings
from django.db import transaction
from django.db.models import Q
from django.http import FileResponse, Http404, HttpResponse
from django.shortcuts import get_object_or_404
from django.utils import timezone
from django.utils.dateparse import parse_date

# --- Django REST Framework ---
from rest_framework import generics, permissions, status
from rest_framework.decorators import api_view, permission_classes, authentication_classes
from rest_framework.permissions import IsAuthenticated, AllowAny, IsAdminUser
from rest_framework.response import Response
from rest_framework.views import APIView

# --- DRF SimpleJWT ---
from rest_framework_simplejwt.views import TokenObtainPairView, TokenRefreshView

# --- Local (project) imports ---
from .data_import.data_import_from_buh import import_products_from_xlsx, import_clients_from_xlsx
from .exports.apskaita5 import export_documents_group_to_apskaita5_files
from .exports.centas import export_documents_group_to_centras_xml
from .exports.finvalda import (
    export_pirkimai_group_to_finvalda,
    export_pardavimai_group_to_finvalda,
)
from .exports.agnum import (
    export_pirkimai_group_to_agnum,
    export_pardavimai_group_to_agnum,
)
from .exports.rivile import (
    export_clients_group_to_rivile,
    export_pirkimai_group_to_rivile,
    export_pardavimai_group_to_rivile,
    export_prekes_paslaugos_kodai_group_to_rivile,
)
from .exports.rivile_erp import (
    export_clients_to_rivile_erp_xlsx,
    export_prekes_and_paslaugos_to_rivile_erp_xlsx,
    export_documents_to_rivile_erp_xlsx,
)
from .exports.dineta import send_dineta_bundle, DinetaError
from .exports.pragma4 import export_to_pragma40_xml
from .exports.pragma3 import export_to_pragma_full, save_pragma_export_to_files
from .exports.butent import export_to_butent
from .validators.required_fields_checker import check_required_fields_for_export
from .validators.math_validator_for_export import validate_document_math_for_export

from .models import (
    CustomUser,
    ScannedDocument,
    ProductAutocomplete,
    ClientAutocomplete,
    LineItem,
    AdClick,
)
from .serializers import (
    CustomUserSerializer,
    ViewModeSerializer,
    ScannedDocumentSerializer,
    ScannedDocumentListSerializer,
    ScannedDocumentDetailSerializer,
    ScannedDocumentAdminDetailSerializer,
    AdClickSerializer,
    LineItemSerializer,
    CustomUserAdminListSerializer,
    DinetaSettingsSerializer,
)
from django.db.models import Prefetch


from .tasks import process_uploaded_file_task
from .utils.data_resolver import build_preview
from .utils.pirkimas_pardavimas import determine_pirkimas_pardavimas
from .utils.prekes_kodas import assign_random_prekes_kodai
from .utils.save_document import _apply_sumiskai_defaults_from_user
from .utils.update_currency_rates import update_currency_rates
from .validators.vat_klas import auto_select_pvm_code

#dlia superuser dashboard
from django.db.models import Count
from .permissions import IsSuperUser, IsOwner

#wagtail imports
from rest_framework import viewsets, mixins
from .models import GuidePage, GuideCategoryPage
from rest_framework.decorators import action
from .serializers import (
    GuideCategoryListSerializer,
    GuideCategoryDetailSerializer,
    GuideArticleListSerializer,
    GuideArticleDetailSerializer,
)

#emails
from .emails import siusti_sveikinimo_laiska, siusti_kontakto_laiska
from .emails import siusti_masini_laiska_visiems

from time import perf_counter


# --- Logging setup ---
logging.config.dictConfig(settings.LOGGING)
logger = logging.getLogger('docscanner_app')
site_url = settings.SITE_URL_FRONTEND  # –±–µ—Ä—ë–º –∏–∑ settings.py


#admin dashboard
from datetime import datetime, time, timedelta
from django.utils import timezone
from rest_framework.decorators import api_view, permission_classes
from rest_framework.response import Response
import json

# from .models import ScannedDocument, CustomUser
# from .permissions import IsSuperUser
# from .views import summarize_doc_issues  # –µ—Å–ª–∏ –≤ —Ç–æ–º –∂–µ —Ñ–∞–π–ª–µ ‚Äî –Ω–µ –Ω—É–∂–Ω–æ

def _today_dates():
    today = timezone.localdate()
    yesterday = today - timedelta(days=1)
    return today, yesterday

def _count_by_date(model, date_field: str, target_date):
    return model.objects.filter(**{f"{date_field}__date": target_date}).count()

def _qs_by_date(model, date_field: str, target_date):
    start = timezone.make_aware(datetime.combine(target_date, time.min))
    end = timezone.make_aware(datetime.combine(target_date, time.max))
    return model.objects.filter(**{f"{date_field}__range": (start, end)})

def _qs_last_n_days(model, date_field: str, days: int):
    since = timezone.now() - timedelta(days=days)
    return model.objects.filter(**{f"{date_field}__gte": since})

def _ensure_dict(maybe_json):
    if not maybe_json:
        return {}
    if isinstance(maybe_json, dict):
        return maybe_json
    if isinstance(maybe_json, str):
        try:
            return json.loads(maybe_json)
        except Exception:
            return {}
    return {}

def _doc_has_issues(doc):
    raw = getattr(doc, 'structured_json', None) or getattr(doc, 'gpt_raw_json', None) or {}
    struct = _ensure_dict(raw)
    res = summarize_doc_issues(struct)
    return bool(res.get("has_issues"))

def _count_errors_in_qs(qs):
    n = 0
    for doc in qs.only('id', 'structured_json', 'gpt_raw_json'):
        if _doc_has_issues(doc):
            n += 1
    return n

def _pct(part, whole):
    return round((part / whole * 100.0), 2) if whole else 0.0

def _rate(ok_count, total_count):
    """% —É—Å–ø–µ—à–Ω—ã—Ö (–±–µ–∑ –æ—à–∏–±–æ–∫) –æ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞."""
    return _pct(ok_count, total_count)

@api_view(["GET"])
@permission_classes([IsSuperUser])
def superuser_dashboard_stats(request):
    doc_date_field = "uploaded_at"
    user_date_field = "date_joined"

    today, yesterday = _today_dates()

    qs_all      = ScannedDocument.objects.all()
    qs_today    = _qs_by_date(ScannedDocument, doc_date_field, today)
    qs_yesterday= _qs_by_date(ScannedDocument, doc_date_field, yesterday)
    qs_7d       = _qs_last_n_days(ScannedDocument, doc_date_field, 7)
    qs_30d      = _qs_last_n_days(ScannedDocument, doc_date_field, 30)

    docs_today      = qs_today.count()
    docs_yesterday  = qs_yesterday.count()
    docs_7d         = qs_7d.count()
    docs_30d        = qs_30d.count()
    total_docs      = qs_all.count()

    err_today       = _count_errors_in_qs(qs_today)
    err_yesterday   = _count_errors_in_qs(qs_yesterday)
    err_7d          = _count_errors_in_qs(qs_7d)
    err_30d         = _count_errors_in_qs(qs_30d)
    err_total       = _count_errors_in_qs(qs_all)

    ok_today        = max(docs_today - err_today, 0)
    ok_yesterday    = max(docs_yesterday - err_yesterday, 0)
    ok_7d           = max(docs_7d - err_7d, 0)
    ok_30d          = max(docs_30d - err_30d, 0)
    ok_total        = max(total_docs - err_total, 0)

    # —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ (–ø—Ä–∏–º–µ—Ä –∏–∑ —Ç–≤–æ–µ–π –≤–µ—Ä—Å–∏–∏)
    start_today = timezone.make_aware(datetime.combine(today, time.min))
    end_today   = timezone.make_aware(datetime.combine(today, time.max))
    unique_users_excl_1_2_today = (
        ScannedDocument.objects
        .exclude(user_id__in=[1, 2])
        .filter(**{f"{doc_date_field}__range": (start_today, end_today)})
        .values("user_id").distinct().count()
    )

    # –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏/—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
    new_users_today      = _count_by_date(CustomUser, user_date_field, today)
    new_users_yesterday  = _count_by_date(CustomUser, user_date_field, yesterday)
    new_users_7d         = _qs_last_n_days(CustomUser, user_date_field, 7).count()
    new_users_30d        = _qs_last_n_days(CustomUser, user_date_field, 30).count()
    total_users          = CustomUser.objects.count()

    # —Ä–∞–∑–±–∏–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º
    st_sumiskai = qs_all.filter(scan_type="sumiskai").count()
    st_detaliai = qs_all.filter(scan_type="detaliai").count()

    data = {
        "documents": {
            "today":            {"count": docs_today,     "errors": err_today},
            "yesterday":        {"count": docs_yesterday, "errors": err_yesterday},
            "last_7_days":      {"count": docs_7d,        "errors": err_7d},
            "last_30_days":     {"count": docs_30d,       "errors": err_30d},
            "total":            {"count": total_docs,     "errors": err_total},

            # ‚úÖ –ù–æ–≤—ã–π –±–ª–æ–∫ ‚Äî Success rate (–±–µ–∑ –æ—à–∏–±–æ–∫)
            "success_rate": {
                "today":        _rate(ok_today,     docs_today),
                "yesterday":    _rate(ok_yesterday, docs_yesterday),
                "last_7_days":  _rate(ok_7d,        docs_7d),
                "last_30_days": _rate(ok_30d,       docs_30d),
                "total":        _rate(ok_total,     total_docs),
            },

            "unique_users_excluding_1_2_today": unique_users_excl_1_2_today,
            "scan_types": {
                "sumiskai": {"count": st_sumiskai, "pct": _pct(st_sumiskai, total_docs)},
                "detaliai": {"count": st_detaliai, "pct": _pct(st_detaliai, total_docs)},
            },
        },
        "users": {
            "new_today":        new_users_today,
            "new_yesterday":    new_users_yesterday,
            "new_last_7_days":  new_users_7d,
            "new_last_30_days": new_users_30d,
            "total":            total_users,
        },
        "meta": {
            "timezone": str(timezone.get_current_timezone()),
            "generated_at": timezone.now().isoformat(),
        },
    }
    return Response(data)








def strip_diacritics(text):
    """
    Pakeiƒçia visas lietuvi≈°kas ir kitas lotyni≈°kas raides su diakritika
    ƒØ paprastas: ≈°->s, ƒÖ->a, ≈Ω->Z ir t.t.
    """
    if not isinstance(text, str):
        return text
    normalized = unicodedata.normalize("NFD", text)
    return "".join(ch for ch in normalized if unicodedata.category(ch) != "Mn")



@api_view(['POST'])
@permission_classes([IsAuthenticated])
def export_documents(request):
    from datetime import date
    import io
    import zipfile
    import tempfile
    import logging

    logger = logging.getLogger(__name__)
    log_ctx = {"user": getattr(request.user, "id", None)}

    # ---- –≤—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    ids = request.data.get('ids', [])
    export_type = request.data.get('export_type') or getattr(request.user, 'default_accounting_program', 'centas')
    raw_overrides = request.data.get('overrides', {}) or {}
    mode_raw = (request.data.get('mode') or "").strip().lower()  # <<< NEW

    logger.info("[EXP] start user=%s export_type_raw=%r ids=%s raw_overrides=%r mode_raw=%r",
                log_ctx["user"], export_type, ids, raw_overrides, mode_raw)

    if not ids:
        logger.warning("[EXP] no ids provided")
        return Response({"error": "No document ids provided"}, status=400)

    user = request.user
    export_type = str(export_type).lower()

    # Rivilƒó: ar reikia nuimti lietuvi≈°kas raides (≈°->s ir t.t.)
    extra_settings = getattr(user, "extra_settings", {}) or {}
    rivile_strip_lt = bool(extra_settings.get("rivile_strip_lt_letters"))
    logger.info("[EXP] user extra_settings: rivile_strip_lt_letters=%s", rivile_strip_lt)

    # --- –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è overrides (id -> 'pirkimas'|'pardavimas')
    overrides = {}
    for k, v in raw_overrides.items():
        key = str(k)
        val = str(v).lower()
        if val in ('pirkimas', 'pardavimas'):
            overrides[key] = val
        else:
            logger.warning("[EXP] skip override key=%r val=%r (invalid)", key, v)

    # --- –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å mode: –±–µ—Ä—ë–º –∏–∑ –∫–ª–∏–µ–Ω—Ç–∞, –∏–Ω–∞—á–µ –∫–∞–∫ —Ä–∞–Ω—å—à–µ (–ø–æ overrides)
    if mode_raw in ("multi", "single"):                       # <<< NEW
        mode = mode_raw
        logger.info("[EXP] view mode taken from request: %s", mode)
    else:
        mode = 'multi' if overrides else 'single'
        logger.info("[EXP] view mode inferred for backward-compat: %s", mode)

    # –î–æ–ø. –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ –ø—Ä–∏—à—ë–ª multi, –Ω–æ overrides –ø—É—Å—Ç–æ–π
    if mode == "multi" and not overrides:
        logger.info("[EXP] mode is 'multi' but overrides are EMPTY (will rely on resolver/doc DB fields)")

    logger.info("[EXP] export_type=%s overrides_norm=%r", export_type, overrides)

    today_str = date.today().strftime('%Y-%m-%d')

    documents = ScannedDocument.objects.filter(pk__in=ids, user=user)
    if not documents:
        logger.warning("[EXP] no documents found by ids=%s user=%s", ids, log_ctx["user"])
        return Response({"error": "No documents found"}, status=404)

    # === —Ä–µ–∑–æ–ª–≤–µ—Ä ===
    from .utils.data_resolver import prepare_export_groups
    logger.info("[EXP] resolver_mode=%s", mode)

    try:
        prepared = prepare_export_groups(
            documents,
            user=user,
            overrides=overrides if mode == 'multi' else {},  # <<< —É–≤–∞–∂–∞—Ç—å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º
            view_mode=mode,                                   # <<< —É–≤–∞–∂–∞—Ç—å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º
        )
    except Exception as e:
        logger.exception("[EXP] prepare_export_groups failed: %s", e)
        return Response({"error": "Resolver failed", "detail": str(e)}, status=500)

    # –±—ã—Å—Ç—Ä—ã–π –¥–∞–º–ø —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–∏—à–ª–æ –∏–∑ —Ä–µ–∑–æ–ª–≤–µ—Ä–∞
    def _debug_dump(prepared_obj, where):
        for bucket in ("pirkimai", "pardavimai", "unknown"):
            packs = prepared_obj.get(bucket) or []
            logger.info("[EXPDBG:%s] bucket=%s count=%d", where, bucket, len(packs))
            for p in packs:
                d = p.get("doc")
                dpk = getattr(d, "pk", None)
                li = p.get("line_items") or []
                logger.info(
                    "[EXPDBG:%s] bucket=%s doc=%s dir=%r pack_keys=%s pvm=%r lines=%d",
                    where, bucket, dpk, p.get("direction"), list(p.keys()),
                    p.get("pvm_kodas", None), len(li)
                )
                if li:
                    preview = [(x.get("id"), x.get("pvm_kodas")) for x in li[:3]]
                    logger.info("[EXPDBG:%s] doc=%s sample_line_items=%s", where, dpk, preview)

    _debug_dump(prepared, "after_resolver")

    # –ø—Ä–∏–º–µ–Ω—è–µ–º ¬´–≤ –ø–∞–º—è—Ç—å¬ª (–±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î)
    def _apply_resolved(pack_list, tag):
        out_docs = []
        for pack in pack_list:
            d = pack["doc"]
            setattr(d, "pirkimas_pardavimas", pack.get("direction"))
            setattr(d, "pvm_kodas", pack.get("pvm_kodas", None))  # —è–≤–Ω–æ–µ –ø–µ—Ä–µ—Ç–∏—Ä–∞–Ω–∏–µ

            line_map = {}
            for li in (pack.get("line_items") or []):
                li_id = li.get("id")
                if li_id is not None:
                    line_map[li_id] = li.get("pvm_kodas")
            setattr(d, "_pvm_line_map", line_map)

            logger.info("[EXPDBG:apply] tag=%s doc=%s dir=%r pvm_kodas=%r line_map_size=%d",
                        tag, getattr(d, "pk", None), getattr(d, "pirkimas_pardavimas", None),
                        getattr(d, "pvm_kodas", None), len(line_map))
            out_docs.append(d)
        return out_docs

    pirkimai_docs   = _apply_resolved(prepared.get("pirkimai", []), "pirkimai")
    pardavimai_docs = _apply_resolved(prepared.get("pardavimai", []), "pardavimai")
    unknown_docs    = _apply_resolved(prepared.get("unknown", []), "unknown")

    logger.info("[EXP] ready_for_export counts: pirkimai=%d pardavimai=%d unknown=%d",
                len(pirkimai_docs), len(pardavimai_docs), len(unknown_docs))

    # --- –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    response = None
    export_success = False
    exported_ids = [d.pk for d in (pirkimai_docs + pardavimai_docs)]

    # –æ–±—â–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä (–≤–Ω—É—Ç—Ä–∏ –≤–µ—Ç–æ–∫ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—Ç—å/–æ—á–∏—â–∞—Ç—å)
    files_to_export = []

    # ========================= CENTAS =========================
    if export_type == 'centas':
        logger.info("[EXP] CENTAS export started")
        assign_random_prekes_kodai(documents)

        if pirkimai_docs:
            logger.info("[EXP] CENTAS exporting pirkimai: %d docs", len(pirkimai_docs))
            # –ò–ó–ú–ï–ù–ï–ù–û: –¥–æ–±–∞–≤–ª–µ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä user=request.user
            xml_bytes = export_documents_group_to_centras_xml(
                pirkimai_docs, 
                direction="pirkimas",
                user=request.user  # <-- –î–û–ë–ê–í–¨ –≠–¢–û
            )
            files_to_export.append((f"{today_str}_pirkimai.xml", xml_bytes))
            
        if pardavimai_docs:
            logger.info("[EXP] CENTAS exporting pardavimai: %d docs", len(pardavimai_docs))
            # –ò–ó–ú–ï–ù–ï–ù–û: –¥–æ–±–∞–≤–ª–µ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä user=request.user
            xml_bytes = export_documents_group_to_centras_xml(
                pardavimai_docs, 
                direction="pardavimas",
                user=request.user  # <-- –î–û–ë–ê–í–¨ –≠–¢–û
            )
            files_to_export.append((f"{today_str}_pardavimai.xml", xml_bytes))

        logger.info("[EXP] CENTAS files_to_export=%s", [n for n, _ in files_to_export])

        if len(files_to_export) > 1:
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w") as zf:
                for filename, xml_content in files_to_export:
                    zf.writestr(filename, xml_content)
            zip_buffer.seek(0)
            response = HttpResponse(zip_buffer.read(), content_type='application/zip')
            response['Content-Disposition'] = f'attachment; filename={today_str}_importui.zip'
            export_success = True
        elif len(files_to_export) == 1:
            filename, xml_content = files_to_export[0]
            response = HttpResponse(
                xml_content,
                content_type='application/xml; charset=utf-8'
            )
            response['Content-Disposition'] = f'attachment; filename={filename}'
            export_success = True
        else:
            logger.warning("[EXP] CENTAS nothing to export")
            response = Response({"error": "No documents to export"}, status=400)

    # ========================= RIVILƒñ (EIP) =========================

    elif export_type == 'rivile':
        logger.info("[EXP] RIVILE export started")
        assign_random_prekes_kodai(documents)

        files_to_export = []

        # 1) –ö–ª–∏–µ–Ω—Ç—ã (N08+N33): —Å–æ–±–∏—Ä–∞–µ–º –ò–ó –î–û–ö–£–ú–ï–ù–¢–û–í; –∫—ç—à –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–µ–Ω
        docs_for_clients = (pirkimai_docs or []) + (pardavimai_docs or [])
        if docs_for_clients:
            klientai_xml = export_clients_group_to_rivile(
                clients=None,
                documents=docs_for_clients,
            )
            if klientai_xml and klientai_xml.strip():
                files_to_export.append(('klientai.eip', klientai_xml))
                logger.info("[EXP] RIVILE clients exported")

        # 2) –ü–ò–†–ö–ò–ú–ê–ò (I06/I07)
        if pirkimai_docs:
            logger.info("[EXP] RIVILE exporting pirkimai: %d docs", len(pirkimai_docs))
            pirkimai_xml = export_pirkimai_group_to_rivile(pirkimai_docs, request.user)
            files_to_export.append(('pirkimai.eip', pirkimai_xml))

        # 3) –ü–ê–†–î–ê–í–ò–ú–ê–ò (I06/I07)
        if pardavimai_docs:
            logger.info("[EXP] RIVILE exporting pardavimai: %d docs", len(pardavimai_docs))
            pardavimai_xml = export_pardavimai_group_to_rivile(pardavimai_docs, request.user)
            files_to_export.append(('pardavimai.eip', pardavimai_xml))

        # 4) N17/N25 - –ò–ó–ú–ï–ù–ï–ù–û: –ø–µ—Ä–µ–¥–∞—ë–º request.user
        prekes_xml, paslaugos_xml, kodai_xml = export_prekes_paslaugos_kodai_group_to_rivile(
            documents, 
            request.user  # ‚Üê –î–û–ë–ê–í–õ–ï–ù–û
        )
        if prekes_xml and prekes_xml.strip():
            files_to_export.append(('prekes.eip', prekes_xml))
        if paslaugos_xml and paslaugos_xml.strip():
            files_to_export.append(('paslaugos.eip', paslaugos_xml))
        if kodai_xml and kodai_xml.strip():
            files_to_export.append(('kodai.eip', kodai_xml))

        logger.info("[EXP] RIVILE files_to_export=%s", [n for n, _ in files_to_export])

        # Jei profilyje nustatyta ‚Äûrivile_strip_lt_letters" ‚Äì nuimame diakritikƒÖ
        if rivile_strip_lt and files_to_export:
            new_files = []
            for filename, xml_content in files_to_export:
                if isinstance(xml_content, bytes):
                    try:
                        xml_text = xml_content.decode("utf-8", errors="ignore")
                    except Exception:
                        xml_text = xml_content.decode("latin-1", errors="ignore")
                else:
                    xml_text = xml_content

                stripped = strip_diacritics(xml_text)
                logger.info("[EXP] RIVILE strip_lt applied to %s (len %d -> %d)",
                            filename, len(xml_text), len(stripped))
                new_files.append((filename, stripped))
            files_to_export = new_files

        if files_to_export:
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w") as zf:
                for filename, xml_content in files_to_export:
                    zf.writestr(filename, xml_content)
            zip_buffer.seek(0)
            response = HttpResponse(zip_buffer.read(), content_type='application/zip')
            response['Content-Disposition'] = f'attachment; filename={today_str}_rivile_eip.zip'
            export_success = True
        else:
            logger.warning("[EXP] RIVILE nothing to export")
            response = Response({"error": "No documents to export"}, status=400)
    # elif export_type == 'rivile':
    #     logger.info("[EXP] RIVILE export started")
    #     assign_random_prekes_kodai(documents)

    #     files_to_export = []

    #     # 1) –ö–ª–∏–µ–Ω—Ç—ã (N08+N33): —Å–æ–±–∏—Ä–∞–µ–º –ò–ó –î–û–ö–£–ú–ï–ù–¢–û–í; –∫—ç—à –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–µ–Ω
    #     docs_for_clients = (pirkimai_docs or []) + (pardavimai_docs or [])
    #     if docs_for_clients:
    #         klientai_xml = export_clients_group_to_rivile(
    #             clients=None,
    #             documents=docs_for_clients,
    #         )
    #         if klientai_xml and klientai_xml.strip():
    #             files_to_export.append(('klientai.eip', klientai_xml))
    #             logger.info("[EXP] RIVILE clients exported")

    #     # 2) –ü–ò–†–ö–ò–ú–ê–ò (I06/I07)
    #     if pirkimai_docs:
    #         logger.info("[EXP] RIVILE exporting pirkimai: %d docs", len(pirkimai_docs))
    #         pirkimai_xml = export_pirkimai_group_to_rivile(pirkimai_docs, request.user)
    #         files_to_export.append(('pirkimai.eip', pirkimai_xml))

    #     # 3) –ü–ê–†–î–ê–í–ò–ú–ê–ò (I06/I07)
    #     if pardavimai_docs:
    #         logger.info("[EXP] RIVILE exporting pardavimai: %d docs", len(pardavimai_docs))
    #         pardavimai_xml = export_pardavimai_group_to_rivile(pardavimai_docs, request.user)
    #         files_to_export.append(('pardavimai.eip', pardavimai_xml))

    #     # 4) N17/N25
    #     prekes_xml, paslaugos_xml, kodai_xml = export_prekes_paslaugos_kodai_group_to_rivile(documents)
    #     if prekes_xml and prekes_xml.strip():
    #         files_to_export.append(('prekes.eip', prekes_xml))
    #     if paslaugos_xml and paslaugos_xml.strip():
    #         files_to_export.append(('paslaugos.eip', paslaugos_xml))
    #     if kodai_xml and kodai_xml.strip():
    #         files_to_export.append(('kodai.eip', kodai_xml))

    #     logger.info("[EXP] RIVILE files_to_export=%s", [n for n, _ in files_to_export])

    #     # Jei profilyje nustatyta ‚Äûrivile_strip_lt_letters" ‚Äì nuimame diakritikƒÖ
    #     if rivile_strip_lt and files_to_export:
    #         new_files = []
    #         for filename, xml_content in files_to_export:
    #             if isinstance(xml_content, bytes):
    #                 try:
    #                     xml_text = xml_content.decode("utf-8", errors="ignore")
    #                 except Exception:
    #                     xml_text = xml_content.decode("latin-1", errors="ignore")
    #             else:
    #                 xml_text = xml_content

    #             stripped = strip_diacritics(xml_text)
    #             logger.info("[EXP] RIVILE strip_lt applied to %s (len %d -> %d)",
    #                         filename, len(xml_text), len(stripped))
    #             new_files.append((filename, stripped))
    #         files_to_export = new_files

    #     if files_to_export:
    #         zip_buffer = io.BytesIO()
    #         with zipfile.ZipFile(zip_buffer, "w") as zf:
    #             for filename, xml_content in files_to_export:
    #                 zf.writestr(filename, xml_content)
    #         zip_buffer.seek(0)
    #         response = HttpResponse(zip_buffer.read(), content_type='application/zip')
    #         response['Content-Disposition'] = f'attachment; filename={today_str}_rivile_eip.zip'
    #         export_success = True
    #     else:
    #         logger.warning("[EXP] RIVILE nothing to export")
    #         response = Response({"error": "No documents to export"}, status=400)


    # ========================= FINVALDA =========================
    elif export_type == 'finvalda':
        logger.info("[EXP] FINVALDA export started")
        assign_random_prekes_kodai(documents)

        files_to_export = []

        if pirkimai_docs:
            logger.info("[EXP] FINVALDA exporting pirkimai: %d docs", len(pirkimai_docs))
            xml_bytes = export_pirkimai_group_to_finvalda(pirkimai_docs)
            files_to_export.append((f"{today_str}_pirkimai_finvalda.xml", xml_bytes))
        if pardavimai_docs:
            logger.info("[EXP] FINVALDA exporting pardavimai: %d docs", len(pardavimai_docs))
            xml_bytes = export_pardavimai_group_to_finvalda(pardavimai_docs)
            files_to_export.append((f"{today_str}_pardavimai_finvalda.xml", xml_bytes))

        logger.info("[EXP] FINVALDA files_to_export=%s", [n for n, _ in files_to_export])

        if len(files_to_export) > 1:
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w") as zf:
                for filename, xml_content in files_to_export:
                    zf.writestr(filename, xml_content)
            zip_buffer.seek(0)
            response = HttpResponse(zip_buffer.read(), content_type='application/zip')
            response['Content-Disposition'] = f'attachment; filename={today_str}_finvalda.zip'
            export_success = True
        elif len(files_to_export) == 1:
            filename, xml_content = files_to_export[0]
            response = HttpResponse(xml_content, content_type='application/xml')
            response['Content-Disposition'] = f'attachment; filename={filename}'
            export_success = True
        else:
            logger.warning("[EXP] FINVALDA nothing to export")
            response = Response({"error": "No documents to export"}, status=400)



    # ====================================================================
    # PRAGMA 3.2 - –¥–æ–±–∞–≤–∏—Ç—å —ç—Ç–æ—Ç –±–ª–æ–∫ –≤ views.py –ø–æ—Å–ª–µ –¥—Ä—É–≥–∏—Ö export_type
    # ====================================================================

    # ========================= PRAGMA 3.2 =========================
    elif export_type == 'pragma3':
        logger.info("[EXP] PRAGMA32 export started")
        assign_random_prekes_kodai(documents)

        files_to_export = []

        try:
            # –ü–æ–ª–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç —Å –∞–≤—Ç–æ–¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤ (4 —Ñ–∞–π–ª–∞)
            export_data = export_to_pragma_full(
                documents=(pirkimai_docs or []) + (pardavimai_docs or []),
                include_reference_data=True
            )
            
            logger.info("[EXP] PRAGMA32 export_data keys: %s", list(export_data.keys()))

            # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–π–ª—ã –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏
            if export_data.get('documents'):
                files_to_export.append((
                    f'{today_str}_pardavimai.txt',
                    export_data['documents']
                ))
            
            if export_data.get('items'):
                files_to_export.append((
                    f'{today_str}_pardavimai_det.txt',
                    export_data['items']
                ))
            
            if export_data.get('companies'):
                files_to_export.append((
                    f'{today_str}_Imones.txt',
                    export_data['companies']
                ))
            
            if export_data.get('products'):
                files_to_export.append((
                    f'{today_str}_Prekes.txt',
                    export_data['products']
                ))

            logger.info("[EXP] PRAGMA32 files_to_export=%s", [n for n, _ in files_to_export])

        except Exception as e:
            logger.exception("[EXP] PRAGMA32 export failed: %s", e)
            return Response({"error": "Pragma 3.2 export failed", "detail": str(e)}, status=500)

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
        if len(files_to_export) > 1:
            # –ù–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ -> ZIP
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w") as zf:
                for filename, txt_content in files_to_export:
                    zf.writestr(filename, txt_content)
            zip_buffer.seek(0)
            response = HttpResponse(zip_buffer.read(), content_type='application/zip')
            response['Content-Disposition'] = f'attachment; filename={today_str}_pragma32.zip'
            export_success = True
            
        elif len(files_to_export) == 1:
            # –û–¥–∏–Ω —Ñ–∞–π–ª -> –ø—Ä—è–º–∞—è –æ—Ç–¥–∞—á–∞
            filename, txt_content = files_to_export[0]
            response = HttpResponse(
                txt_content,
                content_type='text/plain; charset=windows-1257'
            )
            response['Content-Disposition'] = f'attachment; filename={filename}'
            export_success = True
            
        else:
            logger.warning("[EXP] PRAGMA32 nothing to export")
            response = Response({"error": "No documents to export"}, status=400)


    # ========================= PRAGMA 4.0 =========================
    elif export_type == 'pragma4':
        logger.info("[EXP] PRAGMA40 export started")
        assign_random_prekes_kodai(documents)

        # –¢–≤–æ–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç—ë—Ä: —Å–∞–º —Ä–µ—à–∞–µ—Ç, XML –∏–ª–∏ ZIP
        try:
            content = export_to_pragma40_xml(
                pirkimai_documents=pirkimai_docs,
                pardavimai_documents=pardavimai_docs
            )
        except Exception as e:
            logger.exception("[EXP] PRAGMA40 export failed: %s", e)
            return Response({"error": "Pragma 4.0 export failed", "detail": str(e)}, status=500)

        if not content:
            logger.warning("[EXP] PRAGMA40 nothing to export")
            response = Response({"error": "No documents to export"}, status=400)
        else:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, ZIP —ç—Ç–æ –∏–ª–∏ XML –ø–æ —Å–∏–≥–Ω–∞—Ç—É—Ä–µ ZIP ("PK")
            if content[:2] == b'PK':
                filename = f"{today_str}_pragma40.zip"
                content_type = "application/zip"
            else:
                # –ï—Å–ª–∏ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–ª–∏—Å—å —Ç–æ–ª—å–∫–æ –ø–æ–∫—É–ø–∫–∏/–ø—Ä–æ–¥–∞–∂–∏ ‚Äì –±–æ–ª–µ–µ –≥–æ–≤–æ—Ä—è—â–∏–µ –∏–º–µ–Ω–∞
                if pirkimai_docs and not pardavimai_docs:
                    filename = f"{today_str}_pragma40_pirkimai.xml"
                elif pardavimai_docs and not pirkimai_docs:
                    filename = f"{today_str}_pragma40_pardavimai.xml"
                else:
                    filename = f"{today_str}_pragma40.xml"

                content_type = "application/xml; charset=utf-8"

            response = HttpResponse(content, content_type=content_type)
            response['Content-Disposition'] = f'attachment; filename="{filename}"'
            export_success = True


    # ========================= DINETA =========================
    elif export_type == 'dineta':
        logger.info("[EXP] DINETA export started")

        # –î–ª—è Dineta –Ω–∞–º –ª–æ–≥–∏—á–Ω–µ–µ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å —Ç–æ–ª—å–∫–æ —É–∂–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏/–ø—Ä–æ–¥–∞–∂–∏
        all_docs = (pirkimai_docs or []) + (pardavimai_docs or [])

        if not all_docs:
            logger.warning("[EXP] DINETA no documents to export (no pirkimai/pardavimai)")
            return Response({"error": "No documents to send to Dineta"}, status=400)

        try:
            dineta_result = send_dineta_bundle(request.user, all_docs)
        except DinetaError as e:
            logger.exception("[EXP] DINETA export failed (DinetaError): %s", e)
            return Response(
                {
                    "error": "Dineta export failed",
                    "detail": str(e),
                },
                status=502,  # bad gateway / –≤–Ω–µ—à–Ω—è—è —Å–∏—Å—Ç–µ–º–∞
            )
        except Exception as e:
            logger.exception("[EXP] DINETA export failed (unexpected): %s", e)
            return Response(
                {
                    "error": "Dineta export failed (unexpected)",
                    "detail": str(e),
                },
                status=500,
            )

        # –ï—Å–ª–∏ —Å—é–¥–∞ –¥–æ—à–ª–∏ ‚Äì —Å—á–∏—Ç–∞–µ–º —ç–∫—Å–ø–æ—Ä—Ç —É—Å–ø–µ—à–Ω—ã–º
        export_success = True
        # response ‚Äî –æ–±—ã—á–Ω—ã–π JSON —Å —Ç–µ–º, —á—Ç–æ –≤–µ—Ä–Ω—É–ª–∞ Dineta
        response = Response(
            {
                "status": "ok",
                "dineta": dineta_result,
            },
            status=200,
        )



    # ========================= Butent =========================
    elif export_type == 'butent':
        logger.info("[EXP] BUTENT export started")
        assign_random_prekes_kodai(documents)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ (B≈´tent –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–º–µ—à–∏–≤–∞–Ω–∏–µ)
        all_docs = (pirkimai_docs or []) + (pardavimai_docs or [])
        
        if not all_docs:
            logger.warning("[EXP] BUTENT no documents to export")
            return Response({"error": "No documents to export"}, status=400)

        try:
            # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ Excel (mode='auto' –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Dict[str, bytes])
            result = export_to_butent(
                documents=all_docs,
                mode='auto',
                user=request.user
            )
            
            logger.info("[EXP] BUTENT export completed, files=%s", list(result.keys()))
            
            # –ï—Å–ª–∏ –æ–¥–∏–Ω —Ñ–∞–π–ª - –æ—Ç–¥–∞–µ–º –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é
            if len(result) == 1:
                mode, excel_bytes = list(result.items())[0]
                filename = f'{today_str}_butent_{mode}_import.xlsx'
                
                logger.info("[EXP] BUTENT single file: %s, size=%d bytes", filename, len(excel_bytes))
                
                response = HttpResponse(
                    excel_bytes,
                    content_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
                )
                response['Content-Disposition'] = f'attachment; filename="{filename}"'
                export_success = True
            
            # –ï—Å–ª–∏ –¥–≤–∞ —Ñ–∞–π–ª–∞ - —Å–æ–∑–¥–∞–µ–º ZIP –∞—Ä—Ö–∏–≤
            else:
                import zipfile
                from io import BytesIO
                
                zip_buffer = BytesIO()
                
                with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                    for mode, excel_bytes in result.items():
                        filename = f'{today_str}_butent_{mode}_import.xlsx'
                        zip_file.writestr(filename, excel_bytes)
                        logger.info("[EXP] BUTENT added to ZIP: %s, size=%d bytes", filename, len(excel_bytes))
                
                zip_buffer.seek(0)
                zip_bytes = zip_buffer.read()
                
                logger.info("[EXP] BUTENT ZIP created, size=%d bytes", len(zip_bytes))
                
                response = HttpResponse(zip_bytes, content_type='application/zip')
                response['Content-Disposition'] = f'attachment; filename="{today_str}_butent_import.zip"'
                export_success = True

        except FileNotFoundError as e:
            logger.error("[EXP] BUTENT template not found: %s", e)
            return Response({
                "error": "B≈´tent template not found",
                "detail": "Please create template using create_butent_template()"
            }, status=500)
        
        except Exception as e:
            logger.exception("[EXP] BUTENT export failed: %s", e)
            return Response({
                "error": "B≈´tent export failed",
                "detail": str(e)
            }, status=500)


    # ========================= APSKAITA5 =========================
    elif export_type == 'apskaita5':
        logger.info("[EXP] APSKAITA5 export started")
        assign_random_prekes_kodai(documents)

        content, filename, content_type = export_documents_group_to_apskaita5_files(
            documents=documents,
            site_url=site_url,   # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –≤—ã—à–µ –ø–æ –º–æ–¥—É–ª—é/–∫–æ–Ω—Ñ–∏–≥—É
            company_code=None,
            direction=None,
        )
        logger.info("[EXP] APSKAITA5 produced file=%s content_type=%s size=%d",
                    filename, content_type, len(content))
        response = HttpResponse(content, content_type=content_type)
        response['Content-Disposition'] = f'attachment; filename="{filename}"'
        response['X-Content-Type-Options'] = 'nosniff'
        export_success = True

    # ========================= AGNUM =========================
    elif export_type == 'agnum':
        logger.info("[EXP] AGNUM export started")
        assign_random_prekes_kodai(documents)

        files_to_export = []

        # 1) Pirkimai (Type="2")
        if pirkimai_docs:
            logger.info("[EXP] AGNUM exporting pirkimai: %d docs", len(pirkimai_docs))
            pirkimai_xml = export_pirkimai_group_to_agnum(pirkimai_docs, request.user)
            files_to_export.append((f'{today_str}_pirkimai_agnum.xml', pirkimai_xml))

        # 2) Pardavimai (Type="4")
        if pardavimai_docs:
            logger.info("[EXP] AGNUM exporting pardavimai: %d docs", len(pardavimai_docs))
            pardavimai_xml = export_pardavimai_group_to_agnum(pardavimai_docs, request.user)
            files_to_export.append((f'{today_str}_pardavimai_agnum.xml', pardavimai_xml))

        logger.info("[EXP] AGNUM files_to_export=%s", [n for n, _ in files_to_export])

        if len(files_to_export) > 1:
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w") as zf:
                for filename, xml_content in files_to_export:
                    zf.writestr(filename, xml_content)
            zip_buffer.seek(0)
            response = HttpResponse(zip_buffer.read(), content_type='application/zip')
            response['Content-Disposition'] = f'attachment; filename={today_str}_agnum.zip'
            export_success = True
        elif len(files_to_export) == 1:
            filename, xml_content = files_to_export[0]
            response = HttpResponse(
                xml_content,
                content_type='application/xml; charset=utf-8'
            )
            response['Content-Disposition'] = f'attachment; filename={filename}'
            export_success = True
        else:
            logger.warning("[EXP] AGNUM nothing to export")
            response = Response({"error": "No documents to export"}, status=400)



    # ========================= RIVILƒñ ERP (XLSX) =========================
    elif export_type == 'rivile_erp':
        logger.info("[EXP] RIVILE_ERP export started")
        assign_random_prekes_kodai(documents)
        rivile_defaults = getattr(request.user, "rivile_erp_extra_fields", None) or {}

        klientai = []
        seen = set()

        for pack in (prepared.get("pirkimai", []) + prepared.get("pardavimai", [])):
            doc = pack["doc"]
            dir_ = pack.get("direction")

            if dir_ == 'pirkimas':
                is_person = doc.seller_is_person
                klient_type = 'pirkimas'
                # –ö–æ–¥ –∫–ª–∏–µ–Ω—Ç–∞: id ‚Üí vat_code ‚Üí id_programoje (–∫–∞–∫ –≤ get_party_code)
                client_code = doc.seller_id or doc.seller_vat_code or doc.seller_id_programoje or ""
                client = {
                    'id': client_code,
                    'vat': doc.seller_vat_code or "",
                    'name': doc.seller_name or "",
                    'address': doc.seller_address or "",
                    'country_iso': doc.seller_country_iso or "",
                    'currency': doc.currency or "EUR",
                    'kodas_ds': 'PT001',
                    'type': klient_type,
                    'is_person': is_person,
                    'iban': doc.seller_iban or "",
                }
            elif dir_ == 'pardavimas':
                is_person = doc.buyer_is_person
                klient_type = 'pardavimas'
                # –ö–æ–¥ –∫–ª–∏–µ–Ω—Ç–∞: id ‚Üí vat_code ‚Üí id_programoje (–∫–∞–∫ –≤ get_party_code)
                client_code = doc.buyer_id or doc.buyer_vat_code or doc.buyer_id_programoje or ""
                client = {
                    'id': client_code,
                    'vat': doc.buyer_vat_code or "",
                    'name': doc.buyer_name or "",
                    'address': doc.buyer_address or "",
                    'country_iso': doc.buyer_country_iso or "",
                    'currency': doc.currency or "EUR",
                    'kodas_ds': 'PT001',
                    'type': klient_type,
                    'is_person': is_person,
                    'iban': doc.buyer_iban or "",
                }
            else:
                continue

            client_key = (client['id'], client['vat'], client['name'], client['type'])
            if client['id'] and client_key not in seen:
                klientai.append(client)
                seen.add(client_key)

        logger.info("[EXP] RIVILE_ERP klientai=%d docs_pirk=%d docs_pard=%d",
                    len(klientai), len(pirkimai_docs), len(pardavimai_docs))

        files_to_export = []

        if klientai:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
                export_clients_to_rivile_erp_xlsx(klientai, tmp.name)
                tmp.seek(0)
                klientai_xlsx_bytes = tmp.read()
            files_to_export.append((f'klientai_{today_str}.xlsx', klientai_xlsx_bytes))

        with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
            export_prekes_and_paslaugos_to_rivile_erp_xlsx(documents, tmp.name)
            tmp.seek(0)
            prekes_xlsx_bytes = tmp.read()
        files_to_export.append((f'prekes_paslaugos_{today_str}.xlsx', prekes_xlsx_bytes))

        if pirkimai_docs:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
                export_documents_to_rivile_erp_xlsx(
                    pirkimai_docs,
                    tmp.name,
                    doc_type="pirkimai",
                    rivile_erp_extra_fields=rivile_defaults,  # üîπ –≤–æ—Ç –∑–¥–µ—Å—å
                )
                tmp.seek(0)
                pirkimai_xlsx_bytes = tmp.read()
            files_to_export.append((f'pirkimai_{today_str}.xlsx', pirkimai_xlsx_bytes))

        if pardavimai_docs:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
                export_documents_to_rivile_erp_xlsx(
                    pardavimai_docs,
                    tmp.name,
                    doc_type="pardavimai",
                    rivile_erp_extra_fields=rivile_defaults,  # üîπ –∏ –∑–¥–µ—Å—å
                )
                tmp.seek(0)
                pardavimai_xlsx_bytes = tmp.read()
            files_to_export.append((f'pardavimai_{today_str}.xlsx', pardavimai_xlsx_bytes))

        logger.info("[EXP] RIVILE_ERP files_to_export=%s", [n for n, _ in files_to_export])

        if len(files_to_export) > 1:
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w") as zf:
                for filename, file_bytes in files_to_export:
                    zf.writestr(filename, file_bytes)
            zip_buffer.seek(0)
            response = HttpResponse(zip_buffer.read(), content_type='application/zip')
            response['Content-Disposition'] = f'attachment; filename={today_str}_rivile_erp.zip'
            export_success = True
        elif len(files_to_export) == 1:
            filename, file_bytes = files_to_export[0]
            response = HttpResponse(
                file_bytes,
                content_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
            )
            response['Content-Disposition'] = f'attachment; filename={filename}'
            export_success = True
        else:
            logger.warning("[EXP] RIVILE_ERP nothing to export")
            response = Response({"error": "No clients or products to export"}, status=400)




    else:
        logger.error("[EXP] unknown export type: %s", export_type)
        return Response({"error": "Unknown export type"}, status=400)

    # --- universal finalize ---
    if response is not None:
        try:
            if export_success and exported_ids:
                ScannedDocument.objects.filter(pk__in=exported_ids).update(status="exported")
                logger.info("[EXP] Marked %d documents as exported (universal)", len(exported_ids))
        except Exception as e:
            logger.warning("[EXP] Failed to mark documents as exported: %s", e)
        return response

    logger.warning("[EXP] fell through unexpectedly")
    return Response({"error": "No documents to export"}, status=400)



# Soxranenije user infy s Dineta
class DinetaSettingsView(APIView):
    permission_classes = [IsAuthenticated]

    def get(self, request):
        """
        –í–µ—Ä–Ω—É—Ç—å —Ç–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Dineta —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        –ü–∞—Ä–æ–ª—å –ù–ï –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è.
        """
        user = request.user
        settings_dict = user.dineta_settings or {}

        serializer = DinetaSettingsSerializer(instance=settings_dict)
        return Response(serializer.data, status=status.HTTP_200_OK)

    def put(self, request):
        """
        –û–±–Ω–æ–≤–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Dineta.
        –§—Ä–æ–Ω—Ç —à–ª—ë—Ç server/client/username/password (+ –æ–ø—Ü–∏–∏).
        """
        user = request.user

        serializer = DinetaSettingsSerializer(data=request.data)
        serializer.is_valid(raise_exception=True)

        settings_to_store = serializer.build_settings_dict()

        user.dineta_settings = settings_to_store
        user.save(update_fields=["dineta_settings"])

        # –≤ –æ—Ç–≤–µ—Ç –æ—Ç–¥–∞—ë–º –±–µ–∑ –ø–∞—Ä–æ–ª—è (serializer.instance ‚Üí dict)
        response_serializer = DinetaSettingsSerializer(instance=settings_to_store)
        return Response(response_serializer.data, status=status.HTTP_200_OK)






@api_view(['POST'])
@permission_classes([IsAuthenticated])
def process_image(request):
    raw_files = request.FILES.getlist("files")
    scan_type = request.data.get("scan_type", "sumiskai")

    if not raw_files:
        return Response({'error': '–§–∞–π–ª—ã –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã'}, status=400)

    user = request.user

    # –í—ã–±–∏—Ä–∞–µ–º —Ü–µ–Ω—É –∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç
    if scan_type == "detaliai":
        credits_per_doc = Decimal("1.3")
    else:
        credits_per_doc = Decimal("1")

    files_count = len(raw_files)

    # --- –ü–†–û–í–ï–†–ö–ê –∫—Ä–µ–¥–∏—Ç–æ–≤ –î–û –æ–±—Ä–∞–±–æ—Ç–∫–∏ ---
    if user.credits < credits_per_doc * files_count:
        return Response({
            'error': f'Nepakanka kredit≈≥. Liko ‚Äì {user.credits}, reikia ‚Äì {credits_per_doc * files_count}.'
        }, status=402)

    results = []
    for raw_file in raw_files:
        original_filename = raw_file.name

        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–ø–∏—Å—å –≤ –ë–î —Å—Ä–∞–∑—É!
        doc = ScannedDocument.objects.create(
            user=user,
            original_filename=original_filename,
            status='processing',
            scan_type=scan_type
        )
        doc.file.save(original_filename, raw_file)
        doc.save()

        # 2. –ó–∞–ø—É—Å–∫–∞–µ–º celery-task c ID
        process_uploaded_file_task.delay(
            user.id,
            doc.id,
            scan_type
        )

        results.append({
            "id": doc.id,
            "original_filename": original_filename,
            "status": "processing",
            "uploaded_at": doc.uploaded_at
        })

    return Response({
        'status': 'processing',
        'results': results,
        'msg': 'Dokumentai u≈æregistruoti ir apdorojami. Po keli≈≥ sekund≈æi≈≥ statusas atsinaujins.'
    })






# Obnovit company details v Nustatymai

@api_view(['PATCH'])
@permission_classes([IsAuthenticated])
def update_own_company_details(request):
    user = request.user
    serializer = CustomUserSerializer(user, data=request.data, partial=True)
    if serializer.is_valid():
        serializer.save()
        return Response(serializer.data)
    return Response(serializer.errors, status=400)



# Udaliajet zapisi s dashboard i BD

@api_view(['DELETE'])
@permission_classes([IsAuthenticated])
def bulk_delete_documents(request):
    ids = request.data.get('ids', [])
    if not ids:
        return Response({'error': 'No IDs provided'}, status=status.HTTP_400_BAD_REQUEST)
    # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ç–µ–∫—É—â–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    queryset = ScannedDocument.objects.filter(id__in=ids, user=request.user)
    deleted, _ = queryset.delete()
    return Response({'deleted': deleted}, status=status.HTTP_200_OK)




# #poluciajem vsio infu iz BD dlia otobrazhenija v dashboard pri zagruzke

# /documents/
@api_view(['GET'])
@permission_classes([IsAuthenticated])
def get_user_documents(request):
    user = request.user
    status = request.GET.get('status')
    date_from = request.GET.get('from')
    date_to = request.GET.get('to')

    docs = ScannedDocument.objects.filter(user=user)
    if status:
        docs = docs.filter(status=status)
    if date_from:
        docs = docs.filter(created_at__date__gte=parse_date(date_from))
    if date_to:
        docs = docs.filter(created_at__date__lte=parse_date(date_to))

    serializer = ScannedDocumentListSerializer(docs.order_by('-uploaded_at'), many=True)
    return Response(serializer.data)





# @api_view(['GET'])
# @permission_classes([IsAuthenticated])
# def get_document_detail(request, pk):
#     """
#     –î–µ—Ç–∞–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
#     - Superuser: –º–æ–∂–µ—Ç —Å–º–æ—Ç—Ä–µ—Ç—å –õ–Æ–ë–û–ô –¥–æ–∫—É–º–µ–Ω—Ç; –µ—Å–ª–∏ view_mode == "multi", –¥–æ–±–∞–≤–ª—è–µ–º preview.
#     - –û–±—ã—á–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:
#         * single-—Ä–µ–∂–∏–º ‚Äî –ø—Ä–æ—Å—Ç–æ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î;
#         * multi-—Ä–µ–∂–∏–º ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º preview (–Ω–∏—á–µ–≥–æ –≤ –ë–î –Ω–µ –ø–∏—à–µ–º).
#     """
#     user = request.user

#     # --- –°—É–ø–µ—Ä—é–∑–µ—Ä ---
#     if user.is_superuser:
#         doc = get_object_or_404(ScannedDocument, pk=pk)
#         ser = ScannedDocumentAdminDetailSerializer(doc, context={'request': request})
#         data = ser.data

#         if getattr(user, "view_mode", None) == "multi":
#             cp_key = request.query_params.get("cp_key")
#             preview = build_preview(
#                 doc,
#                 user,
#                 cp_key=cp_key,
#                 view_mode="multi",
#                 base_vat_percent=data.get("vat_percent"),
#                 base_preke_paslauga=data.get("preke_paslauga"),
#             )
#             data["preview"] = preview

#         return Response(data)

#     # --- –û–±—ã—á–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å (—Ç–æ–ª—å–∫–æ —Å–≤–æ–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã) ---
#     doc = get_object_or_404(ScannedDocument, pk=pk, user=user)
#     ser = ScannedDocumentDetailSerializer(doc, context={'request': request})
#     data = ser.data

#     # preview —Ç–æ–ª—å–∫–æ –≤ multi-—Ä–µ–∂–∏–º–µ
#     if getattr(user, "view_mode", None) != "multi":
#         return Response(data)

#     cp_key = request.query_params.get("cp_key")
#     preview = build_preview(
#         doc,
#         user,
#         cp_key=cp_key,
#         view_mode="multi",
#         base_vat_percent=data.get("vat_percent"),
#         base_preke_paslauga=data.get("preke_paslauga"),
#     )
#     data["preview"] = preview
#     return Response(data)

@api_view(['GET'])
@permission_classes([IsAuthenticated])
def get_document_detail(request, pk):
    """
    –î–µ—Ç–∞–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    - Superuser: –º–æ–∂–µ—Ç —Å–º–æ—Ç—Ä–µ—Ç—å –õ–Æ–ë–û–ô –¥–æ–∫—É–º–µ–Ω—Ç; –µ—Å–ª–∏ view_mode == "multi", –¥–æ–±–∞–≤–ª—è–µ–º preview.
    - –û–±—ã—á–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:
        * single-—Ä–µ–∂–∏–º ‚Äî –ø—Ä–æ—Å—Ç–æ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î;
        * multi-—Ä–µ–∂–∏–º ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º preview (–Ω–∏—á–µ–≥–æ –≤ –ë–î –Ω–µ –ø–∏—à–µ–º).
    """
    user = request.user

    # Prefetch —Å —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–æ–π –¥–ª—è line_items
    line_items_prefetch = Prefetch(
        'line_items',
        queryset=LineItem.objects.order_by('id')
    )

    # --- –°—É–ø–µ—Ä—é–∑–µ—Ä ---
    if user.is_superuser:
        doc = get_object_or_404(
            ScannedDocument.objects.prefetch_related(line_items_prefetch),
            pk=pk
        )
        ser = ScannedDocumentAdminDetailSerializer(doc, context={'request': request})
        data = ser.data

        if getattr(user, "view_mode", None) == "multi":
            cp_key = request.query_params.get("cp_key")
            preview = build_preview(
                doc,
                user,
                cp_key=cp_key,
                view_mode="multi",
                base_vat_percent=data.get("vat_percent"),
                base_preke_paslauga=data.get("preke_paslauga"),
            )
            data["preview"] = preview

        return Response(data)

    # --- –û–±—ã—á–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å (—Ç–æ–ª—å–∫–æ —Å–≤–æ–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã) ---
    doc = get_object_or_404(
        ScannedDocument.objects.prefetch_related(line_items_prefetch),
        pk=pk,
        user=user
    )
    ser = ScannedDocumentDetailSerializer(doc, context={'request': request})
    data = ser.data

    # preview —Ç–æ–ª—å–∫–æ –≤ multi-—Ä–µ–∂–∏–º–µ
    if getattr(user, "view_mode", None) != "multi":
        return Response(data)

    cp_key = request.query_params.get("cp_key")
    preview = build_preview(
        doc,
        user,
        cp_key=cp_key,
        view_mode="multi",
        base_vat_percent=data.get("vat_percent"),
        base_preke_paslauga=data.get("preke_paslauga"),
    )
    data["preview"] = preview
    return Response(data)


# @api_view(['GET'])
# @permission_classes([IsAuthenticated])
# def get_document_detail(request, pk):
#     """
#     –î–µ—Ç–∞–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
#     - –í single-—Ä–µ–∂–∏–º–µ: –æ—Ç–¥–∞–µ–º —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î (–±–µ–∑ preview).
#     - –í multi-—Ä–µ–∂–∏–º–µ: –¥–æ–±–∞–≤–ª—è–µ–º preview —á–µ—Ä–µ–∑ data_resolver.build_preview (–Ω–∏—á–µ–≥–æ –≤ –ë–î –Ω–µ –ø–∏—à–µ–º).
#     """
#     try:
#         doc = ScannedDocument.objects.get(pk=pk, user=request.user)
#     except ScannedDocument.DoesNotExist:
#         return Response({'error': 'Not found'}, status=404)

#     serializer = ScannedDocumentDetailSerializer(doc)
#     data = serializer.data

#     # –ü—Ä–µ–≤—å—é —Ç–æ–ª—å–∫–æ –≤ multi
#     if getattr(request.user, "view_mode", None) != "multi":
#         return Response(data)

#     cp_key = request.query_params.get("cp_key")
#     preview = build_preview(
#         doc,
#         request.user,
#         cp_key=cp_key,
#         view_mode="multi",
#         base_vat_percent=data.get("vat_percent"),
#         base_preke_paslauga=data.get("preke_paslauga"),
#     )

#     data["preview"] = preview
#     return Response(data)






#Obnovit extra field vzavisimosti ot vybranoj buh programy
@api_view(['PATCH'])
@permission_classes([IsAuthenticated])
def update_scanned_document_extra_fields(request, pk):
    import logging
    from django.db import transaction
    from .models import ScannedDocument, LineItem
    from .serializers import ScannedDocumentSerializer
    from .utils.pirkimas_pardavimas import determine_pirkimas_pardavimas
    from .validators.vat_klas import auto_select_pvm_code
    from .utils.save_document import _apply_sumiskai_defaults_from_user
    from .validators.required_fields_checker import check_required_fields_for_export  # –î–û–ë–ê–í–ò–¢–¨

    log = logging.getLogger("docscanner_app.api.update_extra_fields")

    doc = ScannedDocument.objects.filter(pk=pk, user=request.user).first()
    if not doc:
        log.warning("PATCH extra_fields pk=%s: document not found for user=%s", pk, request.user.id)
        return Response({'error': 'Dokumentas nerastas'}, status=404)

    ALLOWED_FIELDS = [
        'buyer_id', 'buyer_name', 'buyer_vat_code', 'buyer_iban', 'buyer_address', 'buyer_country_iso',
        'seller_id', 'seller_name', 'seller_vat_code', 'seller_iban', 'seller_address', 'seller_country_iso',
        'prekes_kodas', 'prekes_barkodas', 'prekes_pavadinimas', 'preke_paslauga',
        'vat_percent', 'scan_type', 'doc_96_str',
    ]

    # helpers
    # def _is_cleared(prefix: str) -> bool:
    #     keys = [
    #         f"{prefix}_name", f"{prefix}_id", f"{prefix}_vat_code",
    #         f"{prefix}_iban", f"{prefix}_address", f"{prefix}_country_iso",
    #     ]
    #     touched = any(k in request.data for k in keys)
    #     if not touched:
    #         return False
    #     return all(not str(request.data.get(k) or "").strip() for k in keys)

    def _is_cleared(prefix: str) -> bool:
        keys = [
            f"{prefix}_name", f"{prefix}_id", f"{prefix}_vat_code",
            f"{prefix}_iban", f"{prefix}_address", f"{prefix}_country_iso",
        ]
        provided = [k for k in keys if k in request.data]  # —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω–æ –ø—Ä–∏—Å–ª–∞–Ω–Ω—ã–µ
        if not provided:
            return False
        return all(not str(request.data.get(k) or "").strip() for k in provided)

    def _to_bool_allow(x):
        if x is None: return None
        if isinstance(x, bool): return x
        s = str(x).strip().lower()
        if s in {"0","false","no","ne","off"}: return False
        if s in {"1","true","taip","yes","on"}: return True
        return None

    def _normalize_ps(v):
        if v is None: return None
        if isinstance(v, int): return v if v in (1,2,3,4) else None
        s = str(v).strip()
        return int(s) if s.isdigit() and int(s) in (1,2,3,4) else None

    def _normalize_vat_percent(v):
        if v is None: return None
        try:
            from decimal import Decimal
            if isinstance(v, Decimal): return float(v)
            s = str(v).strip().replace(",", ".")
            if not s: return None
            if s.endswith("%"): s = s[:-1]
            return float(Decimal(s))
        except Exception:
            return None

    def _nz(s):
        if s is None: return None
        s2 = str(s).strip()
        return s2 if s2 else None

    # –ø—Ä–∏–º–µ–Ω—è–µ–º –≤—Ö–æ–¥–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è (—Å—ã—Ä–æ), –ª–æ–≥–∏—Ä—É–µ–º
    fields_to_update = []
    for field in ALLOWED_FIELDS:
        if field in request.data:
            old_val = getattr(doc, field, None)
            new_val = request.data[field]
            setattr(doc, field, new_val)
            fields_to_update.append(field)
            if str(old_val) != str(new_val):
                log.info("pk=%s: field %s changed: %r -> %r", pk, field, old_val, new_val)

    buyer_cleared = _is_cleared("buyer")
    seller_cleared = _is_cleared("seller")
    if buyer_cleared or seller_cleared:
        log.info("pk=%s: clear detected: buyer_cleared=%s seller_cleared=%s", pk, buyer_cleared, seller_cleared)

    apply_defaults_req = _to_bool_allow(request.data.get("apply_defaults", None))
    log.info("pk=%s: apply_defaults_req=%r", pk, apply_defaults_req)

    with transaction.atomic():
        # 0) –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–∏—Å–ª–∞–Ω–Ω—ã–µ –ø–æ–ª—è
        if fields_to_update:
            doc.save(update_fields=fields_to_update)

        # 1) –ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å pirkimas/pardavimas
        doc_struct = {
            "seller_id": doc.seller_id,
            "seller_vat_code": doc.seller_vat_code,
            "seller_name": doc.seller_name,
            "buyer_id": doc.buyer_id,
            "buyer_vat_code": doc.buyer_vat_code,
            "buyer_name": doc.buyer_name,
        }
        doc.pirkimas_pardavimas = determine_pirkimas_pardavimas(doc_struct, request.user)
        log.info("pk=%s: pirkimas_pardavimas=%r", pk, doc.pirkimas_pardavimas)

        # 1.1) –§–ª–∞–≥–∏ –Ω–∞–ª–∏—á–∏—è VAT –∫–æ–¥–∞
        buyer_has_vat_code = bool((doc.buyer_vat_code or "").strip())
        seller_has_vat_code = bool((doc.seller_vat_code or "").strip())
        if hasattr(doc, "buyer_has_vat_code"):
            doc.buyer_has_vat_code = buyer_has_vat_code
        if hasattr(doc, "seller_has_vat_code"):
            doc.seller_has_vat_code = seller_has_vat_code
        log.info("pk=%s: buyer_has_vat_code=%s seller_has_vat_code=%s", pk, buyer_has_vat_code, seller_has_vat_code)

        # 1.2) –ï–°–õ–ò –æ—á–∏—â–∞–µ–º buyer/seller ‚Äî —á–∏—Å—Ç–∏–º —Ç–æ–≤–∞—Ä–Ω—ã–µ –ø–æ–ª—è –∏ PVM –ò –í–´–•–û–î–ò–ú –†–ê–ù–û
        if buyer_cleared or seller_cleared:
            doc.prekes_pavadinimas = ""
            doc.prekes_kodas = ""
            doc.prekes_barkodas = ""
            doc.preke_paslauga = ""
            doc.pvm_kodas = None
            update_fields_now = ["prekes_pavadinimas","prekes_kodas","prekes_barkodas","preke_paslauga","pvm_kodas","pirkimas_pardavimas"]

            if hasattr(doc, "buyer_has_vat_code"): update_fields_now.append("buyer_has_vat_code")
            if hasattr(doc, "seller_has_vat_code"): update_fields_now.append("seller_has_vat_code")

            if (doc.scan_type or "").strip().lower() == "detaliai":
                cleared = LineItem.objects.filter(document=doc).update(pvm_kodas=None)
                log.info("pk=%s: cleared LineItem.pvm_kodas for %d items", pk, cleared)

            doc.save(update_fields=update_fields_now)
            
            # ============ –î–û–ë–ê–í–ò–¢–¨ –í–ê–õ–ò–î–ê–¶–ò–Æ –ü–ï–†–ï–î –†–ê–ù–ù–ò–ú –í–û–ó–í–†–ê–¢–û–ú ============
            try:
                is_valid = check_required_fields_for_export(doc)
                doc.ready_for_export = is_valid
                doc.save(update_fields=['ready_for_export'])
                log.info("pk=%s: validated after clear, ready_for_export=%s", pk, is_valid)
            except Exception as e:
                log.error("pk=%s: validation error after clear: %s", pk, str(e))
            # ====================================================================
            
            log.info("pk=%s: PVM cleared due to party clear, early return", pk)
            return Response(ScannedDocumentSerializer(doc).data)

        # 2) –ü—Ä–∏–º–µ–Ω–∏—Ç—å –¥–µ—Ñ–æ–ª—Ç—ã (sumiskai, –µ—Å–ª–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ)
        scan_type = (doc.scan_type or "").strip().lower()
        allow_defaults = (scan_type == "sumiskai" and (apply_defaults_req is None or apply_defaults_req is True))
        if allow_defaults:
            changed = _apply_sumiskai_defaults_from_user(doc, request.user)
            log.info("pk=%s: defaults applied=%s", pk, changed)
            if changed:
                doc.save(update_fields=["prekes_pavadinimas","prekes_kodas","prekes_barkodas","preke_paslauga"])

        # 3) –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞
        buyer_iso = _nz(doc.buyer_country_iso)
        seller_iso = _nz(doc.seller_country_iso)
        doc_vat_norm = _normalize_vat_percent(doc.vat_percent)
        doc_ps = _normalize_ps(doc.preke_paslauga)

        log.info("pk=%s: buyer_iso=%r seller_iso=%r vat_percent_norm=%r preke_paslauga_norm=%r",
                 pk, buyer_iso, seller_iso, doc_vat_norm, doc_ps)

        # —Ç—Ä–µ–±—É–µ–º —Å—Ç—Ä–∞–Ω—ã/–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ 0%
        need_countries_doc = (doc_vat_norm == 0.0)
        missing_crit = need_countries_doc and (
            doc.pirkimas_pardavimas not in ("pirkimas", "pardavimas") or not (buyer_iso and seller_iso)
        )
        log.info("pk=%s: need_countries_doc=%s missing_crit=%s", pk, need_countries_doc, missing_crit)

        # 4) –ü–µ—Ä–µ—Å—á—ë—Ç PVM
        if scan_type == "detaliai":
            items = list(LineItem.objects.filter(document=doc))
            pvm_codes = set()
            vat_percents = set()
            log.info("pk=%s: recalc items count=%d", pk, len(items))

            for item in items:
                item_vat = item.vat_percent if item.vat_percent is not None else doc.vat_percent
                item_vat_norm = _normalize_vat_percent(item_vat)
                item_ps = _normalize_ps(item.preke_paslauga)
                if item_ps is None:
                    item_ps = doc_ps

                item_pvm = auto_select_pvm_code(
                    pirkimas_pardavimas=doc.pirkimas_pardavimas,
                    buyer_country_iso=buyer_iso,
                    seller_country_iso=seller_iso,
                    preke_paslauga=item_ps,
                    vat_percent=item_vat_norm,
                    separate_vat=bool(doc.separate_vat),
                    buyer_has_vat_code=buyer_has_vat_code,
                    seller_has_vat_code=seller_has_vat_code,
                    doc_96_str=bool(getattr(doc, "doc_96_str", False)),
                )

                old = item.pvm_kodas
                item.pvm_kodas = item_pvm
                item.save(update_fields=["pvm_kodas"])
                log.info("pk=%s: item[%s] vat=%r ps=%r -> pvm %r (was %r)",
                         pk, item.id, item_vat_norm, item_ps, item_pvm, old)

                if item_pvm is not None: pvm_codes.add(item_pvm)
                if item_vat_norm is not None: vat_percents.add(item_vat_norm)

            if bool(doc.separate_vat):
                doc.pvm_kodas = "Keli skirtingi PVM"
                doc.vat_percent = None
                log.info("pk=%s: separate_vat=True -> doc.pvm_kodas='Keli skirtingi PVM'", pk)
            else:
                if len(pvm_codes) == 1 and len(vat_percents) == 1:
                    doc.pvm_kodas = next(iter(pvm_codes))
                    doc.vat_percent = next(iter(vat_percents))
                    log.info("pk=%s: unified items -> doc.pvm_kodas=%r vat_percent=%r",
                             pk, doc.pvm_kodas, doc.vat_percent)
                else:
                    doc.pvm_kodas = ""
                    doc.vat_percent = None
                    log.info("pk=%s: heterogeneous items -> doc.pvm_kodas cleared", pk)

        else:
            # sumiskai / detaliai –±–µ–∑ —Å—Ç—Ä–æ–∫ ‚Äî –¥–æ–∫—É–º–µ–Ω—Ç–Ω—ã–π —Ä–∞—Å—á—ë—Ç
            doc_pvm = auto_select_pvm_code(
                pirkimas_pardavimas=doc.pirkimas_pardavimas,
                buyer_country_iso=buyer_iso,
                seller_country_iso=seller_iso,
                preke_paslauga=doc_ps,
                vat_percent=doc_vat_norm,
                separate_vat=bool(doc.separate_vat),
                buyer_has_vat_code=buyer_has_vat_code,
                seller_has_vat_code=seller_has_vat_code,
                doc_96_str=bool(getattr(doc, "doc_96_str", False)),
            )
            old_doc_pvm = doc.pvm_kodas
            doc.pvm_kodas = doc_pvm
            log.info("pk=%s: doc-level recalc -> pvm %r (was %r)", pk, doc_pvm, old_doc_pvm)

        # 5) –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
        update_set = {"pirkimas_pardavimas","pvm_kodas","vat_percent"}
        if hasattr(doc, "buyer_has_vat_code"): update_set.add("buyer_has_vat_code")
        if hasattr(doc, "seller_has_vat_code"): update_set.add("seller_has_vat_code")

        doc.save(update_fields=list(update_set))
        log.info("pk=%s: saved fields=%s", pk, sorted(update_set))

    # ============ –î–û–ë–ê–í–ò–¢–¨ –í–ê–õ–ò–î–ê–¶–ò–Æ –í –ö–û–ù–¶–ï ============
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –ø–æ–ª—è buyer/seller
        buyer_seller_changed = any(
            k.startswith(('buyer_', 'seller_')) 
            for k in request.data.keys()
        )
        
        if buyer_seller_changed:
            is_valid = check_required_fields_for_export(doc)
            doc.ready_for_export = is_valid
            doc.save(update_fields=['ready_for_export'])
            log.info("pk=%s: validated after update, ready_for_export=%s", pk, is_valid)
    except Exception as e:
        log.error("pk=%s: validation error: %s", pk, str(e))
    # =====================================================

    return Response(ScannedDocumentSerializer(doc).data)





# Udaliajet produkt s doka
@api_view(['POST'])
@permission_classes([IsAuthenticated])
def clear_document_product(request, pk):
    from .models import ScannedDocument
    from .serializers import ScannedDocumentSerializer

    doc = ScannedDocument.objects.filter(pk=pk, user=request.user).first()
    if not doc:
        return Response({'error': 'Not found'}, status=404)

    # –û—á–∏—â–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ–ª—è —Ç–æ–≤–∞—Ä–∞
    doc.prekes_pavadinimas = ""
    doc.prekes_kodas = ""
    doc.prekes_barkodas = ""
    doc.preke_paslauga = ""
    doc.save(update_fields=["prekes_pavadinimas", "prekes_kodas", "prekes_barkodas", "preke_paslauga"])

    return Response(ScannedDocumentSerializer(doc).data)



# Udaliajet produkt s lineitem
@api_view(['POST'])
@permission_classes([IsAuthenticated])
def clear_lineitem_product(request, pk, lineitem_id):
    from .models import ScannedDocument, LineItem
    from .serializers import LineItemSerializer

    doc = ScannedDocument.objects.filter(pk=pk, user=request.user).first()
    if not doc:
        return Response({'error': 'Document not found'}, status=404)

    item = LineItem.objects.filter(document=doc, pk=lineitem_id).first()
    if not item:
        return Response({'error': 'Line item not found'}, status=404)

    item.prekes_pavadinimas = ""
    item.prekes_kodas = ""
    item.prekes_barkodas = ""
    item.preke_paslauga = ""
    item.save(update_fields=["prekes_pavadinimas", "prekes_kodas", "prekes_barkodas", "preke_paslauga"])

    return Response(LineItemSerializer(item).data)



@api_view(['PATCH'])
@permission_classes([IsAuthenticated])
def update_view_mode(request):
    """
    PATCH /users/me/view-mode/
    Body: { "view_mode": "single" | "multi" }
    """
    serializer = ViewModeSerializer(data=request.data)
    serializer.is_valid(raise_exception=True)

    request.user.view_mode = serializer.validated_data['view_mode']
    request.user.save(update_fields=['view_mode'])

    return Response({'view_mode': request.user.view_mode}, status=status.HTTP_200_OK)




@api_view(['PATCH'])
@permission_classes([IsAuthenticated])
def update_lineitem_fields(request, doc_id, lineitem_id):
    from .serializers import LineItemSerializer  # —É–±–µ–¥–∏—Å—å, —á—Ç–æ –µ—Å—Ç—å
    doc = get_object_or_404(ScannedDocument, pk=doc_id, user=request.user)
    lineitem = get_object_or_404(LineItem, pk=lineitem_id, document=doc)

    allowed = ['prekes_kodas', 'prekes_pavadinimas', 'prekes_barkodas']
    for field in allowed:
        if field in request.data:
            setattr(lineitem, field, request.data[field])
    lineitem.save()

    return Response(LineItemSerializer(lineitem).data, status=200)





#Autocomplete funkcii

@api_view(['GET'])
@permission_classes([IsAuthenticated])
def autocomplete_products(request):
    query = request.GET.get('q', '').strip()
    qs = ProductAutocomplete.objects.filter(user=request.user)
    if query:
        qs = qs.filter(
            Q(prekes_pavadinimas__icontains=query) |
            Q(prekes_kodas__icontains=query) |
            Q(prekes_barkodas__icontains=query)
        )
    qs = qs.order_by('prekes_pavadinimas')[:30]  # –û–≥—Ä–∞–Ω–∏—á—å 30, —á—Ç–æ–±—ã –Ω–µ –≥—Ä—É–∑–∏—Ç—å –≤—Å—ë
    data = [
        {
            "id": prod.id,
            "prekes_pavadinimas": prod.prekes_pavadinimas,
            "prekes_kodas": prod.prekes_kodas,
            "prekes_barkodas": prod.prekes_barkodas,
            # –¥–æ–±–∞–≤—å –Ω—É–∂–Ω—ã–µ –ø–æ–ª—è!
        }
        for prod in qs
    ]
    return Response(data)




@api_view(['GET'])
@permission_classes([IsAuthenticated])
def autocomplete_clients(request):
    query = request.GET.get('q', '').strip()
    qs = ClientAutocomplete.objects.filter(user=request.user)
    if query:
        qs = qs.filter(
            Q(pavadinimas__icontains=query) |
            Q(imones_kodas__icontains=query) |
            Q(pvm_kodas__icontains=query)
        )
    qs = qs.order_by('pavadinimas')[:30]
    data = [
        {
            "id": c.id,
            "pavadinimas": c.pavadinimas,
            "imones_kodas": c.imones_kodas,
            "pvm_kodas": c.pvm_kodas,
            "company_address": c.address,
            "company_country_iso": c.country_iso,
            "company_iban": c.ibans,
            # –Ω—É–∂–Ω—ã–µ –ø–æ–ª—è
        }
        for c in qs
    ]
    return Response(data)





























# --- –ò–º–ø–æ—Ä—Ç —Ç–æ–≤–∞—Ä–æ–≤ (products) ---
@api_view(["POST"])
@permission_classes([IsAuthenticated])
def import_products_view(request):
    file = request.FILES.get('file')
    if not file:
        return Response({"error": "No file provided."}, status=status.HTTP_400_BAD_REQUEST)
    try:
        report = import_products_from_xlsx(request.user, file)
        return Response(report, status=status.HTTP_200_OK)
    except Exception as e:
        return Response({"error": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# --- –ò–º–ø–æ—Ä—Ç –∫–ª–∏–µ–Ω—Ç–æ–≤ (clients) ---
@api_view(["POST"])
@permission_classes([IsAuthenticated])
def import_clients_view(request):
    file = request.FILES.get('file')
    if not file:
        return Response({"error": "No file provided."}, status=status.HTTP_400_BAD_REQUEST)
    try:
        report = import_clients_from_xlsx(request.user, file)
        return Response(report, status=status.HTTP_200_OK)
    except Exception as e:
        return Response({"error": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)






#Proverka ili obnovlenija default accounting program

@api_view(['GET', 'PATCH'])
@permission_classes([IsAuthenticated])
def user_profile(request):
    user = request.user
    if request.method == 'PATCH':
        old_company_code = user.company_code
        serializer = CustomUserSerializer(user, data=request.data, partial=True, context={'request': request})
        if serializer.is_valid():
            serializer.save()
            new_company_code = serializer.validated_data.get('company_code', old_company_code)

            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å—Ç–∞—Ä—É—é —Å—Ç—Ä–æ–∫—É –ø–æ —Å—Ç–∞—Ä–æ–º—É company_code
            ca = ClientAutocomplete.objects.filter(
                user=user,
                imones_kodas=old_company_code
            ).first()

            if not ca:
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –ø—Ä–æ–±—É–µ–º –ø–æ –Ω–æ–≤–æ–º—É
                ca = ClientAutocomplete.objects.filter(
                    user=user,
                    imones_kodas=new_company_code
                ).first()

            if not ca:
                # –ï—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é
                ca = ClientAutocomplete(user=user, imones_kodas=new_company_code)

            # –¢–µ–ø–µ—Ä—å –æ–±–Ω–æ–≤–ª—è–µ–º –≤—Å–µ –ø–æ–ª—è:
            ca.pavadinimas = user.company_name
            ca.imones_kodas = new_company_code
            ca.pvm_kodas = user.vat_code
            ca.ibans = user.company_iban
            ca.address = user.company_address
            ca.country_iso = user.company_country_iso
            ca.save()
            
            return Response(serializer.data)
        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)
    serializer = CustomUserSerializer(user)
    return Response(serializer.data)





@api_view(['POST'])
@permission_classes([IsAdminUser])
def update_currency_rates_view(request):
    d = request.data.get('date')
    if d:
        try:
            from datetime import datetime
            d = datetime.strptime(d, '%Y-%m-%d').date()
        except Exception:
            return Response({'error': 'Invalid date'}, status=400)
    else:
        d = date.today()
    count = update_currency_rates(d)
    return Response({'message': f'Updated {count} currency rates for {d}.'})





@api_view(['GET'])
@permission_classes([IsAuthenticated])
def user_me_view(request):
    serializer = CustomUserSerializer(request.user)
    return Response(serializer.data)


class TrackAdClickView(generics.CreateAPIView):
    queryset = AdClick.objects.all()
    serializer_class = AdClickSerializer
    permission_classes = [permissions.AllowAny]  # –¥–∞–∂–µ –≥–æ—Å—Ç–∏ –º–æ–≥—É—Ç

    def create(self, request, *args, **kwargs):
        user = request.user if request.user.is_authenticated else None
        ip = request.META.get("REMOTE_ADDR")
        ua = request.META.get("HTTP_USER_AGENT", "")

        ad_click = AdClick.objects.create(
            ad_name=request.data.get("ad_name", "Unknown"),
            user=user,
            ip_address=ip,
            user_agent=ua
        )
        return Response({"status": "ok"})


#Skacivanje Apskaita5 plugina

FILE_PATH = "/var/files/DokSkenas_apskaita5_adapteris.zip"

def _sha256(path: str) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()

@api_view(['GET'])
@permission_classes([IsAuthenticated])  # –¥–æ—Å—Ç—É–ø —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö
def download_apskaita5_adapter(request):
    if not os.path.exists(FILE_PATH):
        raise Http404("Adapter not found")

    resp = FileResponse(open(FILE_PATH, "rb"))
    resp["Content-Type"] = "application/zip"
    resp["Content-Disposition"] = f'attachment; filename="{os.path.basename(FILE_PATH)}"'
    resp["X-Checksum-SHA256"] = _sha256(FILE_PATH)
    return resp




#JWT functions

class CustomTokenObtainPairView(TokenObtainPairView):
    def post(self, request, *args, **kwargs):
        
        try:
            response = super().post(request, *args, **kwargs)
            tokens = response.data

            access_token = tokens['access']
            refresh_token = tokens['refresh']

            res = Response()

            res.data = {'success':True}

            res.set_cookie(
                key="access_token",
                value=access_token,
                httponly=True,
                secure=True,
                samesite='Lax',
                path='/'
            )

            res.set_cookie(
                key="refresh_token",
                value=refresh_token,
                httponly=True,
                secure=True,
                samesite='Lax',
                path='/'
            )

            return res


        except:
            return Response({'success':False})

class CustomRefreshTokenView(TokenRefreshView):
    def post(self, request, *args, **kwargs):
        try:
            refresh_token = request.COOKIES.get('refresh_token')

            request.data['refresh'] = refresh_token

            response = super().post(request, *args, **kwargs)

            tokens = response.data
            access_token = tokens['access']

            res = Response()

            res.data = {'refreshed':True}

            res.set_cookie(
                key='access_token',
                value=access_token,
                httponly=True,
                secure=True,
                samesite='Lax',
                path='/'
            )

            return res

        except:
            return Response({'refreshed':False})


@api_view(['POST'])
def logout(request):
    try:
        res = Response()
        res.data = {'success':True}
        res.delete_cookie('access_token', path='/', samesite='Lax')
        res.delete_cookie('refresh_token', path='/', samesite='Lax')
        return res
    except:
        return Response({'success':False})
    

@api_view(['POST'])
@permission_classes([IsAuthenticated])
def is_authenticated(request):
    return Response({'authenticated':True})


# Funkcija dlia sozdanija trial

@permission_classes([AllowAny])
def create_trial_subscription(user):
    """
    –°–æ–∑–¥–∞—ë—Ç –ø—Ä–æ–±–Ω—É—é –ø–æ–¥–ø–∏—Å–∫—É –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    """
    logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–∏–∞–ª-–ø–æ–¥–ø–∏—Å–∫–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user.email}")
    try:
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç—Ä–∏–∞–ª-–ø–æ–¥–ø–∏—Å–∫—É
        trial_start_date = timezone.now()
        trial_end_date = trial_start_date + timedelta(days=3000)

        user.subscription_status = 'trial'
        user.subscription_plan = 'trial'
        user.subscription_start_date = trial_start_date
        user.subscription_end_date = trial_end_date
        user.save()

        logger.info(f"–¢—Ä–∏–∞–ª-–ø–æ–¥–ø–∏—Å–∫–∞ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user.email}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç—Ä–∏–∞–ª-–ø–æ–¥–ø–∏—Å–∫–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user.email}: {str(e)}")
        raise e



@api_view(['POST'])
@authentication_classes([])  # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
@permission_classes([AllowAny])  # –†–∞–∑—Ä–µ—à–∏—Ç—å –≤—Å–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –¥–æ—Å—Ç—É–ø –∫ —ç—Ç–æ–º—É —ç–Ω–¥–ø–æ–∏–Ω—Ç—É
def register(request):
    """
    –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–∏–∞–ª-–ø–æ–¥–ø–∏—Å–∫–∏.
    """
    logger.info("–ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.")

    # –£–¥–∞–ª–µ–Ω–∏–µ cookies —Å —Ç–æ–∫–µ–Ω–∞–º–∏
    if 'access_token' in request.COOKIES:
        logger.info("–£–¥–∞–ª—è–µ–º access_token –∏–∑ cookies.")
        del request.COOKIES['access_token']

    if 'refresh_token' in request.COOKIES:
        logger.info("–£–¥–∞–ª—è–µ–º refresh_token –∏–∑ cookies.")
        del request.COOKIES['refresh_token']

    try:
        serializer = CustomUserSerializer(data=request.data)
        if serializer.is_valid():
            logger.info("–î–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤–∞–ª–∏–¥–Ω—ã.")

            # –°–æ–∑–¥–∞—ë–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user = serializer.save()
            logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user.email} —É—Å–ø–µ—à–Ω–æ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω.")

            # –°–æ–∑–¥–∞—ë–º —Ç—Ä–∏–∞–ª-–ø–æ–¥–ø–∏—Å–∫—É –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            create_trial_subscription(user)

            # 3Ô∏è‚É£ –°—Ç–∞–≤–∏–º welcome email –≤ –æ—á–µ—Ä–µ–¥—å Celery (–ø–æ—Å–ª–µ –∫–æ–º–º–∏—Ç–∞)

            try:
                t0 = perf_counter()
                siusti_sveikinimo_laiska(user)
                t1 = perf_counter()
                logger.info(f"Welcome email i≈°si≈≥stas vartotojui {user.email} per {t1 - t0:.4f}s (be Celery).")
            except Exception as mail_err:
                logger.exception(f"Nepavyko i≈°si≈≥sti welcome email be Celery: {mail_err}")

            # try:
            #     t_reg = perf_counter()

            #     def _enqueue():
            #         t0 = perf_counter()
            #         logger.info(f"[ENQUEUE] on_commit fired; start apply_async for {user.email}")
            #         try:
            #             task_siusti_sveikinimo_laiska.apply_async(args=[user.id], ignore_result=True)
            #         finally:
            #             t1 = perf_counter()
            #             logger.info(f"[ENQUEUE] apply_async_time={t1 - t0:.4f}s for {user.email}")

            #     transaction.on_commit(_enqueue)

            #     t_reg_done = perf_counter()
            #     logger.info(f"Queued welcome email for {user.email}. on_commit_registration_time={t_reg_done - t_reg:.4f}s")

            # except Exception as mail_err:
            #     logger.exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç–∞–≤–∏—Ç—å welcome email –≤ –æ—á–µ—Ä–µ–¥—å: {mail_err}")

            return Response({
                "message": "User successfully registered!",
                "user": {
                    "id": user.id,
                    "email": user.email,
                    "subscription_status": user.subscription_status,
                    "subscription_plan": user.subscription_plan,
                    "subscription_start_date": user.subscription_start_date,
                    "subscription_end_date": user.subscription_end_date
                }
            }, status=201)

        logger.warning(f"–û—à–∏–±–∫–∞ –≤ –¥–∞–Ω–Ω—ã—Ö —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏: {serializer.errors}")
        return Response(serializer.errors, status=400)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {str(e)}")
        return Response({"error": "An error occurred during registration."}, status=500)




# Proveriajem status subscriptiona usera
@api_view(['GET'])
@permission_classes([IsAuthenticated])
def subscription_status(request):
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user = request.user

        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (CustomUser)
        user_profile = get_object_or_404(CustomUser, pk=user.pk)

        # –í—ã–∑—ã–≤–∞–µ–º –º–µ—Ç–æ–¥ get_subscription_status –∏–∑ –º–æ–¥–µ–ª–∏ CustomUser
        status = user_profile.get_subscription_status()

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∞—Ç—É—Å –ø–æ–¥–ø–∏—Å–∫–∏
        return Response({'status': status}, status=200)

    except Exception as e:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –≤–æ–∑–≤—Ä–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
        return Response({'error': str(e)}, status=500)
    




#DLIA SUPERUSEROV!!!:
#1) dlia admin-suvestine


def _ensure_dict(x):
    if x is None:
        return {}
    if isinstance(x, dict):
        return x
    if isinstance(x, str):
        try:
            return json.loads(x)
        except Exception:
            return {}
    return {}

# def summarize_doc_issues(doc_struct):
#     """
#     –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ 'error' –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –∂–µ—Å—Ç–∫–∏—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤:
#       - _check_minimum_anchors_ok == False
#       - _doc_amounts_consistent == False
#       - ar_sutapo == False –ò (–ª—é–±–æ–π –∏–∑ _lines_sum_matches_wo/with/vat == False)
#       - '–∫—Ä–∞—Å–Ω—ã–µ' hints (DOC-LINES-NOT-MATCHING-*, LI-PRICE-MISMATCH, LI-ZERO-VAT-DISCOUNTS-MISMATCH)
#       - –≤ –ª–æ–≥–∞—Ö –µ—Å—Ç—å —Å—Ç—Ä–æ–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è –Ω–∞ '‚ùó'
#     """
#     doc = _ensure_dict(doc_struct)

#     # --- –ª–æ–≥–∏ / —Ö–∏–Ω—Ç—ã ---
#     logs = doc.get("_global_validation_log") or []
#     if isinstance(logs, str):
#         logs = [logs]
#     hints = doc.get("_lines_structured_hints") or []
#     if isinstance(hints, str):
#         hints = [hints]

#     bang = [s for s in logs if isinstance(s, str) and s.strip().startswith("‚ùó")]
#     red_hints = [h for h in hints if (
#         isinstance(h, str) and (
#             h.startswith("DOC-LINES-NOT-MATCHING-")
#             or "LI-PRICE-MISMATCH" in h
#             or "LI-ZERO-VAT-DISCOUNTS-MISMATCH" in h
#         )
#     )]

#     # --- —Ñ–ª–∞–≥–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ ---
#     min_ok        = bool(doc.get("_check_minimum_anchors_ok", True))
#     doc_consistent= bool(doc.get("_doc_amounts_consistent", True))
#     ar_sutapo     = bool(doc.get("ar_sutapo", True))

#     match_wo   = doc.get("_lines_sum_matches_wo")
#     match_with = doc.get("_lines_sum_matches_with")
#     match_vat  = doc.get("_lines_sum_matches_vat")

#     separate_vat = bool(doc.get("separate_vat"))
#     vat_failed = (match_vat is False) if not separate_vat else False

#     lines_failed = (match_wo is False) or (match_with is False) or vat_failed
#     lines_block  = (not ar_sutapo) and lines_failed

#     # --- error-—É—Å–ª–æ–≤–∏—è ---
#     errors = []
#     if not min_ok:
#         errors.append("min-anchors")
#     if not doc_consistent:
#         errors.append("doc-core")
#     if lines_block:
#         errors.append("lines-vs-doc")
#     if red_hints:
#         errors.append("hints")
#     if bang:
#         errors.append("bang")

#     has_error = bool(errors)

#     # --- –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ---
#     badges = []
#     if not min_ok:        badges.append("min‚äÑ")
#     if not doc_consistent:badges.append("core‚úó")
#     if lines_block:       badges.append("Œ£(lines)‚â†doc")
#     if red_hints:         badges.append("hint!")
#     if bang:              badges.append(f"‚ùó√ó{len(bang)}")

#     # –∫—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞: –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Äî –∫—Ä–∞—Å–Ω—ã–π —Ö–∏–Ω—Ç ‚Üí '‚ùó' ‚Üí –±–µ–π–¥–∂–∏
#     summary = (red_hints[0] if red_hints else (bang[0] if bang else " ".join(badges)))[:300]

#     issue_count = int(len(red_hints) + len(bang)
#                       + (not min_ok) + (not doc_consistent) + (1 if lines_block else 0))

#     return {
#         "has_issues": has_error,
#         "severity": "error" if has_error else "ok",
#         "issue_badges": " ".join(badges),
#         "issue_summary": summary,
#         "issue_count": issue_count,
#     }

def _ensure_dict(x):
    if x is None:
        return {}
    if isinstance(x, dict):
        return x
    if isinstance(x, str):
        try:
            return json.loads(x)
        except Exception:
            return {}
    return {}

def summarize_doc_issues(doc_struct):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç 'error' –¢–û–õ–¨–ö–û –µ—Å–ª–∏ overall_status == "FAIL" –∏–∑ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏.
    –≠—Ç–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    """
    doc = _ensure_dict(doc_struct)

    # ‚úÖ –ï–î–ò–ù–°–¢–í–ï–ù–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: overall_status –∏–∑ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    math_failed = False
    math_badge = None
    validation_type = None
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª—è detaliai (—Å line_items)
    final_validation = doc.get("_final_math_validation")
    if final_validation:
        overall = final_validation.get("summary", {}).get("overall_status")
        if overall == "FAIL":
            math_failed = True
            math_badge = "MATH‚úó"
            validation_type = "detaliai"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª—è sumiskai (–±–µ–∑ line_items)
    sumiskai_validation = doc.get("_final_math_validation_sumiskai")
    if sumiskai_validation:
        overall = sumiskai_validation.get("overall_status")
        if overall == "FAIL":
            math_failed = True
            math_badge = "MATH‚úó"
            validation_type = "sumiskai"

    has_error = math_failed

    # --- –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ---
    badges = []
    if math_badge:
        badges.append(math_badge)

    summary = " ".join(badges) if badges else ""
    if validation_type:
        summary = f"{summary} ({validation_type})".strip()

    issue_count = 1 if has_error else 0

    return {
        "has_issues": has_error,
        "severity": "error" if has_error else "ok",
        "issue_badges": " ".join(badges),
        "issue_summary": summary,
        "issue_count": issue_count,
    }


@api_view(['GET'])
@permission_classes([IsAuthenticated])
def admin_documents_with_errors(request):
    """–î–ª—è superuser ‚Äî –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –æ—à–∏–±–∫–∞–º–∏ (–≤—Å–µ, –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –¥–∞—Ç–µ)."""
    user = request.user
    if not user.is_superuser:
        return Response({"detail": "Not authorized"}, status=status.HTTP_403_FORBIDDEN)

    # —Ñ–∏–ª—å—Ç—Ä –ø–æ —Å—Ç–∞—Ç—É—Å—É ‚Äî –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
    status_filter = request.GET.get('status')

    qs = ScannedDocument.objects.all()

    if status_filter:
        qs = qs.filter(status=status_filter)

    # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–≥—Ä—É–∑–∫–∏ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
    qs = qs.order_by('-uploaded_at')

    ser = ScannedDocumentListSerializer(qs, many=True)

    data = []
    for obj, row in zip(qs, ser.data):
        doc_struct_raw = getattr(obj, 'structured_json', None) or getattr(obj, 'gpt_raw_json', None)
        issues = summarize_doc_issues(doc_struct_raw)
        if not issues["has_issues"]:
            continue

        r = dict(row)
        r["owner_email"]   = getattr(obj.user, "email", None)
        r["issue_has"]     = issues["has_issues"]
        r["issue_badges"]  = issues["issue_badges"]
        r["issue_summary"] = issues["issue_summary"]
        r["issue_count"]   = issues["issue_count"]
        data.append(r)

    return Response(data)


#2) dlia visi-failai
@api_view(['GET'])
@permission_classes([IsAuthenticated])
def admin_all_documents(request):
    """
    –î–ª—è superuser ‚Äî —Å–≤–æ–¥–Ω—ã–π —Å–ø–∏—Å–æ–∫ –í–°–ï–• –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.
    –î–æ–±–∞–≤–ª–µ–Ω—ã user_id –∏ email.
    """
    user = request.user
    if not user.is_superuser:
        return Response({"detail": "Not authorized"}, status=status.HTTP_403_FORBIDDEN)

    qs = ScannedDocument.objects.select_related('user').all()

    # --- —Ñ–∏–ª—å—Ç—Ä—ã ---
    status_filter = request.GET.get('status')
    if status_filter:
        qs = qs.filter(status=status_filter)

    owner = request.GET.get('owner')
    if owner:
        qs = qs.filter(user__email__icontains=owner)

    from django.utils.dateparse import parse_datetime, parse_date
    from datetime import timedelta

    def _parse_any_dt(value):
        if not value:
            return None
        dt = parse_datetime(value)
        if dt:
            return dt
        d = parse_date(value)
        return d

    date_from = _parse_any_dt(request.GET.get('date_from'))
    date_to = _parse_any_dt(request.GET.get('date_to'))

    if date_from:
        qs = qs.filter(uploaded_at__gte=date_from)
    if date_to:
        # –µ—Å–ª–∏ —Ç–æ–ª—å–∫–æ –¥–∞—Ç–∞ ‚Äî –¥–æ–±–∞–≤–∏–º –∫–æ–Ω–µ—Ü –¥–Ω—è
        if hasattr(date_to, 'hour'):
            qs = qs.filter(uploaded_at__lt=date_to)
        else:
            qs = qs.filter(uploaded_at__lt=(date_to + timedelta(days=1)))

    # --- —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ ---
    order = request.GET.get('order') or '-uploaded_at'
    if order not in {'uploaded_at', '-uploaded_at'}:
        order = '-uploaded_at'
    qs = qs.order_by(order)

    # --- –ø–∞–≥–∏–Ω–∞—Ü–∏—è ---
    from rest_framework.pagination import PageNumberPagination
    paginator = PageNumberPagination()
    paginator.page_size = int(request.GET.get('page_size', 50))
    page = paginator.paginate_queryset(qs, request)

    from .serializers import ScannedDocumentListSerializer
    ser = ScannedDocumentListSerializer(page, many=True)

    # --- –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö ---
    data = []
    for obj, row in zip(page, ser.data):
        doc_struct_raw = getattr(obj, 'structured_json', None) or getattr(obj, 'gpt_raw_json', None)
        issues = summarize_doc_issues(doc_struct_raw)

        # –≤—Å—Ç–∞–≤–ª—è–µ–º user_id –∏ email –≤ –Ω–∞—á–∞–ª–æ
        enriched_row = {
            "user_id": getattr(obj.user, "id", None),
            "owner_email": getattr(obj.user, "email", None),
        }
        enriched_row.update(row)

        # –¥–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö
        enriched_row["issue_has"] = issues["has_issues"]
        enriched_row["issue_badges"] = issues["issue_badges"]
        enriched_row["issue_summary"] = issues["issue_summary"]
        enriched_row["issue_count"] = issues["issue_count"]

        data.append(enriched_row)

    return paginator.get_paginated_response(data)


#3) Dlia users
@api_view(["GET"])
@permission_classes([IsAuthenticated])
def admin_users_simple(request):
    """
    –î–ª—è superuser ‚Äî —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (CustomUser).
    –ë–µ–∑ –ø–æ–¥–ø–∏—Å–æ—á–Ω—ã—Ö –ø–æ–ª–µ–π, –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤, –±–µ–∑ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏.
    """
    if not request.user.is_superuser:
        return Response({"detail": "Not authorized"}, status=status.HTTP_403_FORBIDDEN)

    qs = CustomUser.objects.all().order_by("-date_joined")
    ser = CustomUserAdminListSerializer(qs, many=True, context={'request': request})
    return Response(ser.data)



#Wagtail blog
class GuideCategoryViewSet(viewsets.ReadOnlyModelViewSet):
    """
    /guides-api/v2/guide-categories/                 -> —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    /guides-api/v2/guide-categories/<slug>/          -> –∫–∞—Ç–µ–≥–æ—Ä–∏—è + articles[] (–¥–µ—Ç–∞–ª—å–Ω–æ)
    /guides-api/v2/guide-categories/<slug>/articles/ -> —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    """
    permission_classes = [AllowAny]
    lookup_field = "slug"
    queryset = GuideCategoryPage.objects.live().public().order_by("order", "title")

    def get_serializer_class(self):
        # list -> –∫–æ—Ä–æ—Ç–∫–∏–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ç–æ—Ä
        # retrieve -> –¥–µ—Ç–∞–ª—å–Ω—ã–π (—Å –≤–ª–æ–∂–µ–Ω–Ω—ã–º –º–∞—Å—Å–∏–≤–æ–º —Å—Ç–∞—Ç–µ–π)
        return (
            GuideCategoryDetailSerializer
            if self.action == "retrieve"
            else GuideCategoryListSerializer
        )

    @action(detail=True, methods=["get"], url_path="articles")
    def articles(self, request, slug=None):
        """
        –í–µ—Ä–Ω—ë—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (—É–¥–æ–±–Ω–æ –¥–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ —Ñ—Ä–æ–Ω—Ç–∞).
        GET-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã: ?limit=12&offset=0
        """
        category = self.get_object()

        try:
            limit = int(request.query_params.get("limit", 100))
            offset = int(request.query_params.get("offset", 0))
        except ValueError:
            limit, offset = 100, 0

        qs = (
            GuidePage.objects.child_of(category)
            .live()
            .public()
            .specific()
            .order_by("-first_published_at")
        )
        total = qs.count()
        items = qs[offset : offset + limit]

        data = GuideArticleListSerializer(items, many=True, context={"request": request}).data
        return Response(
            {
                "count": total,
                "limit": limit,
                "offset": offset,
                "results": data,
            }
        )


class GuideArticleViewSet(viewsets.ReadOnlyModelViewSet):
    """
    /guides-api/v2/guides/           -> (–æ–ø—Ü.) —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π (–∫–æ—Ä–æ—Ç–∫–∏–µ –∫–∞—Ä—Ç–æ—á–∫–∏)
    /guides-api/v2/guides/<slug>/    -> –¥–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è
    """
    permission_classes = [AllowAny]
    lookup_field = "slug"

    def get_queryset(self):
        return GuidePage.objects.live().public().specific()

    def get_serializer_class(self):
        return (
            GuideArticleDetailSerializer
            if self.action == "retrieve"
            else GuideArticleListSerializer
        )


# Update doc and item field in Preview

# --- —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –ø–æ–ª—è ---
ALLOWED_DOC_FIELDS = {
    "invoice_date","due_date","operation_date","document_series","document_number","order_number",
    "amount_wo_vat","vat_amount","vat_percent","amount_with_vat","currency","paid_by_cash",
    "buyer_name","buyer_id","buyer_vat_code","seller_name","seller_id","seller_vat_code",
    "prekes_kodas","prekes_pavadinimas","prekes_barkodas", "invoice_discount_wo_vat", "invoice_discount_with_vat"
}

ALLOWED_LINE_FIELDS = {
    "prekes_kodas","prekes_pavadinimas","prekes_barkodas",
    "unit","quantity","price","subtotal","vat","vat_percent","total",
}

REQUIRED_FIELDS = {
    'invoice_date', 'document_number', 'amount_wo_vat', 'vat_amount', 
    'amount_with_vat', 'currency', 'seller_name', 'seller_vat_code', 
    'buyer_name', 'buyer_vat_code', 'seller_id', 'buyer_id'
}

MATH_FIELDS = {
    'amount_wo_vat', 'vat_amount', 'amount_with_vat', 'vat_percent',
    'invoice_discount_wo_vat', 'invoice_discount_with_vat'
}

LINE_MATH_FIELDS = {
    'quantity', 'price', 'subtotal', 'vat', 'vat_percent', 'total',
    'discount_wo_vat', 'discount_with_vat'
}


class InlineDocUpdateView(APIView):
    permission_classes = [IsOwner]

    def patch(self, request, doc_id):
        doc = get_object_or_404(ScannedDocument, pk=doc_id, user=request.user)
        field = request.data.get("field")
        value = request.data.get("value")

        if field not in ALLOWED_DOC_FIELDS:
            return Response({"detail": "Field not allowed"}, status=400)

        if value in ("", None):
            value = None

        setattr(doc, field, value)
        doc.save(update_fields=[field])

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        response_data = {
            "ok": True,
            "id": doc.id,
            field: getattr(doc, field),
        }
        
        try:
            if field in REQUIRED_FIELDS:
                is_valid = check_required_fields_for_export(doc)
                doc.ready_for_export = is_valid
                response_data['ready_for_export'] = is_valid
            
            if field in MATH_FIELDS:
                is_valid, _ = validate_document_math_for_export(doc)
                doc.math_validation_passed = is_valid
                response_data['math_validation_passed'] = is_valid
            
            if field in REQUIRED_FIELDS or field in MATH_FIELDS:
                update_fields = []
                if field in REQUIRED_FIELDS:
                    update_fields.append('ready_for_export')
                if field in MATH_FIELDS:
                    update_fields.append('math_validation_passed')
                if update_fields:
                    doc.save(update_fields=update_fields)
        except Exception as e:
            logger.error(f"Validation error: {str(e)}")
        
        return Response(response_data)


class InlineLineUpdateView(APIView):
    permission_classes = [IsOwner]

    def patch(self, request, doc_id, line_id):
        doc = get_object_or_404(ScannedDocument, pk=doc_id, user=request.user)
        line = get_object_or_404(LineItem, pk=line_id, document=doc)

        field = request.data.get("field")
        value = request.data.get("value")

        if field not in ALLOWED_LINE_FIELDS:
            return Response({"detail": "Field not allowed"}, status=400)

        if value in ("", None):
            value = None

        setattr(line, field, value)
        line.save(update_fields=[field])

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        response_data = {
            "ok": True,
            "id": line.id,
            field: getattr(line, field),
        }
        
        try:
            if field in LINE_MATH_FIELDS:
                is_valid, _ = validate_document_math_for_export(doc)
                doc.math_validation_passed = is_valid
                doc.save(update_fields=['math_validation_passed'])
                response_data['math_validation_passed'] = is_valid
        except Exception as e:
            logger.error(f"Validation error: {str(e)}")
        
        return Response(response_data)


# Add / delete lineitem in Preview

class ScannedDocumentViewSet(viewsets.ModelViewSet):
    queryset = ScannedDocument.objects.all()
    serializer_class = ScannedDocumentDetailSerializer
    permission_classes = [IsAuthenticated]

    # --- –î–û–ë–ê–í–ò–¢–¨ –ü–£–°–¢–û–ô LINE ITEM ---
    @action(detail=True, methods=["post"], url_path="add-lineitem")
    def add_lineitem(self, request, pk=None):
        doc = self.get_object()
        line = LineItem.objects.create(document=doc)
        return Response(LineItemSerializer(line).data, status=status.HTTP_201_CREATED)

    # --- –£–î–ê–õ–ò–¢–¨ LINE ITEM ---
    @action(detail=True, methods=["delete"], url_path="delete-lineitem/(?P<line_id>[^/.]+)")
    def delete_lineitem(self, request, pk=None, line_id=None):
        doc = self.get_object()
        try:
            line = doc.line_items.get(id=line_id)
            line.delete()
            return Response(status=status.HTTP_204_NO_CONTENT)
        except LineItem.DoesNotExist:
            return Response({"detail": "Line item not found"}, status=status.HTTP_404_NOT_FOUND)
        


# contact email sender
@api_view(["POST"])
@permission_classes([AllowAny])
def contact_form(request):
    vardas  = (request.data.get("name") or "").strip()
    email   = (request.data.get("email") or "").strip()
    zinute  = (request.data.get("message") or "").strip()
    # subject nƒóra formoje ‚Äì paliekame None (bus generinƒó)

    if not vardas or not email or len(zinute) < 10:
        return Response({"detail": "Klaida formoje"}, status=status.HTTP_400_BAD_REQUEST)

    ok = siusti_kontakto_laiska(vardas=vardas, email=email, zinute=zinute, tema=None)
    if ok:
        return Response({"detail": "≈Ωinutƒó sƒókmingai i≈°si≈≥sta. Aƒçi≈´!"})
    return Response({"detail": "Nepavyko i≈°si≈≥sti ≈æinutƒós. Pabandykite vƒóliau."},
                    status=status.HTTP_500_INTERNAL_SERVER_ERROR)





def send_newsletter():
    text_tpl = (
        "Sveiki,\n\n"
        "dabar su DokSkenu apskaitƒÖ vesite dar lengviau.\n\n"
        "Pridƒójome automatizacijas ir skaitmenizuojant detaliai su eilutƒómis.\n"
        "Nustatymuose rasite skiltƒØ \"Numatytosios preki≈≥ reik≈°mƒós (skaitmenizuojant detaliai)\",\n"
        "kur galƒósite nusistatyti sƒÖlygas dokument≈≥ eilutƒóms.\n\n"
        "Jei j≈´s≈≥ sƒÖlygos bus ƒØvykdytos, eilutei priskirs j≈´s≈≥ i≈°laid≈≥/pajam≈≥ kodƒÖ, tipƒÖ, "
        "pavadinimƒÖ ar barkodƒÖ.\n"
        "SƒÖlygas galit nusistatyti pagal pvm procentƒÖ, eilutƒós pavadinimƒÖ, "
        "pirkƒójo/pardavƒójo rekvezitus.\n\n"
        "Plaƒçiau parod≈æiau ≈°iame video: https://www.facebook.com/reel/1547084576311150\n\n"
        "Jei turƒósite pastebƒójim≈≥, ra≈°ykite.\n\n"
        "Gero savaitgalio,\n"
        "Denis"
    )

    siusti_masini_laiska_visiems(
        subject="Naujos DokSkeno automatizacijos",
        text_template=text_tpl,
        html_template_name=None,      # ‚Üê –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º HTML –≤–æ–æ–±—â–µ
        extra_context=None,           # –º–æ–∂–Ω–æ –æ–ø—É—Å—Ç–∏—Ç—å
        exclude_user_ids=[2, 16, 24, 31, 69, 105],   # –∫–æ–≥–æ –∏—Å–∫–ª—é—á–∏—Ç—å (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        tik_aktyviems=True,
    )